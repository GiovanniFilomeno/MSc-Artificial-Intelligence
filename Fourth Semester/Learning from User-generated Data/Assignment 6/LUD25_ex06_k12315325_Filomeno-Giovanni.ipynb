{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79cb2e6346d62ce6",
   "metadata": {},
   "source": [
    "*UE Learning from User-generated Data, CP MMS, JKU Linz 2025*\n",
    "# Exercise 6: Content based Filtering\n",
    "\n",
    "In this exercise, we delve into content-based filtering, a type of recommender system that makes recommendations by utilizing the features of items and a profile of the user's preferences. Unlike collaborative filtering, which relies on the user-item interactions, content-based filtering focuses on the properties of the items themselves to make recommendations. This approach is particularly useful when we have detailed metadata about items or when dealing with cold start problems related to new users or items.\n",
    "\n",
    "An example of item content properties could be (for a music track): artist name, track title, year of release, genre, as well as the audio track itself (remember, in the collaborative filtering scenario we only worked with item ids without caring about what they actually were). Such complex information as the audio track is usually handled in a form of (item) embeddings, high-dimensional vector representations that capture the characteristics of each item. By analyzing these embeddings, we can identify items that are similar to those a user has liked in the past and recommend them accordingly.\n",
    "\n",
    "Please consult the lecture slides on content-based filtering for a recap.\n",
    "\n",
    "Make sure to rename the notebook according to the convention:\n",
    "\n",
    "LUD25_ex06_k<font color='red'><Matr. Number\\></font>_<font color='red'><Surname-Name\\></font>.ipynb\n",
    "\n",
    "for example:\n",
    "\n",
    "LUD25_ex06_k000007_Bond_James.ipynb\n",
    "\n",
    "## Implementation\n",
    "In this exercise, you will implement two content-based filtering algorithms using item embeddings. We provide the embeddings for each item, and your task is to find items most similar to a user's consumption history. You will then evaluate the performance of your algorithms using the normalized Discounted Cumulative Gain (nDCG) metric across different user groups.\n",
    "\n",
    "Please **only use libraries already imported in the notebook**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75eabaff7ba3409e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:22:02.646333Z",
     "start_time": "2024-06-18T16:21:57.397984Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine_similarity\n",
    "from rec import inter_matr_implicit\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb2c50381214b0",
   "metadata": {},
   "source": [
    "## Data Overview and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a182e348bc73c",
   "metadata": {},
   "source": [
    "This exercise utilizes a dataset that consists of user-item interactions and item embeddings. The dataset is split into several files:\n",
    "\n",
    "`*.user`: Contains information about users.\n",
    "`*.item`: Contains information about items.\n",
    "`*.inter_train`: Contains user-item interactions used for training the recommender system.\n",
    "`*.inter_test`: Contains user-item interactions held out for testing the recommender system.\n",
    "`*.id_musicnn`: Contains embeddings for each item.\n",
    "\n",
    "Let's start by loading these files and taking a closer look at their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68f81ac85fe199a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:22:02.695574Z",
     "start_time": "2024-06-18T16:22:02.648324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users Data Head:\n",
      "   user_id country  age_at_registration gender    registration_date\n",
      "0        0      RU                   25      m  2006-06-12 13:25:12\n",
      "1        1      US                   23      m  2005-08-18 15:25:41\n",
      "2        2      FR                   25      m  2006-02-26 22:39:03\n",
      "3        3      DE                    2      m  2007-02-28 10:12:13\n",
      "4        4      UA                   23      n  2007-10-09 15:21:20\n",
      "\n",
      "Item Data Head:\n",
      "                artist                                   track  item_id\n",
      "0           Black Flag                              Rise Above        0\n",
      "1                 Blur  For Tomorrow - 2012 Remastered Version        1\n",
      "2          Damien Rice                            Moody Mooday        2\n",
      "3                 Muse                            Feeling Good        3\n",
      "4  My Bloody Valentine                                    Soon        4\n",
      "\n",
      "Training Interactions Head:\n",
      "   user_id  item_id  listening_events\n",
      "0      510       50                 3\n",
      "1      510      324                 5\n",
      "2      510       80                 4\n",
      "3      510      266                 3\n",
      "4      510      152                 2\n",
      "\n",
      "Testing Interactions Head:\n",
      "   user_id  item_id  listening_events\n",
      "0      510        0                 6\n",
      "1      510       23                 2\n",
      "2      699       54                22\n",
      "3      699       55                 4\n",
      "4      699       43                 3\n",
      "\n",
      "Embeddings Head:\n",
      "   item_id         0         1         2         3         4         5  \\\n",
      "0      179  0.125290  0.012606  0.081399  0.080409  0.017942  0.169751   \n",
      "1       34  0.320514  0.007550  0.164825  0.008963  0.032615  0.110442   \n",
      "2      107  0.016870  0.000331  0.005580  0.666278  0.000631  0.223498   \n",
      "3       51  0.095227  0.000541  0.010128  0.213568  0.003663  0.117152   \n",
      "4      158  0.305471  0.001308  0.021437  0.003959  0.003706  0.075450   \n",
      "\n",
      "          6         7         8  ...        40        41        42        43  \\\n",
      "0  0.078891  0.019038  0.018354  ...  0.106704  0.038682  0.002625  0.001995   \n",
      "1  0.017253  0.171200  0.012153  ...  0.028441  0.005372  0.000978  0.004036   \n",
      "2  0.291173  0.280088  0.379135  ...  0.004347  0.063467  0.000037  0.000170   \n",
      "3  0.139817  0.246973  0.170498  ...  0.121255  0.024034  0.000129  0.000223   \n",
      "4  0.004312  0.773173  0.099721  ...  0.009150  0.000868  0.000152  0.000584   \n",
      "\n",
      "         44        45        46        47        48        49  \n",
      "0  0.007829  0.024301  0.015865  0.001093  0.092035  0.003197  \n",
      "1  0.006219  0.009023  0.089043  0.004408  0.022120  0.001234  \n",
      "2  0.021398  0.064615  0.000129  0.047967  0.005985  0.000318  \n",
      "3  0.007875  0.020640  0.010570  0.010205  0.091741  0.000644  \n",
      "4  0.003667  0.002294  0.041627  0.170266  0.005763  0.000170  \n",
      "\n",
      "[5 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "def read(dataset, file):\n",
    "    return pd.read_csv(dataset + '/' + dataset + '.' + file, sep='\\t')\n",
    "\n",
    "# Load User Data\n",
    "users = read('lfm-tiny-tunes', 'user')\n",
    "print(\"Users Data Head:\")\n",
    "print(users.head())\n",
    "\n",
    "# Load Item Data\n",
    "items = read('lfm-tiny-tunes', 'item')\n",
    "print(\"\\nItem Data Head:\")\n",
    "print(items.head())\n",
    "\n",
    "# Load Training Interactions\n",
    "train_inters = read('lfm-tiny-tunes', 'inter_train')\n",
    "print(\"\\nTraining Interactions Head:\")\n",
    "print(train_inters.head())\n",
    "\n",
    "# Load Testing Interactions\n",
    "test_inters = read('lfm-tiny-tunes', 'inter_test')\n",
    "print(\"\\nTesting Interactions Head:\")\n",
    "print(test_inters.head())\n",
    "\n",
    "# Load Embeddings\n",
    "embedding = read('lfm-tiny-tunes', 'id_musicnn')\n",
    "print(\"\\nEmbeddings Head:\")\n",
    "print(embedding.head())\n",
    "\n",
    "train_interaction_matrix = inter_matr_implicit(users, items, train_inters, 'lfm-tiny-tunes')\n",
    "test_interaction_matrix = inter_matr_implicit(users, items, test_inters, 'lfm-tiny-tunes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d77c5201de7d0d",
   "metadata": {},
   "source": [
    "Item Similarity Calculation\n",
    "\n",
    "Here you can experiment with the cosine similarity between two item embeddings, to get a feeling what sensible inputs and outputs are. Cosine similarity is a measure that calculates the cosine of the angle between two vectors, often used to measure item similarity in recommendation systems. We will use this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e4cdcac7078c4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:22:02.702968Z",
     "start_time": "2024-06-18T16:22:02.696572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: [[0.10540926]]\n"
     ]
    }
   ],
   "source": [
    "embedding_x = np.array([0.1, 0.2, 0.3, 0.4]).reshape(1, -1)\n",
    "embedding_y = np.array([0.0, 0.1, 0.1, -0.1]).reshape(1, -1)\n",
    "similarity = cosine_similarity(embedding_x, embedding_y)\n",
    "print(f\"Cosine Similarity: {similarity}\")\n",
    "assert -1 <= similarity <= 1, \"Cosine similarity is out of bounds.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ab200e04304908",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 1/3</font>: Implementing the Average Embedding Similarity Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98ef00b8765c030",
   "metadata": {},
   "source": [
    "The idea of the first content-based recommender is to use the embeddings of the items the user already consumed to create one representation of the user's taste (by just averaging them). Then the recommedation score is assigned to each of the items in the collection as cosine_similarity between the user's profile and each item's embedding. Highest scoring items not present in the user's history are then selected for recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63174e832805d24e",
   "metadata": {},
   "source": [
    "First, familiarize yourself with the concept of item embeddings, which are vector representations capturing the essential features or qualities of each item.\n",
    "\n",
    "After that, develop a function that takes a user's interaction history (items they have already interacted with/seen) and the item embeddings as input. This function should:\n",
    "\n",
    "* Create the user's profile embedding as discussed, calculate the similarity between the profile and all items in the collection.\n",
    "* Rank the items based on their similarity scores.\n",
    "* To ensure that only unseen items are recommended, set the aggregated similarity scores of the items the user has already interacted with to **0** as a floating point number. This will effectively remove them from consideration during the ranking process.\n",
    "* Recommend the top-K most similar items that the user has not interacted with yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac3d92646de6fa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:22:02.710260Z",
     "start_time": "2024-06-18T16:22:02.704655Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_user_profile_embedding(seen_item_ids: list, item_embeddings: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates the average embedding of items a user has interacted with to create a user profile embedding.\n",
    "\n",
    "    Parameters:\n",
    "    - seen_item_ids - list[int], IDs of items already seen by the user, used to filter the embeddings;\n",
    "    - item_embeddings - pd.DataFrame, Unsorted DataFrame containing item_id and item embeddings as separate columns;\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: numpy array of shape (1,embedding_dim), representing the user's average embedding profile. \n",
    "    \"\"\"\n",
    "    \n",
    "    user_profile_embedding = None\n",
    "    \n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "\n",
    "    # Identifying id and the embedding columns\n",
    "    id_col = 'item_id' if 'item_id' in item_embeddings.columns else item_embeddings.columns[0]\n",
    "    embed_cols = item_embeddings.columns.drop(id_col)\n",
    "\n",
    "    # Taking user already seen item\n",
    "    seen_rows = item_embeddings[item_embeddings[id_col].isin(seen_item_ids)][embed_cols]\n",
    "\n",
    "    if seen_rows.empty:\n",
    "        return np.zeros((1, len(embed_cols)))\n",
    "    \n",
    "    user_profile_embedding = seen_rows.to_numpy(dtype=float).mean(axis=0, keepdims=True)\n",
    "    \n",
    "    return user_profile_embedding\n",
    "\n",
    "\n",
    "def average_embedding_similarity_rec(seen_item_ids: list, item_embeddings: pd.DataFrame, _calculate_user_profile_embedding: Callable[[List[int], pd.DataFrame], np.ndarray], top_k: int=10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Recommends items to a user based on the average embedding similarity of items they have interacted with.\n",
    "    It computes the cosine similarity between the user profile and all other items, recommending\n",
    "    the top-K most similar items that the user has not yet interacted with.\n",
    "\n",
    "    seen_item_ids - list[int], ids of items already seen by the user (to exclude from recommendation);\n",
    "    embedding - pd.DataFrame, Unsorted DataFrame containing item_id and item embeddings as separate columns;\n",
    "    _calculate_user_profile_embedding - function, function to calculate the user's average embedding profile;\n",
    "    topK - int, number of recommendations per user to be returned;\n",
    "\n",
    "    returns - 1D np.ndarray, array of IDs of the top-K recommended items, sorted by decreasing similarity\n",
    "            to the user's average embedding profile;\n",
    "    \"\"\"\n",
    "\n",
    "    recommended_item_ids = None\n",
    "    user_profile_embedding = None\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "\n",
    "    id_col = 'item_id' if 'item_id' in item_embeddings.columns else item_embeddings.columns[0]\n",
    "    embed_cols = item_embeddings.columns.drop(id_col)\n",
    "\n",
    "    user_profile_embedding = calculate_user_profile_embedding(seen_item_ids, item_embeddings)\n",
    "\n",
    "    # Similar profile every item \n",
    "    item_matrix = item_embeddings[embed_cols].to_numpy(dtype=float)\n",
    "    sims = cosine_similarity(user_profile_embedding, item_matrix).flatten()\n",
    "\n",
    "    # Mask out the things already consumed\n",
    "    seen_mask = item_embeddings[id_col].isin(seen_item_ids).to_numpy()\n",
    "    sims[seen_mask] = 0.0\n",
    "\n",
    "    # Pick the top-k\n",
    "    k = min(top_k, len(sims))\n",
    "    top_idx = np.argpartition(-sims, k - 1)[:k]   \n",
    "    top_idx = top_idx[np.argsort(-sims[top_idx])]     \n",
    "    recommended_item_ids = item_embeddings.iloc[top_idx][id_col].to_numpy()\n",
    "\n",
    "    return recommended_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9d389aa0ff4dec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:22:02.718421Z",
     "start_time": "2024-06-18T16:22:02.712511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Items for User 1: [244  53 259 201 293]\n"
     ]
    }
   ],
   "source": [
    "user_id_example = users['user_id'].iloc[1]\n",
    "seen_item_ids = train_inters[train_inters['user_id'] == user_id_example]['item_id'].values.tolist()\n",
    "recommended_items = average_embedding_similarity_rec(seen_item_ids, embedding, calculate_user_profile_embedding, top_k=5)\n",
    "print(f\"Recommended Items for User {user_id_example}: {recommended_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c9cad1eac118d3",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 2/3</font>: Implementing a Сontent-based ItemKNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538228199764661",
   "metadata": {},
   "source": [
    "In this task, you have to implement an ItemKNN (similar to exercise 2) but this time using content-based features. This technique builds upon the concept of item embeddings and similarity calculations, but instead of creating a single user profile, it considers the individual similarities between each item the user has interacted with and the top-(KNN)-k items. This allows for a more diverse set of recommendations that capture different aspects of the user's preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7d384018c47a9",
   "metadata": {},
   "source": [
    "**Tips on implementation:**\n",
    "* For each item the user has interacted with, calculate its similarity to all other items in the dataset using their embeddings (using cosine similarity), then keep only the top-k items. Make sure to sort the item embeddings by item_id before calling ```compute_itemknn_scores```.\n",
    "* For each item that could potentially be recommended, combine the similarity scores it received from each of the items the user has already interacted with. This combined score will reflect the overall similarity of the potential recommendation to the user's preferences as expressed through their past interactions. Keep track of the highest similarity score encountered for each potential recommendation.\n",
    "* To ensure that only unseen items are recommended, set the aggregated similarity scores of the items the user has already interacted with to negative infinity. This will effectively remove them from consideration during the ranking process.\n",
    "* The function ```compute_itemknn_scores``` returns an array (sorted by item_id, not by value) of aggregated similarity scores for all items, with higher scores indicating higher similarity.\n",
    "* Rank the items based on their aggregated similarity scores. Recommend the top-K items that the user has not interacted with yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ab47e86c660b90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:22:02.725400Z",
     "start_time": "2024-06-18T16:22:02.719415Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_itemknn_scores(seen_item_ids: list, item_embeddings: pd.DataFrame, k: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    ItemKNN-like scoring using item embeddings\n",
    "    \n",
    "    For each item, find its k most similar items (based on embeddings),\n",
    "    and if any of those have been seen by the user, use their similarities\n",
    "    to compute an aggregated score.\n",
    "    \n",
    "    seen_item_ids - list[int], items the user has seen\n",
    "    item_embeddings - pd.DataFrame, must include 'item_id' and embedding columns\n",
    "    k - int, number of nearest neighbors to consider\n",
    "    \n",
    "    returns - np.ndarray of scores for each item in item_embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    recommendation_scores = None\n",
    "    \n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "\n",
    "    id_col = 'item_id' if 'item_id' in item_embeddings.columns else item_embeddings.columns[0]\n",
    "    embed_cols = item_embeddings.columns.drop(id_col)\n",
    "\n",
    "    item_ids = item_embeddings[id_col].to_numpy()\n",
    "    emb_matrix = item_embeddings[embed_cols].to_numpy(dtype=float)\n",
    "\n",
    "    # Mapping the id to row index (safety)\n",
    "    id2idx = {iid: idx for idx, iid in enumerate(item_ids)}\n",
    "    seen_indices = [id2idx[i] for i in seen_item_ids if i in id2idx]\n",
    "\n",
    "    # Inizialization to -inf, so everything missing stay unrankend\n",
    "    recommendation_scores = np.full(len(item_ids), -np.inf)\n",
    "\n",
    "    if not seen_indices: \n",
    "        return recommendation_scores\n",
    "\n",
    "    # Similarity: all seen items x full catalogue\n",
    "    sim_matrix = cosine_similarity(emb_matrix[seen_indices], emb_matrix)\n",
    "\n",
    "    # Keeping the k closest neighbours per seen item\n",
    "    for row, sidx in zip(sim_matrix, seen_indices):\n",
    "        row[sidx] = -np.inf # --> ignore self-similarity\n",
    "        neighbour_idx = np.argpartition(-row, k)[:k] if k < len(row) else np.arange(len(row))\n",
    "        \n",
    "        # update global score array with the MAX similarity encountered\n",
    "        np.maximum.at(recommendation_scores, neighbour_idx, row[neighbour_idx])\n",
    "\n",
    "    # Never recommend already-seen item\n",
    "    recommendation_scores[seen_indices] = -np.inf\n",
    "\n",
    "    return recommendation_scores\n",
    "\n",
    "\n",
    "def cb_itemknn_recommendation(seen_item_ids: list, item_embeddings: pd.DataFrame, \n",
    "                              _compute_itemknn_scores: Callable[[List[int], pd.DataFrame], np.ndarray], \n",
    "                              top_k: int=10, knn_k: int=10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Recommends items to a user based on the items they have already seen, by sorting the calculated similarity scores\n",
    "    and selecting the top-k items.\n",
    "\n",
    "    seen_item_ids - list[int], ids of items already seen by the user (to exclude from recommendation);\n",
    "    embedding - pd.DataFrame, Unsorted DataFrame containing item_id and item embeddings as separate columns;\n",
    "    _compute_itemknn_scores - function, function to compute aggregated similarity scores for all items;\n",
    "    topK - int, number of recommendations per user to be returned;\n",
    "\n",
    "    returns - 1D np.ndarray, array of IDs of the top-K recommended items, sorted by decreasing similarity\n",
    "            to the user's average embedding profile;\n",
    "    \"\"\"\n",
    "\n",
    "    recommended_item_ids = None\n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "\n",
    "    item_embeddings = item_embeddings.sort_values(\n",
    "        'item_id' if 'item_id' in item_embeddings.columns else item_embeddings.columns[0]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    scores = _compute_itemknn_scores(seen_item_ids, item_embeddings, k=knn_k)\n",
    "\n",
    "    # filter out –inf\n",
    "    valid_mask = np.isfinite(scores)\n",
    "    if not np.any(valid_mask):\n",
    "        return np.array([], dtype=int)\n",
    "\n",
    "    k = min(top_k, valid_mask.sum())\n",
    "    top_idx = np.argpartition(-scores, k - 1)[:k]\n",
    "    top_idx = top_idx[np.argsort(-scores[top_idx])]   \n",
    "\n",
    "    id_col = 'item_id' if 'item_id' in item_embeddings.columns else item_embeddings.columns[0]\n",
    "\n",
    "    recommended_item_ids = item_embeddings.iloc[top_idx][id_col].to_numpy()\n",
    "    \n",
    "    return recommended_item_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff89808ba836e44b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:22:02.732147Z",
     "start_time": "2024-06-18T16:22:02.726396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Items for User 1: [183  43 167 246 202]\n"
     ]
    }
   ],
   "source": [
    "user_id_example = users['user_id'].iloc[1]\n",
    "seen_items_user_example = np.where(train_interaction_matrix[user_id_example, :] > 0)[0]\n",
    "recommended_items = cb_itemknn_recommendation(seen_items_user_example.tolist(), embedding, compute_itemknn_scores, top_k=5)\n",
    "print(f\"Recommended Items for User {user_id_example}: {recommended_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaad68a03922f0a",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 3/3</font>: Evaluating Recommendations with nDCG\n",
    "\n",
    "In this task, you will evaluate the performance of the content-based filtering algorithm you've implemented, alongside other recommender systems that utilize collaborative filtering. Specifically, you will use Precision@K, Recall@K and the normalized Discounted Cumulative Gain (nDCG) metric to assess how effective each recommender system is across different user groups based on their interaction levels.\n",
    "\n",
    "You will compare the following recommender systems:\n",
    "\n",
    "| Recommender Name                      | Abbreviation     | Relative Time Complexity     |\n",
    "|---------------------------------------|------------------|---------------------|\n",
    "| Average Embedding Similarity           | Avg_Item_Embd    |$ O(n^2)   $         |\n",
    "| Content-based Item K-Nearest Neighbors                            | CB_ItemKNN       | $O(n^2) $             |\n",
    "| Singular Value Decomposition         | SVD              | $O(n * m * k)$        |\n",
    "| Collaborative Filtering Item K-Nearest Neighbors             | CF_ItemKNN          | $O(n^2)$              |\n",
    "| Top Popular                          | TopPop           | $O(n \\log{n}) $         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7eb8cf1c9116f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:22:02.737558Z",
     "start_time": "2024-06-18T16:22:02.733123Z"
    }
   },
   "outputs": [],
   "source": [
    "from rec import svd_decompose, svd_recommend_to_list\n",
    "from rec import recTopK\n",
    "from rec import recTopKPop\n",
    "from sklearn.metrics import ndcg_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb1f90df310e8f03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:22:02.748376Z",
     "start_time": "2024-06-18T16:22:02.738563Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation_metrics(recs, g_truth):\n",
    "    predicted_scores = np.zeros(g_truth.shape[1])\n",
    "    predicted_scores[recs] = np.arange(len(recs), 0, -1)\n",
    "    \n",
    "    predicted_scores_binary = np.zeros(g_truth.shape[1])\n",
    "    predicted_scores_binary[recs] = 1\n",
    "    \n",
    "    score_ndcg = ndcg_score(g_truth, predicted_scores.reshape(1, -1), k=len(recs))\n",
    "    p_a_score = precision_score(g_truth[0],predicted_scores_binary)\n",
    "    r_a_score = recall_score(g_truth[0],predicted_scores_binary)     \n",
    "    \n",
    "    return p_a_score, r_a_score, score_ndcg\n",
    "\n",
    "def evaluate_recommender_by_user_groups(user_groups: dict, recommenders: dict, train_interaction_matrix: np.ndarray, test_interaction_matrix: np.ndarray,\n",
    "                                 U: np.ndarray, V: np.ndarray, item_embeddings: pd.DataFrame, _calculate_user_profile_embedding, \n",
    "                                 _compute_aggregated_scores, topK: int=10, n_neighbors: int=5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates recommender systems across user groups, calculating average evaluation metrics (Precision@K, Recall@K, nDCG).\n",
    "\n",
    "    user_groups - dict, keys - names of the user groups (str), values - lists of user IDs belonging to each group;\n",
    "    recommenders - dict, keys - names of recommenders (str), values - recommender functions;\n",
    "    train_interaction_matrix - 2D np.ndarray (users x items), interaction matrix from the training set;\n",
    "    test_interaction_matrix - 2D np.ndarray (users x items), interaction matrix from the test set;\n",
    "    U, V - 2D np.ndarray, matrices resulting from SVD decomposition of the interaction matrix;\n",
    "    item_embeddings - pd.DataFrame, DataFrame containing item IDs and their embeddings;\n",
    "    topK - int, number of top recommendations to consider for evaluation;\n",
    "    n_neighbors - int, number of neighbors for ItemKNN recommender;\n",
    "\n",
    "    returns - pd.DataFrame, with columns: 'User Group', 'Recommender', 'Average p@k', \n",
    "              'Average r@k','Average nDCG', containing evaluation results;\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for group_name, users in user_groups.items():\n",
    "        for recommender_name, recommender_func in tqdm(recommenders.items(), desc=f'Evaluating {group_name} Users'):\n",
    "            metric_scores = np.zeros((len(users),3))\n",
    "            for i, user_id in enumerate(users):\n",
    "                seen_items = np.where(train_interaction_matrix[user_id, :] > 0)[0]  # Items already interacted with by the user\n",
    "                if recommender_name == 'SVD':\n",
    "                    recommendations = recommender_func(user_id, seen_items.tolist(), U, V, topK)\n",
    "                elif recommender_name == 'CF_ItemKNN':\n",
    "                    recommendations = recommender_func(train_interaction_matrix, user_id, topK, n_neighbors)\n",
    "                elif recommender_name == 'TopPop':\n",
    "                    recommendations = recommender_func(train_interaction_matrix, user_id, topK)\n",
    "                elif recommender_name == 'Avg_Item_Embd':\n",
    "                    recommendations = recommender_func(seen_items.tolist(), item_embeddings, _calculate_user_profile_embedding, topK)\n",
    "                elif recommender_name == 'CB_ItemKnn':\n",
    "                    recommendations = recommender_func(seen_items.tolist(), item_embeddings, _compute_aggregated_scores, topK)\n",
    "                else:\n",
    "                    raise NotImplementedError(f'Recommender {recommender_name} not implemented.')\n",
    "\n",
    "                if not isinstance(recommendations, np.ndarray):\n",
    "                    recommendations = np.array(recommendations)\n",
    "\n",
    "                true_relevance = test_interaction_matrix[user_id, :].reshape(1, -1)\n",
    "                metric_scores[i,:] = evaluation_metrics(recommendations, true_relevance)\n",
    "\n",
    "            metric_scores_avg = np.mean(metric_scores,axis = 0)\n",
    "            pk_score, rk_score, score_ndcg = metric_scores_avg\n",
    "            \n",
    "            results.append({'User Group': group_name, 'Recommender': recommender_name, \n",
    "                            'Average p@k': pk_score, 'Average r@k': rk_score, \n",
    "                            'Average nDCG': score_ndcg,})\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b21f024e288b",
   "metadata": {},
   "source": [
    "Here, you will implement a function that evaluates the performance of the recommenders across different user groups based on their interaction levels. You will need to split the users into two groups: one with low interaction levels (below or equal a certain threshold) and one with high interaction levels (above the threshold). The function should then call the `evaluate_ndcg_by_user_groups` function to calculate the average nDCG scores for each recommender across the user groups. Make sure to only use the passed variables and parameters in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a60a6b6c1433ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:22:02.754727Z",
     "start_time": "2024-06-18T16:22:02.749364Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_recommenders(_evaluate_recommender_by_user_groups, user_info: pd.DataFrame, parameters: dict, recommender: dict, user_threshold: int) -> (pd.DataFrame, dict):\n",
    "    \"\"\"\n",
    "    Evaluates recommenders across user groups based on interaction levels.\n",
    "\n",
    "    Splits users into low and high interaction groups based on a threshold and calculates\n",
    "    average Precision@K, Recall@K, and nDCG scores for each recommender within each group.\n",
    "\n",
    "    _evaluate_recommender_by_user_groups - function, function to evaluate recommenders across user groups;\n",
    "    user_info - pd.DataFrame, DataFrame containing user information.\n",
    "    parameters - dict, Dictionary containing data and parameters for evaluation, including:\n",
    "        train_interaction_matrix - 2D np.ndarray, test_interaction_matrix - 2D np.ndarray,\n",
    "        U - 2D np.ndarray, V - 2D np.ndarray, item_embeddings - pd.DataFrame, topK - int,\n",
    "        n_neighbors - int.\n",
    "    recommender - dict, Dictionary of recommender functions, with keys as recommender names\n",
    "                        and values as the corresponding functions.\n",
    "    user_threshold - int, Threshold for dividing users into low and high interaction groups.\n",
    "\n",
    "    returns - tuple:\n",
    "        pd.DataFrame, DataFrame containing evaluation results with columns: 'User Group',\n",
    "            'Recommender', 'Average nDCG'.\n",
    "        dict, Dictionary containing the user groups with keys 'Low Interaction' and\n",
    "            'High Interaction', and values as lists of user IDs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    evaluation_results_df = None\n",
    "\n",
    "    user_groups = {\n",
    "        'Low Interaction': [],\n",
    "        'High Interaction': []\n",
    "    }\n",
    "\n",
    "    # TODO: YOUR IMPLEMENTATION\n",
    "\n",
    "    # Unpacking dictionary\n",
    "    train_inter = parameters['train_interaction_matrix']\n",
    "    n_users     = train_inter.shape[0]\n",
    "\n",
    "    # Mapping row index - user_id mapping\n",
    "    if 'user_id' in user_info.columns:\n",
    "        user_ids = user_info['user_id'].to_numpy()\n",
    "    else:\n",
    "        user_ids = np.arange(n_users)\n",
    "\n",
    "    for row_idx, uid in enumerate(user_ids):\n",
    "        n_inter = np.count_nonzero(train_inter[row_idx, :])\n",
    "        if n_inter <= user_threshold:\n",
    "            user_groups['Low Interaction'].append(row_idx)\n",
    "        else:\n",
    "            user_groups['High Interaction'].append(row_idx)\n",
    "\n",
    "    # Evaluation routine\n",
    "    evaluation_results_df = _evaluate_recommender_by_user_groups(\n",
    "        user_groups          = user_groups,\n",
    "        recommenders         = recommender,\n",
    "        train_interaction_matrix = parameters['train_interaction_matrix'],\n",
    "        test_interaction_matrix  = parameters['test_interaction_matrix'],\n",
    "        U = parameters['U'],\n",
    "        V = parameters['V'],\n",
    "        item_embeddings = parameters['item_embeddings'],\n",
    "        _calculate_user_profile_embedding = parameters['_calculate_user_profile_embedding'],\n",
    "        _compute_aggregated_scores        = parameters['_compute_itemknn_scores'],\n",
    "        topK       = parameters['topK'],\n",
    "        n_neighbors= parameters['n_neighbors']\n",
    "    )\n",
    "\n",
    "    return evaluation_results_df, user_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92155767e987e9",
   "metadata": {},
   "source": [
    "The following Cell will evaluate the implemented recommenders on the given dataset. The evaluation results will be displayed in a DataFrame, showing the average nDCG scores for each recommender across different user groups. This Cell is for you to see how the input looks like. For a correct evaluation, the code below needs to run without errors and the nDCG scores need to be output as described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ea80c6459fd855b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:22:03.053401Z",
     "start_time": "2024-06-18T16:22:02.756233Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Low Interaction Users: 100%|██████████| 5/5 [00:18<00:00,  3.65s/it]\n",
      "Evaluating High Interaction Users: 100%|██████████| 5/5 [00:34<00:00,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users with low interaction levels: 560\n",
      "Number of Users with high interaction levels: 655\n",
      "         User Group    Recommender  Average p@k  Average r@k  Average nDCG\n",
      "0   Low Interaction  Avg_Item_Embd     0.006964     0.042857      0.022220\n",
      "1   Low Interaction     CB_ItemKnn     0.010357     0.058036      0.033966\n",
      "2   Low Interaction            SVD     0.027500     0.187500      0.129047\n",
      "3   Low Interaction     CF_ItemKNN     0.039643     0.269643      0.169657\n",
      "4   Low Interaction         TopPop     0.015893     0.121429      0.069482\n",
      "5  High Interaction  Avg_Item_Embd     0.016794     0.051901      0.036076\n",
      "6  High Interaction     CB_ItemKnn     0.019389     0.063305      0.040686\n",
      "7  High Interaction            SVD     0.056031     0.190630      0.154937\n",
      "8  High Interaction     CF_ItemKNN     0.087328     0.303126      0.236496\n",
      "9  High Interaction         TopPop     0.046870     0.162379      0.115514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define recommenders with correct parameters\n",
    "recommenders = {\n",
    "    'Avg_Item_Embd': average_embedding_similarity_rec,\n",
    "    'CB_ItemKnn': cb_itemknn_recommendation,\n",
    "    'SVD': svd_recommend_to_list,\n",
    "    'CF_ItemKNN': recTopK,\n",
    "    'TopPop': recTopKPop\n",
    "}\n",
    "\n",
    "U, V = svd_decompose(train_interaction_matrix)\n",
    "\n",
    "data = {\n",
    "    'train_interaction_matrix': train_interaction_matrix,\n",
    "    'test_interaction_matrix': test_interaction_matrix,\n",
    "    'U': U,\n",
    "    'V': V,\n",
    "    'item_embeddings': embedding,\n",
    "    '_calculate_user_profile_embedding': calculate_user_profile_embedding,\n",
    "    '_compute_itemknn_scores': compute_itemknn_scores,\n",
    "    'topK': 10,\n",
    "    'n_neighbors': 5}\n",
    "\n",
    "evaluation_results_df, user_groups = evaluate_recommenders(evaluate_recommender_by_user_groups, users, data, recommenders, user_threshold=5)\n",
    "print(f\"Number of Users with low interaction levels: {len(user_groups['Low Interaction'])}\")\n",
    "print(f\"Number of Users with high interaction levels: {len(user_groups['High Interaction'])}\")\n",
    "print(evaluation_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f0ffa9e56e9a7a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T16:22:03.058556Z",
     "start_time": "2024-06-18T16:22:03.054386Z"
    }
   },
   "outputs": [],
   "source": [
    "# The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmwTeam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

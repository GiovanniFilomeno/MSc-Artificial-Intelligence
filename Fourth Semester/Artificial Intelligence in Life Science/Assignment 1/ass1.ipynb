{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:37:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:37:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:37:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:37:47] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "n_bits = 2048\n",
    "def smiles_to_fp(smiles, radius=2, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        # Restituisce un array di zeri con tipo intero, come i bit vector originali\n",
    "        return np.zeros(nBits, dtype=np.uint8)\n",
    "\n",
    "    # 1. Crea un'istanza del Morgan fingerprint generator\n",
    "    fpgen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=nBits)\n",
    "\n",
    "    # 2. Usa il generatore per ottenere l'oggetto fingerprint (ExplicitBitVect)\n",
    "    #    Il metodo corretto è GetFingerprint()\n",
    "    fp = fpgen.GetFingerprint(mol) # <<<--- CORREZIONE QUI\n",
    "\n",
    "    # 3. Converti l'oggetto fingerprint in un NumPy array\n",
    "    #    Questo passaggio rimane uguale e ora funzionerà con l'oggetto 'fp'\n",
    "    arr = np.zeros((nBits,), dtype=np.uint8)\n",
    "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "    return arr\n",
    "\n",
    "df = pd.read_csv(\"data/data_train.csv\")\n",
    "\n",
    "task_cols = [f\"task{i}\" for i in range(1,12)]\n",
    "for c in task_cols:\n",
    "    df[c] = df[c].map({\n",
    "        -1: 0,   # inattivo -> 0\n",
    "        1: 1,    # attivo -> 1\n",
    "        0: np.nan  # unknown -> NaN\n",
    "    })\n",
    "\n",
    "df = df.dropna(subset=task_cols, how='all')\n",
    "X = np.array([smiles_to_fp(s, nBits=n_bits) for s in df[\"smiles\"]], dtype=np.float32)\n",
    "Y = df[task_cols].values.astype(np.float32)                    # shape (N, 11) con 0/1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (9224, 2048)\n",
      "float32 (9224, 11)\n"
     ]
    }
   ],
   "source": [
    "# map_dict = {-1: 0, 0: 1, 1: 2}\n",
    "# df[task_cols] = df[task_cols].replace(map_dict).astype(np.float32)\n",
    "\n",
    "# Y_mapped = df[task_cols].values  # shape (N, 11), dtype float32\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(X_train.dtype, X_train.shape)\n",
    "print(Y_train.dtype, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FingerprintDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X).float()  # shape (N, 2048)\n",
    "        self.Y = torch.from_numpy(Y).float()  # shape (N, 11)\n",
    "        self.n_samples = self.X.shape[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]  # (features, label_vector)\n",
    "\n",
    "dataset = FingerprintDataset(X, Y)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "train_dataset = FingerprintDataset(X_train, Y_train)\n",
    "test_dataset  = FingerprintDataset(X_test, Y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiTaskBinaryNet(nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=256, num_tasks=11, num_layers=2, dropout_prob=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): dimensione dell'input (ad es. fingerprint di 2048)\n",
    "            hidden_dim (int): numero di neuroni nei layer intermedi\n",
    "            num_tasks (int): numero di task / teste in uscita\n",
    "            num_layers (int): quanti layer 'densi' nel blocco condiviso (default 2)\n",
    "            dropout_prob (float): probabilità di dropout (default 0.2)\n",
    "        \"\"\"\n",
    "        super(MultiTaskBinaryNet, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Primo layer: input_dim -> hidden_dim\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout_prob))\n",
    "        \n",
    "        # Aggiungi i layer successivi: hidden_dim -> hidden_dim\n",
    "        # (num_layers-1) volte\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "        \n",
    "        # Raggruppiamo i layer condivisi in un Sequential\n",
    "        self.shared_net = nn.Sequential(*layers)\n",
    "        \n",
    "        # 11 \"testine\" di output, ciascuna 1 neurone (logit)\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, 1) for _ in range(num_tasks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (batch_size, input_dim)\n",
    "        Ritorna una lista di length num_tasks,\n",
    "        ognuna shape (batch_size, 1) => i logit per ogni task\n",
    "        \"\"\"\n",
    "        # Passa attraverso i layer condivisi\n",
    "        x = self.shared_net(x)\n",
    "        \n",
    "        # Calcolo delle 11 uscite\n",
    "        outputs = [head(x) for head in self.heads]\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# useremo 'none' per poi fare la media su tutti i task manualmente\n",
    "\n",
    "def multi_task_bce_loss_masked(outputs, targets):\n",
    "    \"\"\"\n",
    "    outputs: lista di [batch_size,1]\n",
    "    targets: shape (batch_size, num_tasks), con 0,1 o NaN\n",
    "    \"\"\"\n",
    "    bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    losses = []\n",
    "    for i in range(len(outputs)):\n",
    "        logit_i = outputs[i].squeeze(1)      # (batch_size,)\n",
    "        target_i = targets[:, i]            # (batch_size,)\n",
    "        \n",
    "        mask = ~torch.isnan(target_i)\n",
    "        if mask.any():\n",
    "            # BCE solo sui sample dove la label non è NaN\n",
    "            filtered_loss = bce(logit_i[mask], target_i[mask])\n",
    "            losses.append(filtered_loss)\n",
    "    \n",
    "    if len(losses) == 0:\n",
    "        # Nel caso estremo non ci sia nessuna label definita\n",
    "        return torch.tensor(0.0, requires_grad=True, device=outputs[0].device)\n",
    "    \n",
    "    # Concateniamo e facciamo la media su TUTTI i sample di TUTTI i tasks definiti\n",
    "    all_losses = torch.cat(losses, dim=0)\n",
    "    return all_losses.mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Epoch 1/10, Loss: 0.3697\n",
      "Epoch 2/10, Loss: 0.3519\n",
      "Epoch 3/10, Loss: 0.3385\n",
      "Epoch 4/10, Loss: 0.3327\n",
      "Epoch 5/10, Loss: 0.3377\n",
      "Epoch 6/10, Loss: 0.3288\n",
      "Epoch 7/10, Loss: 0.3213\n",
      "Epoch 8/10, Loss: 0.3308\n",
      "Epoch 9/10, Loss: 0.3101\n",
      "Epoch 10/10, Loss: 0.3089\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model = MultiTaskBinaryNet(\n",
    "    input_dim=n_bits,\n",
    "    hidden_dim=256,\n",
    "    num_tasks=11,\n",
    "    num_layers=3\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    sample_count = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(device)  # (batch_size, 2048)\n",
    "        batch_y = batch_y.to(device)  # (batch_size, 11)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(batch_x)  # lista di 11 tensor (batch_size, 1)\n",
    "        loss = multi_task_bce_loss_masked(outputs, batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size_ = batch_x.size(0)\n",
    "        running_loss += loss.item() * batch_size_\n",
    "        sample_count += batch_size_\n",
    "    \n",
    "    epoch_loss = running_loss / sample_count\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3403\n",
      "Task 1 - Accuracy: 91.24%\n",
      "Task 2 - Accuracy: 46.83%\n",
      "Task 3 - Accuracy: 78.03%\n",
      "Task 4 - Accuracy: 60.00%\n",
      "Task 5 - Accuracy: 85.66%\n",
      "Task 6 - Accuracy: 93.15%\n",
      "Task 7 - Accuracy: 95.26%\n",
      "Task 8 - Accuracy: 82.54%\n",
      "Task 9 - Accuracy: 76.30%\n",
      "Task 10 - Accuracy: 96.69%\n",
      "Task 11 - Accuracy: 98.27%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct_counts = [0]*11\n",
    "total_counts = [0]*11\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # outputs = lista di 11 tensori (batch_size, 1) => logit grezzi\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # 1) Calcolo della BCE media su tutti i task\n",
    "        loss = multi_task_bce_loss_masked(outputs, batch_y)\n",
    "        # Se la tua multi_task_bce_loss fa la media (batch e tasks),\n",
    "        # moltiplica per batch_x.size(0) per sommare \"loss * nSample\"\n",
    "        test_loss += loss.item() * batch_x.size(0)\n",
    "        \n",
    "        # 2) Calcolo dell'accuracy per ciascun task\n",
    "        for i in range(11):\n",
    "            logit_i = outputs[i].squeeze(dim=1)   # (batch_size,)\n",
    "            target_i = batch_y[:, i]             # (batch_size,)\n",
    "\n",
    "            # 1) maschera booleana: True dove la label NON è NaN\n",
    "            mask = ~torch.isnan(target_i)\n",
    "\n",
    "            # 2) calcoliamo prob e pred SOLO su quei sample\n",
    "            prob_i  = torch.sigmoid(logit_i[mask])  # (numero_di_sample_validi,)\n",
    "            preds_i = (prob_i >= 0.5).long()         # (numero_di_sample_validi,)\n",
    "\n",
    "            valid_targets = target_i[mask].long()    # 0 o 1\n",
    "\n",
    "            # 3) calcolo accuracy\n",
    "            correct_counts[i] += (preds_i == valid_targets).sum().item()\n",
    "            total_counts[i]   += len(valid_targets)\n",
    "\n",
    "# Calcolo della \"average loss\" sul test set\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Stampo l'accuracy di ciascun task\n",
    "for i in range(11):\n",
    "    acc_i = correct_counts[i] / total_counts[i]\n",
    "    print(f\"Task {i+1} - Accuracy: {acc_i*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Loss: 0.5958\n",
    "# Task 1 - Accuracy: 89.18%\n",
    "# Task 2 - Accuracy: 56.35%\n",
    "# Task 3 - Accuracy: 85.23%\n",
    "# Task 4 - Accuracy: 56.57%\n",
    "# Task 5 - Accuracy: 86.07%\n",
    "# Task 6 - Accuracy: 93.03%\n",
    "# Task 7 - Accuracy: 90.05%\n",
    "# Task 8 - Accuracy: 79.37%\n",
    "# Task 9 - Accuracy: 81.52%\n",
    "# Task 10 - Accuracy: 91.74%\n",
    "# Task 11 - Accuracy: 98.27%\n",
    "\n",
    "\n",
    "# ## ---------\n",
    "# Test Loss: 0.6776\n",
    "# Task 1 - Accuracy: 88.66%\n",
    "# Task 2 - Accuracy: 53.97%\n",
    "# Task 3 - Accuracy: 85.98%\n",
    "# Task 4 - Accuracy: 64.00%\n",
    "# Task 5 - Accuracy: 84.69%\n",
    "# Task 6 - Accuracy: 93.03%\n",
    "# Task 7 - Accuracy: 95.26%\n",
    "# Task 8 - Accuracy: 82.54%\n",
    "# Task 9 - Accuracy: 76.78%\n",
    "# Task 10 - Accuracy: 95.04%\n",
    "# Task 11 - Accuracy: 98.27%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

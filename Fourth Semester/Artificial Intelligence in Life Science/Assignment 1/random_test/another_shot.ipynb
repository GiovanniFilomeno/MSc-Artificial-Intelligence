{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Tuning per task5 ***\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "from rdkit import RDLogger\n",
    "\n",
    "# Imposta il livello di log a ERROR (ignora INFO e WARNING)\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.ERROR)\n",
    "\n",
    "# 1) Leggi dataset, mappa -1->0, +1->1, 0->NaN\n",
    "df = pd.read_csv(\"../data/data_train.csv\")\n",
    "task_cols = [f\"task{i}\" for i in range(1,12)]\n",
    "\n",
    "for c in task_cols:\n",
    "    df[c] = df[c].map({\n",
    "        -1: 0,\n",
    "        1: 1,\n",
    "        0: np.nan\n",
    "    })\n",
    "\n",
    "import numpy as np\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, MACCSkeys\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
    "\n",
    "def standardize_mol(mol):\n",
    "    \"\"\"\n",
    "    Esempio minimal di standardizzazione:\n",
    "    1) Cleanup chimico di base\n",
    "    2) Tieni solo il frammento più grande\n",
    "    (Volendo puoi aggiungere tautomer enumeration, neutralizzazione, etc.)\n",
    "    \"\"\"\n",
    "    # 1) Pulizia di base\n",
    "    mol = rdMolStandardize.Cleanup(mol)\n",
    "    # 2) Tieni solo il frammento principale\n",
    "    lfc = rdMolStandardize.LargestFragmentChooser()\n",
    "    mol = lfc.choose(mol)\n",
    "    return mol\n",
    "\n",
    "def smiles_to_fp(smiles, \n",
    "                 nBits=1024, \n",
    "                 radius=2, \n",
    "                 use_MACCS=False, \n",
    "                 standardize=True):\n",
    "    \"\"\"\n",
    "    Converte uno SMILES in fingerprint Morgan (di default).\n",
    "    Se use_MACCS=True, concatena anche il fingerprint MACCS\n",
    "    Se standardize=True, prova a \"ripulire\" la molecola prima di calcolare il FP\n",
    "    \"\"\"\n",
    "\n",
    "    # Converte da SMILES a Mol\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        # Se la conversione fallisce, restituisci un vettore di zeri\n",
    "        # con dimensione pari a nBits (Morgan) o nBits + 166 se concateni MACCS\n",
    "        maccs_size = 166 if use_MACCS else 0\n",
    "        return np.zeros(nBits + maccs_size, dtype=np.uint8)\n",
    "\n",
    "    # Standardizzazione (opzionale)\n",
    "    if standardize:\n",
    "        mol = standardize_mol(mol)\n",
    "\n",
    "    # Genera fingerprint Morgan (usando rdFingerprintGenerator o AllChem)\n",
    "    fpgen = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=nBits)\n",
    "    fp_morgan = fpgen.GetFingerprint(mol)\n",
    "    arr_morgan = np.zeros((nBits,), dtype=np.uint8)\n",
    "    DataStructs.ConvertToNumpyArray(fp_morgan, arr_morgan)\n",
    "\n",
    "    if not use_MACCS:\n",
    "        # Ritorna solo Morgan\n",
    "        return arr_morgan\n",
    "\n",
    "    # Altrimenti, calcola MACCS e concatena\n",
    "    maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "    arr_maccs = np.zeros((maccs_fp.GetNumBits(),), dtype=np.uint8)\n",
    "    DataStructs.ConvertToNumpyArray(maccs_fp, arr_maccs)\n",
    "\n",
    "    # Concatenazione Morgan + MACCS\n",
    "    combined_fp = np.concatenate([arr_morgan, arr_maccs])\n",
    "    return combined_fp\n",
    "\n",
    "\n",
    "# 3) Costruisci fingerprint per tutti\n",
    "X = np.array([smiles_to_fp(s, \n",
    "                           nBits=1024, \n",
    "                           radius=2, \n",
    "                           use_MACCS=True, \n",
    "                           standardize=True) \n",
    "              for s in df[\"smiles\"]],\n",
    "             dtype=np.float32)\n",
    "\n",
    "# 4) Unico split train/test\n",
    "df_train, df_test, X_train, X_test = train_test_split(df, X, test_size=0.05, random_state=42)\n",
    "\n",
    "# Parametri di esempio per la ricerca\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [None, 5, 10]\n",
    "#     # puoi aggiungere 'min_samples_split': [2, 5] ecc.\n",
    "# }\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [5, 10, 20, 50, 100, 200, 300, 500, 700, 1000],  # Numero di alberi nella foresta\n",
    "#     'criterion': ['gini', 'entropy'],         # Funzione per misurare la qualità di uno split (per classificazione)\n",
    "#                                               # Se fai regressione, usa: ['squared_error', 'absolute_error']\n",
    "#     'max_depth': [None, 1, 3, 5, 10, 20, 22, 27, 30],       # Massima profondità di ciascun albero (None = nodi espansi finché puri o min_samples_split)\n",
    "#     'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],          # Numero minimo di campioni richiesti per splittare un nodo interno\n",
    "#     'min_samples_leaf': [1, 3, 5, 6, 7],           # Numero minimo di campioni richiesti in un nodo foglia\n",
    "#     'max_features': ['sqrt', 'log2', 0.5, 0.7],\n",
    "#     'class_weight': [None, 'balanced'] \n",
    "# }\n",
    "\n",
    "search_spaces = {\n",
    "    'n_estimators': Integer(5, 2500),  # Range intero (limiti inclusi)\n",
    "    'criterion': Categorical(['gini', 'entropy']), # Valori discreti\n",
    "    'max_depth': Categorical([None, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35]), # Valori discreti, incluso None e range affinato\n",
    "    'min_samples_split': Integer(2, 15), # Range intero\n",
    "    'min_samples_leaf': Integer(1, 10),  # Range intero\n",
    "    'max_features': Categorical(['sqrt', 'log2', 0.5, 0.7]), # Valori discreti (manteniamo questo semplice per ora)\n",
    "    # Nota: Si potrebbe usare Real(0.1, 1.0) per max_features, ma gestire misto float/string è più complesso.\n",
    "    # Mantenere Categorical con i valori che hanno funzionato è ragionevole.\n",
    "    'class_weight': Categorical([None, 'balanced']) # Valori discreti\n",
    "}\n",
    "\n",
    "auc_list = []\n",
    "\n",
    "for i, task in enumerate(task_cols[4:5], start=5):\n",
    "    print(f\"\\n*** Tuning per {task} ***\")\n",
    "    # a) Seleziona i sample definiti per training\n",
    "    train_mask = ~df_train[task].isna()\n",
    "    X_train_f = X_train[train_mask]\n",
    "    y_train_f = df_train.loc[train_mask, task].values.astype(int)\n",
    "\n",
    "    if len(y_train_f) == 0:\n",
    "        print(f\"Nessun sample train per {task}, skip.\")\n",
    "        continue\n",
    "\n",
    "    # b) Esegui una Grid Search\n",
    "    rf = RandomForestClassifier(random_state=0)  \n",
    "    # grid_search = GridSearchCV(\n",
    "    #     estimator=rf,\n",
    "    #     param_grid=param_grid,\n",
    "    #     scoring='roc_auc', # ottimizziamo AUC\n",
    "    #     cv=3,              # 3-fold cross validation\n",
    "    #     n_jobs=-1\n",
    "    # )\n",
    "\n",
    "    # grid_search = RandomizedSearchCV(\n",
    "    #     estimator=rf,\n",
    "    #     param_distributions=param_grid, # Nota: per RandomizedSearchCV si chiama param_distributions\n",
    "    #     n_iter=1000,                     # <<< AGGIUNGI QUESTO! Numero di combinazioni da provare\n",
    "    #     scoring='roc_auc',\n",
    "    #     cv=5,\n",
    "    #     n_jobs=-1,\n",
    "    #     random_state=42 # Aggiungi per riproducibilità della ricerca casuale\n",
    "    # )\n",
    "    grid_search = BayesSearchCV(\n",
    "        estimator=rf,\n",
    "        search_spaces=search_spaces, # Usa gli spazi definiti sopra\n",
    "        n_iter=100,       # Numero di combinazioni da provare\n",
    "        scoring='roc_auc',\n",
    "        cv=5,                      # CONSIGLIATO: 5-fold CV\n",
    "        n_jobs=-1,\n",
    "        random_state=42            # Per riproducibilità della ricerca\n",
    "    )\n",
    "    grid_search.fit(X_train_f, y_train_f)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Best params for {task}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Salvataggio del modello\n",
    "    dump(best_model, f\"rf_task{i}.joblib\")\n",
    "    print(f\"Salvato: rf_task{i}.joblib\")\n",
    "\n",
    "    # c) Valutazione su test\n",
    "    test_mask = ~df_test[task].isna()\n",
    "    X_test_f = X_test[test_mask]\n",
    "    y_test_f = df_test.loc[test_mask, task].values.astype(int)\n",
    "\n",
    "    if len(y_test_f) == 0:\n",
    "        print(f\"Nessun sample test per {task}, skip.\")\n",
    "        continue\n",
    "\n",
    "    # Predici prob\n",
    "    y_proba = best_model.predict_proba(X_test_f)[:, 1]  # output continuo\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "    precision = precision_score(y_test_f, y_pred, zero_division=0)\n",
    "    if len(np.unique(y_test_f)) == 2:\n",
    "        auc_val = roc_auc_score(y_test_f, y_proba)\n",
    "    else:\n",
    "        auc_val = np.nan\n",
    "\n",
    "    print(f\"{task} -> Precision={precision:.3f}, AUC={auc_val if not np.isnan(auc_val) else 'N/A'}\")\n",
    "    auc_list.append(auc_val)\n",
    "\n",
    "valid_aucs = [x for x in auc_list if not np.isnan(x)]\n",
    "mean_auc = np.mean(valid_aucs) if valid_aucs else np.nan\n",
    "print(f\"\\nAUC Media sui task: {mean_auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

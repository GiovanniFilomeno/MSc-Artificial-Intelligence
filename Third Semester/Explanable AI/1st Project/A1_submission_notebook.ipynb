{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7BhPR-nzsRW"
   },
   "source": [
    "# Exploration of Projection Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhsjOTkrzsRZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Matplotlib inline configuration for Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scipy and Scikit-Learn Libraries\n",
    "from scipy import interpolate\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "# Dimensionality Reduction and Clustering Libraries\n",
    "from openTSNE import TSNE\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.offsetbox import OffsetImage\n",
    "import altair as alt\n",
    "from altair import datum\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "# Import custom module\n",
    "from utils import CliffWalkingVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBe0cTuizsRa"
   },
   "source": [
    "## Data\n",
    "\n",
    "In this project, we explore a classic reinforcement learning problem called **Cliff Walking**, where the objective is to navigate a grid world from a starting state to a target destination while avoiding the treacherous \"cliff\" along the way. This environment, provided by the popular gym library, presents a challenging landscape for learning algorithms. Stepping off the cliff leads in a steep penalty, requiring intelligent decision-making to find optimal paths.\n",
    "\n",
    "To investigate different strategies in this task, we compare the behaviors of four distinct algorithms: a baseline random policy, SARSA with an epsilon-greedy policy, Q-learning with an epsilon-greedy policy, and Expected SARSA with an epsilon-greedy policy. Each algorithm was trained over 5000 episodes, allowing us to analyze convergence patterns and path efficiency.\n",
    "\n",
    "We thought the Cliff Walking dataset is well-suited for down-projection of learning trajectories for several reasons:\n",
    "\n",
    "1. **Grid Structure**: The finite, discrete nature of the grid world simplifies state representation, making it easier to visualize and project trajectories.\n",
    "\n",
    "2. **Convergence Patterns**: As agents learn, their trajectories stabilize, revealing patterns of convergence. Down-projection highlights these trends, showcasing how algorithms adapt their policies to avoid the cliff.\n",
    "\n",
    "3. **Natural Clusters**: The environment's distinct regions—cliff areas, safe zones, and the goal—create interpretable clusters in state space. Down-projected trajectories make it easier to identify decision points and areas of difficulty.\n",
    "\n",
    "4. **Algorithm Comparison**: By projecting trajectories into lower dimensions, we can visualize and compare strategies across different algorithms.\n",
    "\n",
    "5. **High-Revisit States**: States near the cliff are frequently revisited, creating denser regions that reflect the strategic decision-making of the agents.\n",
    "\n",
    "Overall, the structured environment and dynamic learning behaviors make the Cliff Walking dataset an excellent candidate for visualizing learning trajectories through down-projection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "9_SCu2djzsRa",
    "outputId": "6eff30bd-73d5-4ec8-9d3f-c8b0727a4db0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the visualizer:\n",
    "visualizer = CliffWalkingVisualizer()\n",
    "\n",
    "# Visualize the grid world with the agent in a specified state, e.g., 36:\n",
    "visualizer.visualize_grid_world(agent_state=36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the grid world. The blue state reflects the starting state while the green state corresponds to the goal state. The agent needs to reach this goal state. With every step it takes, it gets a reward of -1. Thus, it is encouraged to take as little steps as possible. However, when the agent falls of the cliff (depicted in grey), it gets punishment of -100. Due to the exploration behaviour of the policies, some random actions are also taken which might cause the agent to fall off the cliff when it is too close. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZGP05eTzsRb"
   },
   "source": [
    "### Read and Prepare Data\n",
    "Read in your data from a file or create your own data.\n",
    "\n",
    "Document any data processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0hxybVyzsRb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load trajectory data for each algorithm\n",
    "with open('data/cliff_walking/expected_sarsa.npy', 'rb') as f:\n",
    "    expected_sarsa = np.load(f, allow_pickle=True)\n",
    "with open('data/cliff_walking/q_learning.npy', 'rb') as f:\n",
    "    q_learning = np.load(f, allow_pickle=True)\n",
    "with open('data/cliff_walking/random.npy', 'rb') as f:\n",
    "    random = np.load(f, allow_pickle=True)\n",
    "with open('data/cliff_walking/sarsa.npy', 'rb') as f:\n",
    "    sarsa = np.load(f, allow_pickle=True)\n",
    "\n",
    "# Set algorithm names and pair with corresponding trajectory data\n",
    "algorithms = [\"Expected SARSA\", \"Q Learning\", \"RANDOM\", \"SARSA\"]\n",
    "trajectories_algos = [expected_sarsa, q_learning, random, sarsa]\n",
    "\n",
    "# Initialize list to store episode data for DataFrame\n",
    "data = []\n",
    "\n",
    "# Process trajectories for each algorithm\n",
    "for trajectories_algo, algo in zip(trajectories_algos, algorithms):\n",
    "    for episode_index, trajectory in enumerate(trajectories_algo):\n",
    "        # Sample every fifth episode\n",
    "        if episode_index % 5 == 0:\n",
    "            episode_length = len(trajectory)\n",
    "\n",
    "            for step_index, step in enumerate(trajectory):\n",
    "                state, action, reward, next_state, done = step\n",
    "\n",
    "                # Label step position within the episode\n",
    "                if step_index == 0:\n",
    "                    cp = 'start'\n",
    "                elif step_index == episode_length - 1:\n",
    "                    cp = 'end'\n",
    "                else:\n",
    "                    cp = 'intermediate'\n",
    "\n",
    "                # Append episode step data to list\n",
    "                data.append({\n",
    "                    'line': episode_index,\n",
    "                    'cp': cp,\n",
    "                    'algorithm': algo,\n",
    "                    'state': state\n",
    "                    #'action': action,\n",
    "                    #'reward': reward,\n",
    "                    #'next_state': next_state\n",
    "                })\n",
    "\n",
    "# Create DataFrame with all episodes' data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate metadata and projection data\n",
    "meta_data = df.iloc[:, :3]\n",
    "proj_data = df.iloc[:, 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "arnDRtlmzsRc",
    "outputId": "c06e6397-0ee7-492a-cebb-540ea4becb9c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert 'state' column in proj_data to one-hot encoding\n",
    "one_hot_df = pd.get_dummies(proj_data, columns=['state'], prefix=['state'])\n",
    "\n",
    "one_hot_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D5PkbGtzsRc"
   },
   "source": [
    "### Comments\n",
    "- Did you transform, clean, or extend the data?  How/Why?\n",
    "  \n",
    "  For each algorithm, the dataset is organized as lists of episodes, where each episode consists of dicts containing (state, action, reward, next_state, done). We filtered the dataset by taking every fifth episode from each algorithm, resulting in a sample of 1,000 episodes per algorithm. This downsampling aims to prevent overcrowding in plots, allowing for clearer visual analysis.\n",
    "\n",
    "  To simplify and focus our analysis, we used only the state information in the downprojection, as other data (such as actions and rewards) are indirectly represented by the trajectory structure, reducing redundancy while still capturing essential dynamics.\n",
    "\n",
    "  Additionally, defining the state-space representation is critical for effective analysis and visualization. Here, we applied a one-hot encoding to the states to maintain interpretability and ensure that each unique state has a distinct representation. This encoding allows us to downproject the states in a way that retains their unique properties and relationships, supporting a more nuanced understanding of the agent’s movement through the state space and aiding in the clustering and comparison of trajectory patterns across algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPLoUFiCzsRd"
   },
   "source": [
    "## Projection\n",
    "Project your data into a 2D space.\n",
    "Try multiple (3+) projection methods (e.g., t-SNE, UMAP, MDS, PCA, ICA, other methods) with different settings and compare them.\n",
    "\n",
    "Make sure that all additional dependencies are included when submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHX46QQmzsRd"
   },
   "source": [
    "### Comments\n",
    "- Which features did you use? Why?\n",
    "\n",
    "  We selected only the 48 discrete states of our Cliff Walking environment as features. For each episode, our reinforcement learning agent occupied a current state, chose an action that led to a next state, and received a reward. We used only the current state as our primary feature for trajectory data, as other information can be inferred implicitly by observing these trajectories. The constant reward of -1 (except when the agent reaches a cliff) signifies episode termination, while the action can be deduced from state transitions. This approach simplifies the data without compromising information needed for trajectory analysis.\n",
    "\n",
    "- Which projection methods did you use? Why?\n",
    "\n",
    "  To investigate both global and local structures within the trajectory data, we applied three projection techniques with distinct characteristics: **t-SNE**, **PCA**, and **UMAP**. Each method offers unique benefits and limitations, which we explored to understand their suitability and behavior on our dataset.\n",
    "\n",
    "  **t-SNE (t-distributed Stochastic Neighbor Embedding)**  \n",
    "   - t-SNE is a nonlinear dimensionality reduction method that is well-suited to data that are not linearly separable. It preserves local structures effectively, making it useful for analyzing clusters of closely related states in trajectories.\n",
    "   - It is computationally intensive, as it calculates pairwise similarities between points, and often struggles to retain global relationships, focusing instead on preserving the neighborhood structure.\n",
    "   - Its output can vary between runs due to non-deterministic initialization, and its axes lack interpretability.\n",
    "   - We chose t-SNE to observe fine-grained local structures within state clusters, despite its limitations in representing broader patterns across clusters.\n",
    "   - **Hyperparameters**: We experimented with various perplexity values to adjust the balance between local and global structures. We calculated t-sne coordinates for the following perplexities [5, 10, 30, 50, 100, 500, 1000] and choose to plot the one that appears in this particular run to best depict the different tracetories. <br><br>\n",
    "\n",
    "  **PCA (Principal Component Analysis)**  \n",
    "   - PCA is a linear dimensionality reduction method that maintains both global and local relationships in the data and is computationally efficient.\n",
    "   - It is ideal for identifying principal directions of variance in high-dimensional data, offering interpretable axes that represent maximum variance directions.\n",
    "   - Though it may miss non-linear structures, PCA effectively retains global structure, making it useful as a baseline for understanding overall patterns in trajectories.\n",
    "   - **Hyperparameters**: We retained enough principal components to capture a substantial proportion of variance, ensuring that the representation was compact but informative. <br><br>\n",
    "\n",
    "  **UMAP (Uniform Manifold Approximation and Projection)**  \n",
    "   - UMAP is a non-linear technique that provides a balance between preserving local and global structures, offering both interpretable clusters and an approximate overview of overall relationships.\n",
    "   - Its flexibility in retaining both neighborhood structures and larger patterns makes it highly suitable for our data, as it may reveal both fine-grained trajectory clusters and broader state-space patterns.\n",
    "   - **Hyperparameters**: We optimized parameters for minimum distance and number of neighbors to adjust the level of detail in clustering and neighborhood preservation. <br><br>\n",
    "\n",
    "   **ICA (Independent Component Analysis)**\n",
    "    - ICA is a downprojection method that finds statistically independent components in the data. These independent components can then reveal underlying patterns.\n",
    "    - It can produce components that are interpretable, especially if the original high-dimensional data were mixtures of independent sources.\n",
    "    - It operates under the assumption that the source signals are non-Gaussian. This makes ICA well-suited for data that do not adhere to Gaussian distributions, however it may struggle to separate data effectively if this assumption is not fullfilled.\n",
    "    - It additionally assumes that the sources are mixed linearly, which may not always be the case. If the sources are mixed nonlinearly, ICA may not be effective.\n",
    "    - **Hyperparameters**: We optimized the number of components to capture the main independent patterns in trajectory data while ensuring meaningful downprojection. <br> <br>\n",
    "\n",
    "- Why did you choose these hyperparameters?\n",
    "\n",
    "  Each method’s hyperparameters were selected to leverage its strengths: **t-SNE** focused on neighborhood structure, **PCA** maximized variance capture, **UMAP** balanced local/global structures, and **ICA** isolated independent components. These choices were aligned with our goals of uncovering detailed clusters and overall patterns in the state space.\n",
    "\n",
    "- Are there patterns in the global _and_ the local structure?\n",
    "\n",
    "  See the detailed analysis at the end of each method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4aNHzjKzsRd"
   },
   "source": [
    "#### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOxJQLDMzsRe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# tsne_coords = manifold.TSNE(perplexity=p).fit_transform(one_hot_df)\n",
    "# tsne_coords_perplexities.append(tsne_coords)\n",
    "t_sne_perplexities = np.load('data/cliff_walking/tsne_coords_perplexities.npy', allow_pickle=True)[6] # perplexities 5, 10, 30, 50, 100, 500, 1000\n",
    "df_coords = pd.DataFrame(t_sne_perplexities, columns=['X','Y'])\n",
    "plotting_df = pd.concat([meta_data, df_coords], axis='columns')\n",
    "plotting_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKVJdXDNzsRe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "alt.Chart(plotting_df).mark_point(\n",
    "    opacity=0.6\n",
    ").encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    color='cp:N'\n",
    ").transform_filter((datum.cp=='start') | (datum.cp=='end')\n",
    ").properties(\n",
    "    width=250,\n",
    "    height=250,\n",
    "    title=\"TSNE - Start & End States\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMtFSdjzzsRf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "alt.Chart(plotting_df).mark_circle(\n",
    "    opacity=0.6\n",
    ").encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=400,\n",
    "    title=\"TSNE - Projected Data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUUXEBxCzsRf"
   },
   "source": [
    "**Observation t-SNE:** <br>\n",
    "In our dataset, we have many duplicate states, such as a single starting state for all trajectories and a limited set of possible end states. The first plot demonstrates an interesting property of t-SNE: because it’s an iterative optimization process applied to all points simultaneously, identical states are not guaranteed to be mapped to the exact same coordinates. Instead, while these points may cluster close to one another, they may still have slight variations in position. This effect is particularly noticeable for the starting states (orange points), where multiple $(x, y)$ coordinates represent the exact same state after downprojection.\n",
    "\n",
    "In the second plot, we observe a greater variety of projected points than the actual 48 unique states, indicating that a single state is represented by several different t-SNE projections. This is likely due to the sensitivity of t-SNE to local variations, causing identical states to be spread slightly in the projection space.\n",
    "\n",
    "**Are there patterns in the global and the local structure?** <br>\n",
    "Larger global structures are not readily discernible from these t-SNE projections. However, we can identify some smaller clusters. The darker points, where opacity has increased due to overlapping projections, suggest areas with higher revisit frequencies in the original trajectories. These smaller clusters could correspond to specific states among the 48 unique ones, possibly indicating areas of strategic importance or high traffic in the grid (e.g., states near the cliff or around the goal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiAF56V7zsRf"
   },
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4ZlAV75zsRg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting the parameters for UMAP\n",
    "umap_model = UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "\n",
    "# Execute the UMAP on the dataframe with one-hot encoding\n",
    "umap_coords = umap_model.fit_transform(one_hot_df)\n",
    "df_umap_coords = pd.DataFrame(umap_coords, columns=['X', 'Y'])\n",
    "\n",
    "# Combine the metadata with the UMAP coordinates\n",
    "umap_plotting_df = pd.concat([meta_data, df_umap_coords], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjNkDFtgzsRg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Graph for UMAP showing the starting and final points\n",
    "alt.Chart(umap_plotting_df).mark_point(opacity=0.6).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    color='cp:N'\n",
    ").transform_filter(\n",
    "    (datum.cp == 'start') | (datum.cp == 'end')\n",
    ").properties(\n",
    "    width=250,\n",
    "    height=250,\n",
    "    title=\"UMAP - Start & End States\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssuzHlkgzsRg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Graph UMAP of the whole projected dataset\n",
    "alt.Chart(umap_plotting_df).mark_circle(\n",
    "    opacity=0.6\n",
    ").encode(\n",
    "    x='X',\n",
    "    y='Y'\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=400,\n",
    "    title=\"UMAP - Projected Data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QB-eV7HzsRg"
   },
   "source": [
    "**Observation UMAP:** <br>\n",
    "\n",
    "In the UMAP projection, we also observe that identical states do not always yield identical downprojected coordinates. The starting state, for instance, is represented with slight variations in the downprojected space, as are some of the goal states. Despite this variability, UMAP generally keeps similar states close to each other, though not perfectly identical in position. This can be attributed to the fact that UMAP does not preserve exact distances for every instance, focusing instead on broader neighborhood structures. The total number of points in the plot does not match the expected count of $4 \\text{ algorithms} \\times 1000 \\text{ episodes} \\times \\text{number of states per episode}$, indicating that some identical states were mapped very closely or possibly overlapped in the projection. <br> <br>\n",
    "\n",
    "**Are there patterns in the global and the local structure?** <br>\n",
    "UMAP reveals many small clusters, likely corresponding to specific state groups. However, no significant global structures are apparent, aside from a prominent cluster for the starting point. The clustering effect suggests local consistency, but UMAP does not display an overarching pattern across the state space, aligning with its emphasis on neighborhood preservation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGkbNQ99zsRh"
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxCpgpUzzsRj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining PCA over 2 components on the one-hot encoded data\n",
    "pca = PCA(n_components=2)\n",
    "# Project the points on the new coordinate\n",
    "pca_coords = pca.fit_transform(one_hot_df)\n",
    "\n",
    "# Create a DataFrame with the new coordinates\n",
    "df_coords = pd.DataFrame(pca_coords, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Combine PCA coordinates with metadata\n",
    "pca_plotting_df = pd.concat([meta_data.reset_index(drop=True), df_coords], axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2UAJlNMzsRk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting start and end states\n",
    "alt.Chart(pca_plotting_df).mark_point(opacity=0.6).encode(\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='cp:N'\n",
    ").transform_filter(\n",
    "    (datum.cp == 'start') | (datum.cp == 'end')\n",
    ").properties(\n",
    "    width=250,\n",
    "    height=250,\n",
    "    title=\"PCA - Start & End States\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzNuu-ZzzsRk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "alt.Chart(pca_plotting_df).mark_circle(opacity=0.6).encode(\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=400,\n",
    "    title=\"PCA - Projected Data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV2iH0QzzsRk"
   },
   "source": [
    "**Observation PCA:** <br>\n",
    "\n",
    "In the PCA projection, we observe that the starting state is consistently encoded as a single coordinate, maintaining its position across projections. Since PCA preserves meaningful distances, we can see that most of the end states are closely grouped, with one exception that is slightly farther apart. Overall, the projected space is sparse, with only a few distinct points visible. The intermediate states, located in the upper left corner, are notably separated from the start and end clusters. Interestingly, these intermediate states appear clustered tightly, almost as if represented by a single point, rather than being widely spread out. This compactness suggests that PCA has effectively captured the main variance directions but may have merged some states due to the linear projection.\n",
    "\n",
    "\n",
    "**Are there patterns in the global and the local structure?** <br>\n",
    "\n",
    "The PCA projection reveals a clearer separation of global structure compared to other methods. We see distinct clusters for the starting state, end states, and intermediate states, with meaningful distances between them. While PCA may not capture intricate non-linear relationships, it provides a useful overview of the dataset's main structure, allowing us to identify key clusters with significant separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2yGE-t0zsRk"
   },
   "source": [
    "### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9R5rFJHgzsRk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ICA transformation\n",
    "ica = FastICA(n_components=2, random_state=0)\n",
    "ica_coords = ica.fit_transform(one_hot_df)\n",
    "ica_plotting_df = pd.concat([meta_data, pd.DataFrame(ica_coords, columns=['X', 'Y'])], axis='columns')\n",
    "ica_plotting_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSShCQ02zsRl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "alt.Chart(ica_plotting_df).mark_point(\n",
    "    opacity=0.6\n",
    ").encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    color='cp:N'\n",
    ").transform_filter((datum.cp == 'start') | (datum.cp == 'end')\n",
    ").properties(\n",
    "    width=250,\n",
    "    height=250,\n",
    "    title=\"ICA - Start & End States\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAt5ATN9zsRl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "alt.Chart(ica_plotting_df).mark_circle(\n",
    "    opacity=0.6\n",
    ").encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=400,\n",
    "    title=\"ICA - Projected Data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation ICA:** <br>\n",
    "\n",
    "In contrast to t-SNE and UMAP, and similar to PCA, ICA consistently maps both the starting state and goal states to specific, stable component values. The data in the ICA projection appears more sparsely distributed than in t-SNE, reflecting ICA’s focus on finding independent components rather than preserving local neighborhoods or density.\n",
    "\n",
    "**Are there patterns in the global and the local structure?** <br>\n",
    "\n",
    "Similar to PCA, we observe three main clusters in the ICA projection. Two of these clusters contain only a single $(X, Y)$ component value, while the third cluster consists of multiple distinct component values, suggesting a range of variations within this group. This structure indicates that ICA has identified three primary independent patterns in the data, with two relatively stable clusters and one more variable cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyeZn8opzsRl"
   },
   "source": [
    "### Similarities and Differences of Downprojection Methods\n",
    "Overall, t-SNE and UMAP show similar behavior, with multiple encodings for identical states, resulting in a denser representation with more scattered points. In contrast, PCA and ICA produce sparser projections, consistently mapping each state to a single coordinate. Both PCA and ICA exhibit similar downprojection patterns: a single point representing the starting state, a compact cluster of end states, and a single, tightly grouped cluster (or point) for intermediate states. This similarity reflects the linear and independent component focus of PCA and ICA, respectively, in contrast to the neighborhood-preserving nature of t-SNE and UMAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDcquUR4zsRl"
   },
   "source": [
    "### Link States\n",
    "Connect the states that belong together.\n",
    "\n",
    "The states of a single solution should be connected to see the path from the start to the end state.\n",
    "How the points are connected is up to you, for example, with straight lines or splines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9xtZQHUzsRl"
   },
   "source": [
    "#### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abXtWrErzsRl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to plot splines\n",
    "def plot_df_splines(ax, df, x_col='X', y_col='Y', color='blue', alpha=0.3, smoothing=0, n_points=300):\n",
    "    x = df[x_col].values\n",
    "    y = df[y_col].values\n",
    "    if len(x) >= 4 and len(np.unique(x)) >= 4 and len(np.unique(y)) >= 4:\n",
    "        try:\n",
    "            # Prepare the B-spline representation of an N-D curve\n",
    "            tck, u = interpolate.splprep([x, y], s=smoothing)\n",
    "            # Evaluate the values of the spline function at specific points\n",
    "            x_new, y_new = interpolate.splev(np.linspace(0, 1, n_points), tck, der=0)\n",
    "            ax.plot(x_new, y_new, color=color, alpha=alpha)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            # Optionally, print the error message for debugging\n",
    "            # print(f\"Error while creating spline: {e}\")\n",
    "    # else:\n",
    "        # Optionally, print a message if there are not enough points\n",
    "        # print(\"Not enough unique points to create a spline.\")\n",
    "\n",
    "\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Get unique algorithms\n",
    "algorithms = plotting_df['algorithm'].unique()\n",
    "\n",
    "# Loop through each unique algorithm\n",
    "for i, algo in enumerate(algorithms):\n",
    "    algo_data = plotting_df[plotting_df['algorithm'] == algo]\n",
    "    lines = algo_data['line'].unique()\n",
    "\n",
    "    for line in lines:\n",
    "        line_data = algo_data[algo_data['line'] == line]\n",
    "        plot_df_splines(ax, line_data, alpha=0.1, smoothing=0, n_points=9999)\n",
    "\n",
    "ax.set_title(\"TSNE - Dataset Trajectories\")\n",
    "margin = 100\n",
    "plt.xlim(plotting_df['X'].min() - margin, plotting_df['X'].max() + margin)\n",
    "plt.ylim(plotting_df['Y'].min() - margin, plotting_df['Y'].max() + margin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4r-XZt8KzsRl"
   },
   "source": [
    "**Observation t-SNE:** <br>\n",
    "Without additional color encodings or labels, it’s challenging to discern precise patterns in the plot. However, this view highlights some global structures more clearly. Certain paths appear to be more frequently traversed, while others are less common, suggesting preferred routes in the state space. Additionally, there are visible \"knots\" or clusters where multiple trajectories converge, indicating encoded states that many algorithms pass through repeatedly. These high-traffic areas may represent critical decision points or commonly revisited states near the cliff or goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yr590ksMzsRm"
   },
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "913mNaWpzsRm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluating percentile for X and Y to avoid outlayers\n",
    "\n",
    "x_min, x_max = umap_plotting_df['X'].quantile([0.05, 0.95]) \n",
    "y_min, y_max = umap_plotting_df['Y'].quantile([0.05, 0.95])\n",
    "\n",
    "# Creating plot \n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "for i, algo in enumerate(algorithms):\n",
    "    algo_data = umap_plotting_df[umap_plotting_df['algorithm'] == algo]\n",
    "    lines = algo_data['line'].unique()\n",
    "\n",
    "    # Sampling of lines to enhance the clarity\n",
    "    sampled_lines = np.random.choice(lines, size=int(len(lines) * 0.1), replace=False)  # sampling 10%\n",
    "\n",
    "    for line in sampled_lines:\n",
    "        line_data = algo_data[algo_data['line'] == line]\n",
    "        plot_df_splines(ax, line_data, alpha=0.1, smoothing=0, n_points=1000)\n",
    "\n",
    "ax.set_title(\"UMAP - Dataset Trajectories (Zoomed)\")\n",
    "margin = 8\n",
    "plt.xlim(x_min - margin, x_max + margin)\n",
    "plt.ylim(y_min - margin, y_max + margin)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation UMAP:** <br>\n",
    "In the UMAP projection, we observe numerous trajectory \"bundles,\" with varying thicknesses. Thicker bundles indicate frequently occurring paths, suggesting that certain trajectories are revisited often. This pattern may imply convergence among the algorithms, as stable policies produce consistent paths through the state space. However, without additional encodings or labels, further interpretation of specific states or transitions is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5NqD8aezsRm"
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTgnvDHIzsRm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Get unique algorithms\n",
    "algorithms = pca_plotting_df['algorithm'].unique()\n",
    "\n",
    "# Loop through each unique algorithm\n",
    "for i, algo in enumerate(algorithms):\n",
    "    algo_data = pca_plotting_df[pca_plotting_df['algorithm'] == algo]\n",
    "    lines = algo_data['line'].unique()\n",
    "\n",
    "    for line in lines:\n",
    "        line_data = algo_data[algo_data['line'] == line]\n",
    "        plot_df_splines(ax, line_data, x_col='PC1', y_col='PC2', alpha=0.1, smoothing=0, n_points=9999)\n",
    "\n",
    "ax.set_title(\"PCA - Dataset Trajectories\", fontsize=20)\n",
    "margin = 1.2  # Adjusted margin for PCA data\n",
    "plt.xlim(pca_plotting_df['PC1'].min() - margin, pca_plotting_df['PC1'].max() + margin)\n",
    "plt.ylim(pca_plotting_df['PC2'].min() - margin, pca_plotting_df['PC2'].max() + margin)\n",
    "plt.xlabel('PC1', fontsize=16)\n",
    "plt.ylabel('PC2', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation PCA:** <br>\n",
    "The PCA plot appears very different from the t-SNE and UMAP projections. Here, states are not spread out, and identical states are encoded consistently, resulting in more distinct trajectories. The plot reveals three main clusters, which align with our previous observations: the starting state, intermediate states, and end states. All algorithms seem to pass through these key regions, suggesting that PCA has preserved the principal structure of the trajectories while reducing complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUb4asDMzsRm"
   },
   "source": [
    "### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5DG9uvuzsRn"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "algorithms = ica_plotting_df['algorithm'].unique()\n",
    "\n",
    "for i, algo in enumerate(algorithms):\n",
    "    algo_data = ica_plotting_df[ica_plotting_df['algorithm'] == algo]\n",
    "    lines = algo_data['line'].unique()\n",
    "\n",
    "    for line in lines:\n",
    "        line_data = algo_data[algo_data['line'] == line]\n",
    "        plot_df_splines(ax, line_data, alpha=0.1, smoothing=0, n_points=9999)\n",
    "\n",
    "ax.set_title(\"ICA - Dataset Trajectories\")\n",
    "margin = 3\n",
    "plt.xlim(ica_plotting_df['X'].min() - margin, ica_plotting_df['X'].max() + margin)\n",
    "plt.ylim(ica_plotting_df['Y'].min() - margin, ica_plotting_df['Y'].max() + margin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation ICA:** <br>\n",
    "As with PCA, the ICA plot reveals three key regions where all trajectories converge, indicating important areas in the state space. Without additional color encodings or labels, it’s challenging to interpret specific patterns, but the resemblance to the PCA plot is noticeable. This similarity suggests that ICA, like PCA, captures the main structural components of the trajectories while maintaining consistent encoding for identical states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCwFSdp8zsRn",
    "tags": []
   },
   "source": [
    "### Meta Data Encoding\n",
    "Encode addtional features in the visualization.\n",
    "\n",
    "Use features of the source data and include them in the projection, e.g., by using color, opacity, different shapes, or line styles, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXZZRtOizsRn"
   },
   "source": [
    "### Comments\n",
    "- Which features did you use? Why?\n",
    "- How are the features encoded?\n",
    "\n",
    "_TODO_ <br>\n",
    "We tried out various features for our downprojection methods to illustrate a variety of approaches. <br>\n",
    "We worked with different color encodings to mark the 4 different algorithms. In addition, we encoded start, end, and intermediate steps b varying the shapes of the scattered points. Intermediate points are smaller and filled (to avoid overplotting) and start and end points get bigger shapes (circles and squares).\n",
    "\n",
    "In the trajectory density cluster plots, each trajectory line represents the sequence of states visited by an agent under one of the four algorithms in the Cliff Walking environment. The following elements enhance the interpretability of recurring paths and high-density clusters:\n",
    "\n",
    "1. **Density-Based Cluster Highlights**:\n",
    "   - States that are frequently visited across different trajectories are identified as clusters using a density-based approach, specifically the DBSCAN algorithm. \n",
    "   - These clusters are marked by enlarged, semi-transparent circles to represent the state density—larger and more opaque markers indicate a higher frequency of visits. \n",
    "   - Text labels annotate each cluster with the respective state number and scale in size according to cluster density, allowing for quick recognition of high-traffic areas in the grid.\n",
    "\n",
    "2. **Line Encoding for Trajectories**:\n",
    "   - Each trajectory is represented by a line that connects the sequential states visited by the agent. The lines are semi-transparent, making paths shared by multiple algorithms more prominent as they become visually reinforced through overlap.\n",
    "   - This approach highlights common routes among algorithms while still preserving individual paths. The low opacity of each line reduces visual clutter, especially in areas where multiple trajectories converge.\n",
    "\n",
    "Through these density-based highlights and trajectory lines, the plot reveals both common pathways and high-visit states, making it easier to see how each algorithm behaves in the grid, particularly in terms of exploration versus exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VH0XisVzsRn"
   },
   "source": [
    "#### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptRvTN4NzsRn"
   },
   "outputs": [],
   "source": [
    "alt.Chart(plotting_df).mark_point(\n",
    "    opacity=0.7,\n",
    "    filled=True,  # Ensure points are filled\n",
    "    size=10\n",
    ").encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    # draw one line per attempt, but ...\n",
    "    color='algorithm:N', # .. color the lines per solving strategy\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=700,\n",
    "    title=\"TSNE - Scatter Plot of X and Y by Algorithm\"\n",
    ") + alt.Chart(plotting_df).transform_filter(\n",
    "    (datum.cp == 'end') | (datum.cp == 'start') # no intermediate states\n",
    ").mark_point(filled=False).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    shape='cp:N',\n",
    "    color='algorithm:N', # .. color the lines per solving strategy\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=700\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0AGNeWzzsRn"
   },
   "outputs": [],
   "source": [
    "# Base chart for intermediate states\n",
    "base = alt.Chart(plotting_df).mark_point(\n",
    "    opacity=0.7,\n",
    "    filled=True,  # Ensure points are filled\n",
    "    size=10\n",
    ").encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    color='algorithm:N'\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=200\n",
    ")\n",
    "\n",
    "# Chart for start and end states only\n",
    "start_end = alt.Chart(plotting_df).transform_filter(\n",
    "    (alt.datum.cp == 'end') | (alt.datum.cp == 'start')\n",
    ").mark_point(filled=False).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    shape=alt.Shape('cp:N', legend=alt.Legend(title=\"State\")),\n",
    "    color='algorithm:N'\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=200\n",
    ")\n",
    "\n",
    "# Combine both charts and apply faceting to arrange by algorithm in a row\n",
    "chart = (base + start_end).facet(\n",
    "    facet=alt.Facet('algorithm:N', title=\"Algorithm\"),\n",
    "    columns=len(plotting_df['algorithm'].unique())\n",
    ").properties(\n",
    "    title=\"TSNE - Scatter Plot of X and Y by Algorithm\"\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5OhUO8fz259i",
    "outputId": "7ab55708-7e04-4ac9-e4b1-fe8c7e367bc0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "expected_sarsa_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='expected_sarsa', encoding_type='one-hot', down_project_method='t-SNE', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "q_learning_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='q_learning', encoding_type='one-hot', down_project_method='t-SNE', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "random_policy_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='random_policy', encoding_type='one-hot', down_project_method='t-SNE', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "sarsa_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='sarsa', encoding_type='one-hot', down_project_method='t-SNE', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "expected_sarsa_connected_density_plot.display()\n",
    "q_learning_connected_density_plot.display()\n",
    "random_policy_connected_density_plot.display()\n",
    "sarsa_connected_density_plot.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjNQxVR-zsRo"
   },
   "source": [
    "**Observation t-SNE:** <br>\n",
    "In the first scatter plot, we see fewer points for Q-learning and random policies, likely due to the limited number of states and slight variations in encoding, resulting in many values occupying the same space. The second plot reveals insights into the agent’s behavior: Q-learning, Random, and SARSA policies exhibit numerous end points, indicating frequent mistakes. Expected SARSA, however, performs better at avoiding the cliff, as fewer end points are prominent, suggesting a more cautious approach.\n",
    "\n",
    "In the four plots displaying state transitions and clusters, we gain further understanding of state prominence and distribution. The behavior of each policy is distinct. The starting state is consistently the most prominent. For the random policy, there are few frequently revisited states, which aligns with its lack of learning. Expected SARSA frequently visits states toward the middle, maintaining a balance between safety and efficiency. Q-learning shows a tendency to revisit states near the cliff, reflecting a riskier strategy. SARSA, in contrast, takes a path that is farthest from the cliff, favoring a longer but safer trajectory. This behavior is explained in more detail below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcrImccizsRo"
   },
   "outputs": [],
   "source": [
    "# Define colors for each unique algorithm\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "colors_cp = ['#1A2421', '#1A2421']\n",
    "\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Get unique algorithms\n",
    "algorithms = plotting_df['algorithm'].unique()\n",
    "\n",
    "# Loop through each unique algorithm\n",
    "for i, algo in enumerate(algorithms):\n",
    "    algo_data = plotting_df[plotting_df['algorithm'] == algo]\n",
    "    lines = algo_data['line'].unique()\n",
    "\n",
    "    for line in lines:\n",
    "        line_data = algo_data[algo_data['line'] == line]\n",
    "        plot_df_splines(ax=ax, df=line_data, color=colors[i], alpha=0.1, smoothing=0, n_points=9999)\n",
    "\n",
    "# Mark start and end points\n",
    "for i, algo in enumerate(algorithms):\n",
    "    start_data = plotting_df[(plotting_df['cp'] == 'start') & (plotting_df['algorithm'] == algo)]\n",
    "    #checkpoint_data = tsne_algo1[(tsne_algo1['cp'] == ' checkpoint') & (tsne_rubiks_algo1['algo'] == algo)]\n",
    "    intermediate_data = plotting_df[(plotting_df['cp'] == 'intermediate') & (plotting_df['algorithm'] == algo)]\n",
    "    end_data = plotting_df[(plotting_df['cp'] == 'end') & (plotting_df['algorithm'] == algo)]\n",
    "\n",
    "    ax.scatter(start_data['X'], start_data['Y'], color=colors[i], marker='o', alpha=0.25, s=200)\n",
    "    #ax.scatter(checkpoint_data['X'], checkpoint_data['Y'], color=colors[i], marker='s', alpha=0.25)\n",
    "    ax.scatter(intermediate_data['X'], intermediate_data['Y'], color=colors[i], marker='.', alpha=0.25)\n",
    "    ax.scatter(end_data['X'], end_data['Y'], color=colors[i], marker='x', alpha=0.25, s=200)\n",
    "\n",
    "for i, algo in enumerate(algorithms):\n",
    "    ax.scatter([], [], color=colors[i], label=algo)  # Empty scatter for legend only\n",
    "\n",
    "for state_name, state_marker in zip(['Start', 'End'], ['o', 'x']):\n",
    "    ax.scatter([], [], color=colors_cp[0], label=state_name, marker=state_marker)\n",
    "\n",
    "ax.set_title(\"TSNE - Dataset Trajectories by Algorithm\")\n",
    "ax.legend(title=\"Algorithms\", loc=\"best\")\n",
    "\n",
    "margin = 100\n",
    "plt.xlim(plotting_df['X'].min() - margin, plotting_df['X'].max() + margin)\n",
    "plt.ylim(plotting_df['Y'].min() - margin, plotting_df['Y'].max() + margin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSAs0WGbzsRo"
   },
   "source": [
    "**Observation t-SNE:** <br>\n",
    "With color encodings by algorithm and special markers (start='o' and end='X'), the differences in behavior among the algorithms become more evident. The trajectories for Q-learning and Expected SARSA are particularly prominent. Although SARSA also displays visible paths, the algorithms traverse different trajectories, as expected given their distinct behaviors. Even without clustering the data or knowing the specific states, we observe that Q-learning trajectories frequently pass through end points and are notably concentrated near the cliff, reflecting a higher risk approach. The random policy, on the other hand, produces less frequent, non-repetitive paths, resulting in faint green lines in the background due to the lack of intentional structure or repeated trajectories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxvjDi5vzsRo"
   },
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYNFYSv8zsRo"
   },
   "outputs": [],
   "source": [
    "alt.Chart(umap_plotting_df).mark_point(\n",
    "    opacity=0.7,\n",
    "    filled=True,  #\n",
    "    size=10\n",
    ").encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    color='algorithm:N',\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=700,\n",
    "    title=\"UMAP - Scatter Plot of X and Y by Algorithm\"\n",
    ") + alt.Chart(umap_plotting_df).transform_filter(\n",
    "    (datum.cp == 'end') | (datum.cp == 'start')\n",
    ").mark_point(filled=False).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    shape='cp:N',\n",
    "    color='algorithm:N'\n",
    ").properties(\n",
    "    width=700,\n",
    "    height=700\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKPLX4YazsRo"
   },
   "outputs": [],
   "source": [
    "# Graph for intermediate state\n",
    "base = alt.Chart(umap_plotting_df).mark_point(\n",
    "    opacity=0.7,\n",
    "    filled=True,\n",
    "    size=10\n",
    ").encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    color='algorithm:N'\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=200\n",
    ")\n",
    "\n",
    "# Graph for starting/ending point\n",
    "start_end = alt.Chart(umap_plotting_df).transform_filter(\n",
    "    (alt.datum.cp == 'end') | (alt.datum.cp == 'start')\n",
    ").mark_point(filled=False).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    shape=alt.Shape('cp:N', legend=alt.Legend(title=\"State\")),\n",
    "    color='algorithm:N'\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=200\n",
    ")\n",
    "\n",
    "chart = (base + start_end).facet(\n",
    "    facet=alt.Facet('algorithm:N', title=\"Algorithm\"),\n",
    "    columns=len(umap_plotting_df['algorithm'].unique())\n",
    ").properties(\n",
    "    title=\"UMAP - Scatter Plot of X and Y by Algorithm\"\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expected_sarsa_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='expected_sarsa', encoding_type='one-hot', down_project_method='UMAP', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "q_learning_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='q_learning', encoding_type='one-hot', down_project_method='UMAP', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "random_policy_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='random_policy', encoding_type='one-hot', down_project_method='UMAP', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "sarsa_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='sarsa', encoding_type='one-hot', down_project_method='UMAP', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "expected_sarsa_connected_density_plot.display()\n",
    "q_learning_connected_density_plot.display()\n",
    "random_policy_connected_density_plot.display()\n",
    "sarsa_connected_density_plot.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation UMAP:** <br>\n",
    "Similar to t-SNE, UMAP does not distinctly separate unique states by algorithm based solely on color coding. However, by examining the scatter plots of individual algorithms, we can observe that the distribution of clusters varies. The SARSA, Q-learning, and Random policies display similar cluster patterns, with points spread across various states. Expected SARSA, in contrast, has fewer clusters with points densely packed in specific areas, suggesting a more consistent set of preferred states and potentially more stable trajectories. This clustering behavior highlights Expected SARSA's tendency to avoid the cliff, as it appears to focus on a narrower range of safer states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zz-9o35vzsRo"
   },
   "outputs": [],
   "source": [
    "# Defining colors\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "colors_cp = ['#1A2421', '#1A2421']\n",
    "\n",
    "# Creating the plot\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Sampling the 10%\n",
    "sampling_fraction = 0.1  \n",
    "\n",
    "# Loop overall the algorithms\n",
    "for i, algo in enumerate(algorithms):\n",
    "    algo_data = umap_plotting_df[umap_plotting_df['algorithm'] == algo]\n",
    "    lines = algo_data['line'].unique()\n",
    "\n",
    "    # Sampling\n",
    "    sampled_lines = np.random.choice(lines, size=int(len(lines) * sampling_fraction), replace=False)\n",
    "\n",
    "    for line in sampled_lines:\n",
    "        line_data = algo_data[algo_data['line'] == line]\n",
    "        plot_df_splines(ax=ax, df=line_data, color=colors[i], alpha=0.1, smoothing=0, n_points=1000) \n",
    "\n",
    "# Start/End points\n",
    "for i, algo in enumerate(algorithms):\n",
    "    start_data = umap_plotting_df[(umap_plotting_df['cp'] == 'start') & (umap_plotting_df['algorithm'] == algo)]\n",
    "    intermediate_data = umap_plotting_df[(umap_plotting_df['cp'] == 'intermediate') & (umap_plotting_df['algorithm'] == algo)]\n",
    "    end_data = umap_plotting_df[(umap_plotting_df['cp'] == 'end') & (umap_plotting_df['algorithm'] == algo)]\n",
    "\n",
    "    ax.scatter(start_data['X'], start_data['Y'], color=colors[i], marker='o', alpha=0.25, s=200)\n",
    "    ax.scatter(intermediate_data['X'], intermediate_data['Y'], color=colors[i], marker='.', alpha=0.25)\n",
    "    ax.scatter(end_data['X'], end_data['Y'], color=colors[i], marker='x', alpha=0.25, s=200)\n",
    "\n",
    "for i, algo in enumerate(algorithms):\n",
    "    ax.scatter([], [], color=colors[i], label=algo)\n",
    "\n",
    "for state_name, state_marker in zip(['Start', 'End'], ['o', 'x']):\n",
    "    ax.scatter([], [], color=colors_cp[0], label=state_name, marker=state_marker)\n",
    "\n",
    "ax.set_title(\"UMAP - Dataset Trajectories by Algorithm (Sampled)\")\n",
    "ax.legend(title=\"Algorithms\", loc=\"best\")\n",
    "\n",
    "margin = 10\n",
    "plt.xlim(umap_plotting_df['X'].min() - margin, umap_plotting_df['X'].max() + margin)\n",
    "plt.ylim(umap_plotting_df['Y'].min() - margin, umap_plotting_df['Y'].max() + margin)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation UMAP:** <br>\n",
    "Also here in the spline plot, we see very similar patterns to t-SNE. The detected bundles correspond the the trajectories of the three policies (the random policy has naturally no intentional bundles as same trajectories are not frequent due to the random action selection). Again, Q-learning passes through lots of different endpoints near the cliff, indicating the preference for selecting this shortest path with more risk. Moreover, we can also see some intersections at encoded starting states where all the trajectories start/pass through. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJ99bfohzsRp"
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFi3aU1ezsRp"
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from altair import datum\n",
    "\n",
    "# Scatter plot of PC1 and PC2 by Algorithm\n",
    "pca_chart = alt.Chart(pca_plotting_df).mark_point(\n",
    "    opacity=0.7,\n",
    "    filled=True,  #\n",
    "    size=10\n",
    ").encode(\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='algorithm:N',\n",
    ").properties(\n",
    "    width=350,\n",
    "    height=350,\n",
    "    title=\"PCA - Scatter Plot of PC1 and PC2 by Algorithm\"\n",
    ") + alt.Chart(pca_plotting_df).transform_filter(\n",
    "    (datum.cp == 'end') | (datum.cp == 'start')\n",
    ").mark_point(filled=False).encode(\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    shape='cp:N',\n",
    "    color='algorithm:N',\n",
    ").properties(\n",
    "    width=350,\n",
    "    height=350\n",
    ")\n",
    "\n",
    "pca_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zBUsCxDzsRp"
   },
   "outputs": [],
   "source": [
    "# Base chart for intermediate states\n",
    "base_pca = alt.Chart(pca_plotting_df).mark_point(\n",
    "    opacity=0.7,\n",
    "    filled=True,\n",
    "    size=10\n",
    ").encode(\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    color='algorithm:N'\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=200\n",
    ")\n",
    "\n",
    "# Chart for start and end states only\n",
    "start_end_pca = alt.Chart(pca_plotting_df).transform_filter(\n",
    "    (alt.datum.cp == 'end') | (alt.datum.cp == 'start')\n",
    ").mark_point(filled=False).encode(\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    shape=alt.Shape('cp:N', legend=alt.Legend(title=\"State\")),\n",
    "    color='algorithm:N'\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=200\n",
    ")\n",
    "\n",
    "# Combine both charts and apply faceting\n",
    "pca_facet_chart = (base_pca + start_end_pca).facet(\n",
    "    facet=alt.Facet('algorithm:N', title=\"Algorithm\"),\n",
    "    columns=len(pca_plotting_df['algorithm'].unique())\n",
    ").properties(\n",
    "    title=\"PCA - Scatter Plot of PC1 and PC2 by Algorithm\"\n",
    ")\n",
    "\n",
    "pca_facet_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expected_sarsa_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='expected_sarsa', encoding_type='one-hot', down_project_method='PCA', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "q_learning_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='q_learning', encoding_type='one-hot', down_project_method='PCA', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "random_policy_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='random_policy', encoding_type='one-hot', down_project_method='PCA', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "sarsa_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='sarsa', encoding_type='one-hot', down_project_method='PCA', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "expected_sarsa_connected_density_plot.display()\n",
    "q_learning_connected_density_plot.display()\n",
    "random_policy_connected_density_plot.display()\n",
    "sarsa_connected_density_plot.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation PCA:** <br>\n",
    "As previously stated, adding color encoding does not reveal significant insights into the frequently visited downprojected states across algorithms. In the scatter plots of individual algorithms, minor differences are present but are not meaningful enough for deeper interpretation. These four PCA plots largely confirm the interpretations made for t-SNE: they illustrate how different downprojection methods represent state distributions in a 2D grid, but without distinct algorithm-specific clustering patterns.\n",
    "\n",
    "The PCA projections further emphasize the consistency of the algorithms in visiting similar states, though PCA’s linear nature limits its ability to separate subtle nuances between them. This visualization reinforces that PCA is best for capturing global variance rather than distinct algorithm trajectories or state prominence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kFI4H02zsRp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Define colors for each unique algorithm\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "colors_cp = ['#1A2421', '#1A2421']\n",
    "\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Get unique algorithms\n",
    "algorithms = pca_plotting_df['algorithm'].unique()\n",
    "\n",
    "# Loop through each unique algorithm\n",
    "for i, algo in enumerate(algorithms):\n",
    "    algo_data = pca_plotting_df[pca_plotting_df['algorithm'] == algo]\n",
    "    lines = algo_data['line'].unique()\n",
    "\n",
    "    for line in lines:\n",
    "        line_data = algo_data[algo_data['line'] == line]\n",
    "        plot_df_splines(ax=ax, df=line_data, x_col='PC1', y_col='PC2', color=colors[i], alpha=0.1, smoothing=0, n_points=9999)\n",
    "\n",
    "# Mark start, intermediate, and end points\n",
    "for i, algo in enumerate(algorithms):\n",
    "    start_data = pca_plotting_df[(pca_plotting_df['cp'] == 'start') & (pca_plotting_df['algorithm'] == algo)]\n",
    "    intermediate_data = pca_plotting_df[(pca_plotting_df['cp'] == 'intermediate') & (pca_plotting_df['algorithm'] == algo)]\n",
    "    end_data = pca_plotting_df[(pca_plotting_df['cp'] == 'end') & (pca_plotting_df['algorithm'] == algo)]\n",
    "\n",
    "    ax.scatter(start_data['PC1'], start_data['PC2'], color=colors[i], marker='o', alpha=0.25, s=200)\n",
    "    ax.scatter(intermediate_data['PC1'], intermediate_data['PC2'], color=colors[i], marker='.', alpha=0.25)\n",
    "    ax.scatter(end_data['PC1'], end_data['PC2'], color=colors[i], marker='x', alpha=0.25, s=200)\n",
    "\n",
    "# Create legend entries for algorithms\n",
    "for i, algo in enumerate(algorithms):\n",
    "    ax.scatter([], [], color=colors[i], label=algo)  # Empty scatter for legend\n",
    "\n",
    "# Create legend entries for states\n",
    "state_markers = {'Start': 'o', 'End': 'x'}\n",
    "for state_name, state_marker in state_markers.items():\n",
    "    ax.scatter([], [], color='black', marker=state_marker, label=state_name)\n",
    "\n",
    "ax.set_title(\"PCA - Dataset Trajectories by Algorithm\")\n",
    "ax.legend(title=\"Algorithms and States\", loc=\"best\", fontsize=12)\n",
    "\n",
    "margin = 1.2  # Adjusted margin for PCA data\n",
    "plt.xlim(pca_plotting_df['PC1'].min() - margin, pca_plotting_df['PC1'].max() + margin)\n",
    "plt.ylim(pca_plotting_df['PC2'].min() - margin, pca_plotting_df['PC2'].max() + margin)\n",
    "plt.xlabel('PC1', fontsize=16)\n",
    "plt.ylabel('PC2', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation PCA:** <br>\n",
    "In contrast to t-SNE and UMAP, the we do not see lots of bundles that highlight differences in the behaviour of the algorithms. All trajectories pass through the same starting state, traverse to the next state/cluster and then, closeer to the lower left corner, we see that many states are encoded in this region (e.g. all final states). Thus it can be suspected that there are nuances in the tracetories visible in this one cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjfP7B8QzsRp"
   },
   "source": [
    "### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LB7pIxYkzsRp"
   },
   "outputs": [],
   "source": [
    "alt.Chart(ica_plotting_df).mark_point(\n",
    "    opacity=0.7,\n",
    "    filled=True,\n",
    "    size=10\n",
    ").encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    color='algorithm:N'\n",
    ").properties(\n",
    "    width=350,\n",
    "    height=350,\n",
    "    title=\"ICA - Scatter Plot of X and Y by Algorithm\"\n",
    ") + alt.Chart(ica_plotting_df).transform_filter(\n",
    "    (datum.cp == 'end') | (datum.cp == 'start')\n",
    ").mark_point(filled=False).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    shape='cp:N',\n",
    "    color='algorithm:N'\n",
    ").properties(\n",
    "    width=350,\n",
    "    height=350\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vp54nfXFzsRp"
   },
   "outputs": [],
   "source": [
    "# Base chart for intermediate states\n",
    "base = alt.Chart(ica_plotting_df).mark_point(\n",
    "    opacity=0.7,\n",
    "    filled=True,  # Ensure points are filled\n",
    "    size=10\n",
    ").encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    color='algorithm:N'\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=200\n",
    ")\n",
    "\n",
    "# Chart for start and end states only\n",
    "start_end = alt.Chart(ica_plotting_df).transform_filter(\n",
    "    (alt.datum.cp == 'end') | (alt.datum.cp == 'start')\n",
    ").mark_point(filled=False).encode(\n",
    "    x='X',\n",
    "    y='Y',\n",
    "    shape=alt.Shape('cp:N', legend=alt.Legend(title=\"State\")),\n",
    "    color='algorithm:N'\n",
    ").properties(\n",
    "    width=200,\n",
    "    height=200\n",
    ")\n",
    "\n",
    "# Combine both charts and apply faceting to arrange by algorithm in a row\n",
    "chart = (base + start_end).facet(\n",
    "    facet=alt.Facet('algorithm:N', title=\"Algorithm\"),\n",
    "    columns=len(ica_plotting_df['algorithm'].unique())\n",
    ").properties(\n",
    "    title=\"ICA - Scatter Plot of X and Y by Algorithm\"\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expected_sarsa_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='expected_sarsa', encoding_type='one-hot', down_project_method='ICA', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "q_learning_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='q_learning', encoding_type='one-hot', down_project_method='ICA', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "random_policy_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='random_policy', encoding_type='one-hot', down_project_method='ICA', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "sarsa_connected_density_plot = visualizer.process_and_visualize(\n",
    "    algorithm_name='sarsa', encoding_type='one-hot', down_project_method='ICA', sample_n=20, density_plot=True\n",
    ")\n",
    "\n",
    "expected_sarsa_connected_density_plot.display()\n",
    "q_learning_connected_density_plot.display()\n",
    "random_policy_connected_density_plot.display()\n",
    "sarsa_connected_density_plot.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation ICA:** <br>\n",
    "Very similar to PCA no downprojected clusters for the individual algrithms are visible in these plots and differences between the scattered points are only nuanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1l8WdSvUzsRq"
   },
   "outputs": [],
   "source": [
    "# Define colors for each unique algorithm\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "colors_cp = ['#1A2421', '#1A2421']\n",
    "\n",
    "# Create a plot for ICA trajectories\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Get unique algorithms\n",
    "algorithms = ica_plotting_df['algorithm'].unique()\n",
    "\n",
    "# Loop through each unique algorithm\n",
    "for i, algo in enumerate(algorithms):\n",
    "    algo_data = ica_plotting_df[ica_plotting_df['algorithm'] == algo]\n",
    "    lines = algo_data['line'].unique()\n",
    "\n",
    "    for line in lines:\n",
    "        line_data = algo_data[algo_data['line'] == line]\n",
    "        plot_df_splines(ax=ax, df=line_data, color=colors[i], alpha=0.1, smoothing=0, n_points=9999)\n",
    "\n",
    "# Mark start and end points\n",
    "for i, algo in enumerate(algorithms):\n",
    "    start_data = ica_plotting_df[(ica_plotting_df['cp'] == 'start') & (ica_plotting_df['algorithm'] == algo)]\n",
    "    intermediate_data = ica_plotting_df[(ica_plotting_df['cp'] == 'intermediate') & (ica_plotting_df['algorithm'] == algo)]\n",
    "    end_data = ica_plotting_df[(ica_plotting_df['cp'] == 'end') & (ica_plotting_df['algorithm'] == algo)]\n",
    "\n",
    "    ax.scatter(start_data['X'], start_data['Y'], color=colors[i], marker='o', alpha=0.25, s=200)\n",
    "    ax.scatter(intermediate_data['X'], intermediate_data['Y'], color=colors[i], marker='.', alpha=0.25)\n",
    "    ax.scatter(end_data['X'], end_data['Y'], color=colors[i], marker='x', alpha=0.25, s=200)\n",
    "\n",
    "# Legend for algorithms and state markers\n",
    "for i, algo in enumerate(algorithms):\n",
    "    ax.scatter([], [], color=colors[i], label=algo)  # Empty scatter for legend only\n",
    "\n",
    "for state_name, state_marker in zip(['Start', 'End'], ['o', 'x']):\n",
    "    ax.scatter([], [], color=colors_cp[0], label=state_name, marker=state_marker)\n",
    "\n",
    "ax.set_title(\"ICA - Dataset Trajectories by Algorithm\")\n",
    "ax.legend(title=\"Algorithms\", loc=\"best\")\n",
    "\n",
    "margin = 3\n",
    "plt.xlim(ica_plotting_df['X'].min() - margin, ica_plotting_df['X'].max() + margin)\n",
    "plt.ylim(ica_plotting_df['Y'].min() - margin, ica_plotting_df['Y'].max() + margin)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation ICA:** <br>\n",
    "Like PCA, our starting state is encoded where all trajectories pass through and go on to a next state. All other states are encoded in one cluster in the upper right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOwLBybBzsRq",
    "tags": []
   },
   "source": [
    "## Optional\n",
    "<details>\n",
    "    <summary><a>Projection Space Explorer (click to reveal)</a></summary>\n",
    "\n",
    "    \n",
    "<h2>Projection Space Explorer</h2>\n",
    "\n",
    "The <a href=\"https://jku-vds-lab.at/pse/\">Projection Space Explorer</a> is a web application to plot and connect two dimensional points.\n",
    "Metadata of the data points can be used to encode additonal information into the projection, e.g., by using different shapes or colors.\n",
    "    \n",
    "Further Information:\n",
    "<ul>\n",
    "    <li>Paper: <a href=\"https://jku-vds-lab.at/publications/2020_tiis_pathexplorer/\">https://jku-vds-lab.at/publications/2020_tiis_pathexplorer/</a>\n",
    "    <li>Repo: <a href=\"https://github.com/jku-vds-lab/projection-space-explorer/\">https://github.com/jku-vds-lab/projection-space-explorer/</a>\n",
    "    <li>Application Overview: <a href=\"https://jku-vds-lab.at/pse/\">https://jku-vds-lab.at/pse/</a>\n",
    "</ul>\n",
    "\n",
    "<h3>Data Format</h3>\n",
    "How to format the data can be found in the <a href=\"https://github.com/jku-vds-lab/projection-space-explorer/#data-format\">Projection Space Explorer's README</a>.\n",
    "\n",
    "Example data with three lines, with two colors (algo) and additional mark encoding (cp):\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>x</th>\n",
    "    <th>y</th>\n",
    "    <th>line</th>\n",
    "    <th>cp</th>\n",
    "    <th>algo</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>0.0</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "    <td>start</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2.0</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "    <td>state</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4.0</td>\n",
    "    <td>4</td>\n",
    "    <td>0</td>\n",
    "    <td>state</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6.0</td>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "    <td>state</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8.0</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "    <td>state</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12.0</td>\n",
    "    <td>0</td>\n",
    "    <td>0</td>\n",
    "    <td>end</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>-1.0</td>\n",
    "    <td>10</td>\n",
    "    <td>1</td>\n",
    "    <td>start</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0.5</td>\n",
    "    <td>5</td>\n",
    "    <td>1</td>\n",
    "    <td>state</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2.0</td>\n",
    "    <td>3</td>\n",
    "    <td>1</td>\n",
    "    <td>state</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3.5</td>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "    <td>state</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5.0</td>\n",
    "    <td>3</td>\n",
    "    <td>1</td>\n",
    "    <td>state</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6.5</td>\n",
    "    <td>5</td>\n",
    "    <td>1</td>\n",
    "    <td>state</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8.0</td>\n",
    "    <td>10</td>\n",
    "    <td>1</td>\n",
    "    <td>end</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3.0</td>\n",
    "    <td>6</td>\n",
    "    <td>2</td>\n",
    "    <td>start</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2.0</td>\n",
    "    <td>7</td>\n",
    "    <td>2</td>\n",
    "    <td>end</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "Save the dataset to CSV, e.g. using pandas: <tt>df.to_csv('data_path_explorer.csv', encoding='utf-8', index=False)</tt>  \n",
    "    and upload it in the Projection Space Explorer by clicking on `OPEN FILE` in the top left corner.\n",
    "    \n",
    "ℹ You can also include your high dimensionmal data and use it to adapt the visualization.\n",
    "    \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLIpB_4azsRq"
   },
   "source": [
    "## Results\n",
    "You may add additional screenshots of the Projection Space Explorer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXbCUVUwzsRq"
   },
   "source": [
    "### Interpretion\n",
    "- What can be seen in the projection(s)?\n",
    "- Was it what you expected? If not what did you expect?\n",
    "- Can you confirm prior hypotheses from the projection?\n",
    "- Did you get any unexpected insights?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKmv6nhezsRq"
   },
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iu2AO4LkzsRq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Detect Clusters in the dataset\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=400)\n",
    "cluster_labels = clusterer.fit_predict(plotting_df[['X', 'Y']].values)\n",
    "plotting_df['cluster'] = cluster_labels\n",
    "display(plotting_df.head())\n",
    "print()\n",
    "\n",
    "# create a dictionary with the proportion of samples having one state belonging to a cluster\n",
    "df['cluster'] = cluster_labels\n",
    "df.head()\n",
    "cluster_state_dict = {}\n",
    "\n",
    "for cluster_id, cluster_data in df.groupby('cluster'):\n",
    "    #if cluster_id != -1:\n",
    "    cluster_state_dict[cluster_id] = {}\n",
    "    unique_states = cluster_data['state'].unique()  # Extract unique states in this cluster\n",
    "    print(f\"Cluster {cluster_id} has the following unique states:\")\n",
    "    for state in unique_states:\n",
    "        cluster_state_dict[cluster_id][state.item()] = len(cluster_data[cluster_data['state']==state])/ len(cluster_data['state'])\n",
    "        print(f\"  - {state}\")\n",
    "    print(f\"samples in cluster: {len(cluster_data)}\")\n",
    "    print()  # Blank line for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmWw-P6ezsRr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the Gridworld with iproportional state size for each cluster\n",
    "# Define the grid dimensions (4 rows and 12 columns for Cliff Walking)\n",
    "grid_height = 4\n",
    "grid_width = 12\n",
    "total_states = grid_height * grid_width\n",
    "\n",
    "# Function to convert state index to (x, y) grid position\n",
    "def state_to_grid_position(state, grid_width):\n",
    "    return divmod(state, grid_width)  # Returns (row, column)\n",
    "\n",
    "def visualize_grid_world(cluster_id, cluster_state_dict, filname=None):\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    # Create a grid using a matrix\n",
    "    grid = np.zeros((grid_height, grid_width))\n",
    "\n",
    "    # Define special states: start, goal, and cliff\n",
    "    start_state = 36\n",
    "    goal_state = 47\n",
    "    cliff_states = list(range(37, 47))\n",
    "\n",
    "    # Mark the cliff, start, and goal states in the grid\n",
    "    # Using different values for different states\n",
    "    for state in cliff_states:\n",
    "        x, y = state_to_grid_position(state, grid_width)\n",
    "        grid[x, y] = 1  # Cliff (will be gray)\n",
    "\n",
    "    start_x, start_y = state_to_grid_position(start_state, grid_width)\n",
    "    goal_x, goal_y = state_to_grid_position(goal_state, grid_width)\n",
    "    grid[start_x, start_y] = 2  # Start (will be blue)\n",
    "    grid[goal_x, goal_y] = 3    # Goal (will be green)\n",
    "\n",
    "    # Create custom colormap with specified colors\n",
    "    colors = ['white',    # Empty cells (0)\n",
    "             'gray',      # Cliff (1)\n",
    "             'blue',      # Start (2)\n",
    "             'green']     # Goal (3)\n",
    "    custom_cmap = ListedColormap(colors)\n",
    "\n",
    "    # Plot the grid with the custom color map\n",
    "    ax.imshow(grid, cmap=custom_cmap, extent=[0, grid_width, 0, grid_height])\n",
    "\n",
    "    # Plot the agent's current position\n",
    "    base_agent_marker_size = 200\n",
    "    cluster_states = cluster_state_dict[cluster_id]\n",
    "    for agent_state, cluster_proportion in cluster_states.items():\n",
    "        agent_x, agent_y = state_to_grid_position(agent_state, grid_width)\n",
    "        agent_marker_size = base_agent_marker_size * cluster_proportion\n",
    "        ax.scatter(agent_y + 0.5, grid_height - agent_x - 0.5, color='purple',\n",
    "                   s=agent_marker_size,\n",
    "                   label=f'Agent {cluster_proportion:.2f}')\n",
    "\n",
    "    # Annotate the grid with state numbers\n",
    "    #for state in range(total_states):\n",
    "    #    x, y = state_to_grid_position(state, grid_width)\n",
    "    #    ax.text(y + 0.5, grid_height - x - 0.5, str(state), ha='center', va='center', color='black')\n",
    "\n",
    "    # Labels and titles\n",
    "    ax.set_xticks(np.arange(grid_width))\n",
    "    ax.set_yticks(np.arange(grid_height))\n",
    "    ax.set_xticklabels(np.arange(grid_width))\n",
    "    ax.set_yticklabels(np.arange(grid_height - 1, -1, -1))\n",
    "    ax.grid(True)\n",
    "    #ax.set_title(\"Cliff Walking Grid World\")\n",
    "    #ax.legend(loc='upper left')\n",
    "\n",
    "    # Display the plot\n",
    "\n",
    "    if filename is not None:\n",
    "        fig.savefig(filename)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9jibNSVzsRr"
   },
   "outputs": [],
   "source": [
    "# storing gridworld plots for each cluster\n",
    "# Group by cluster and calculate the center point for each cluster\n",
    "cluster_centers = plotting_df.groupby('cluster')[['X', 'Y']].mean()\n",
    "\n",
    "for cluster_id in plotting_df['cluster'].unique():\n",
    "    if cluster_id != -1:\n",
    "        #representative_state = plotting_df[plotting_df['cluster'] == cluster_id]['line'].mode().iloc[0]  # Get a representative state\n",
    "        filename = f'gridworld/gridworld_cluster_{cluster_id}.png'\n",
    "        visualize_grid_world(cluster_id, cluster_state_dict, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq58-OLhzsRr"
   },
   "outputs": [],
   "source": [
    "# Embedding Cluster Plots in Plots\n",
    "from mpl_toolkits.axes_grid1.inset_locator import InsetPosition, mark_inset\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "### SPLINES ###\n",
    "\n",
    "# Define colors for each unique algorithm\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "colors_cp = ['#1A2421', '#1A2421']\n",
    "\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize=(100, 100))\n",
    "\n",
    "# Get unique algorithms\n",
    "algorithms = plotting_df['algorithm'].unique()\n",
    "\n",
    "# Loop through each unique algorithm\n",
    "for i, algo in enumerate(algorithms):\n",
    "    algo_data = plotting_df[plotting_df['algorithm'] == algo]\n",
    "    lines = algo_data['line'].unique()\n",
    "\n",
    "    for line in lines:\n",
    "        line_data = algo_data[algo_data['line'] == line]\n",
    "        plot_df_splines(ax=ax, df=line_data, color=colors[i], alpha=0.1, smoothing=0, n_points=9999)\n",
    "\n",
    "# Mark start and end points\n",
    "for i, algo in enumerate(algorithms):\n",
    "    start_data = plotting_df[(plotting_df['cp'] == 'start') & (plotting_df['algorithm'] == algo)]\n",
    "    intermediate_data = plotting_df[(plotting_df['cp'] == ' intermediate') & (plotting_df['algorithm'] == algo)]\n",
    "    end_data = plotting_df[(plotting_df['cp'] == 'end') & (plotting_df['algorithm'] == algo)]\n",
    "\n",
    "    ax.scatter(start_data['X'], start_data['Y'], color=colors[i], marker='o', alpha=0.25, s=200)\n",
    "    ax.scatter(intermediate_data['X'], intermediate_data['Y'], color=colors[i], marker='.', alpha=0.25)\n",
    "    ax.scatter(end_data['X'], end_data['Y'], color=colors[i], marker='x', alpha=0.25, s=200)\n",
    "\n",
    "for i, algo in enumerate(algorithms):\n",
    "    ax.scatter([], [], color=colors[i], label=algo, marker='.', s=5000)  # Empty scatter for legend only\n",
    "\n",
    "for state_name, state_marker in zip(['Start', 'End'], ['o', 'x']):\n",
    "    ax.scatter([], [], color=colors_cp[0], label=state_name, marker=state_marker, s=1000)\n",
    "\n",
    "ax.set_title(\"TSNE - Dataset Trajectories by Algorithm with Gridworld Cluster Plots\", fontsize = 60)\n",
    "ax.legend(title=\"Algorithms\", loc=\"best\", fontsize = 60)\n",
    "\n",
    "\n",
    "\n",
    "### GRIDWORLD CLUSTERS ###\n",
    "# Plot each point and embed each cluster's gridworld plot at its center\n",
    "cluster_centers = plotting_df.groupby('cluster')[['X', 'Y']].mean()\n",
    "for i, (cluster_id, center) in enumerate(cluster_centers.iterrows()):\n",
    "    if cluster_id != -1:\n",
    "        #cluster_data = plotting_df[plotting_df['cluster'] == cluster_id]\n",
    "        #ax.scatter(cluster_data['X'], cluster_data['Y'], label=f'Cluster {cluster_id}', color=colors[i])\n",
    "\n",
    "        # Load and add the saved gridworld image at the cluster center\n",
    "        filename = f'gridworld/gridworld_cluster_{cluster_id}.png'\n",
    "        img = plt.imread(filename)\n",
    "        imagebox = OffsetImage(img, zoom=0.7)\n",
    "        ab = AnnotationBbox(imagebox, (center['X'], center['Y']), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "margin = 100\n",
    "plt.xlim(plotting_df['X'].min() - margin, plotting_df['X'].max() + margin)\n",
    "plt.ylim(plotting_df['Y'].min() - margin, plotting_df['Y'].max() + margin)\n",
    "plt.show()\n",
    "fig.savefig('data/cliff_walking/TSNE_1000_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jqu_J74uzsRr"
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image, display\n",
    "#display(Image(filename=\"data/cliff_walking/TSNE_1000_plot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p02sLoG0zsRr"
   },
   "source": [
    "**Observation t-SNE:** <br>\n",
    "This final plot, was produced for the t-SNE downprojections by first clustering the points with HDBscan and then plotting the gridworld with the agent states of the cluster into its center.  <br>\n",
    "From the overall structure, we can derive somewhat dense starting and end points (taking different encodings for the same state into consideration). We also have some bundles, very prominent for Q-learning and expected Sarsa. Besides, also some sparse trajectories/points can be detected.\n",
    "\n",
    "We see the different charcteristics of the 3 different policies (algorithms), excluding the random policy: In our observations, we expected to see the following:\n",
    "- Sarsa learns from the actions that the policy slects, also suboptimal actions. Thus, we get a more conservative policy that results in a safer behaviour\n",
    "- Q-learning looks at the maximum Q-value over all actions for the next state. Thus it is more greed and always aims for the best possible outcome.\n",
    "- Expected Sarsa is a trade of between Sarsa and Q-learning. It accounts for all possible actions and doesn't focus on a single action.\n",
    "\n",
    "We can easily confirm this by looking into the plots:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APsWO-F8zsRr"
   },
   "source": [
    "<table style=\"width: 100%; border: none;\">\n",
    "    <tr>\n",
    "        <td style=\"vertical-align: top; width: 50%;\">\n",
    "            <p>In this part of the projection, we see that an agent passes this area under a Q-learning policy a lot.\n",
    "            This policy takes always the greediest action to get the best possible outcome. In our gridworl, we get a reward of -1 for every step. Thus, under this greedy policy, we try to make as little steps as possible to reach the goal. This is only possible when walking very close to the cliff (grey area). Consequently, our trajectories are very prominent at these state near/at the cliff. </p>\n",
    "        </td>\n",
    "        <td style=\"width: 50%;\">\n",
    "            <img src=\"data/cliff_walking/TSNE_1000_plot_snippet_Qlearning.png\" alt=\"Description\" style=\"width: 100%; height: auto;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"vertical-align: top; width: 50%;\">\n",
    "            <p>Although the red Sarsa trajectories are not that visible in the plot, we provided here a snippet where the cluster states are visible: Sarsa is the most conservative algorithm and in the snippet, it has not many overlaps with the other algorithms. The paths are the farthest away from the cliff to avoid the penalities of falling of it. </p>\n",
    "        </td>\n",
    "        <td style=\"width: 50%;\">\n",
    "            <img src=\"data/cliff_walking/TSNE_1000_plot_snippet_Sarsa.png\" alt=\"Description\" style=\"width: 80%; height: 500px;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"vertical-align: top; width: 50%;\">\n",
    "            <p>Here, we can see that under the expected Sarsa algorithm, the agent takes different steps: They can be interpreted as this middle ground between Sarsa and Q-learning as it is more conservative than Q-learning but not as conservative as Sarsa. Thus, we see that the agent took steps that are a litte distant from the cliff but not that far away.   </p>\n",
    "        </td>\n",
    "        <td style=\"width: 50%;\">\n",
    "            <img src=\"data/cliff_walking/TSNE_1000_plot_snippet_expected_sarsa.png\" alt=\"Description\" style=\"width: 100%; height: auto;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"vertical-align: top; width: 50%;\">\n",
    "            <p>In the snippet of the goal state, we see that some trajectories of agents that managed to get past the cliff meet again. In addition, we can see another property of t-SNE: spacial properties are not preserved, thus close states to this goal state are not necessarily in this states neighborhood.</p>\n",
    "        </td>\n",
    "        <td style=\"width: 50%;\">\n",
    "            <img src=\"data/cliff_walking/TSNE_1000_plot_goal_state.png\" alt=\"Description\" style=\"width: 100%; height: auto;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8Ipz7-fzsRs"
   },
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Detect Clusters in the dataset\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=400)\n",
    "cluster_labels = clusterer.fit_predict(umap_plotting_df[['X', 'Y']].values)\n",
    "umap_plotting_df['cluster'] = cluster_labels\n",
    "display(umap_plotting_df.head())\n",
    "print()\n",
    "\n",
    "# create a dictionary with the proportion of samples having one state belonging to a cluster\n",
    "df['cluster'] = cluster_labels\n",
    "df.head()\n",
    "cluster_state_dict = {}\n",
    "\n",
    "for cluster_id, cluster_data in df.groupby('cluster'):\n",
    "    #if cluster_id != -1:\n",
    "    cluster_state_dict[cluster_id] = {}\n",
    "    unique_states = cluster_data['state'].unique()  # Extract unique states in this cluster\n",
    "    print(f\"Cluster {cluster_id} has the following unique states:\")\n",
    "    for state in unique_states:\n",
    "        cluster_state_dict[cluster_id][state.item()] = len(cluster_data[cluster_data['state']==state])/ len(cluster_data['state'])\n",
    "        print(f\"  - {state}\")\n",
    "    print(f\"samples in cluster: {len(cluster_data)}\")\n",
    "    print()  # Blank line for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# storing gridworld plots for each cluster\n",
    "# Group by cluster and calculate the center point for each cluster\n",
    "cluster_centers = umap_plotting_df.groupby('cluster')[['X', 'Y']].mean()\n",
    "\n",
    "for cluster_id in umap_plotting_df['cluster'].unique():\n",
    "    if cluster_id != -1:\n",
    "        #representative_state = umap_plotting_df[umap_plotting_df['cluster'] == cluster_id]['line'].mode().iloc[0]  # Get a representative state\n",
    "        filename = f'gridworld/gridworld_cluster_{cluster_id}.png'\n",
    "        visualize_grid_world(cluster_id, cluster_state_dict, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining colors\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "colors_cp = ['#1A2421', '#1A2421']\n",
    "\n",
    "# Creating the plot\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Sampling the 10%\n",
    "sampling_fraction = 0.1  \n",
    "\n",
    "# Loop overall the algorithms\n",
    "for i, algo in enumerate(algorithms):\n",
    "    algo_data = umap_plotting_df[umap_plotting_df['algorithm'] == algo]\n",
    "    lines = algo_data['line'].unique()\n",
    "\n",
    "    # Sampling\n",
    "    sampled_lines = np.random.choice(lines, size=int(len(lines) * sampling_fraction), replace=False)\n",
    "\n",
    "    for line in sampled_lines:\n",
    "        line_data = algo_data[algo_data['line'] == line]\n",
    "        plot_df_splines(ax=ax, df=line_data, color=colors[i], alpha=0.1, smoothing=0, n_points=1000) \n",
    "\n",
    "# Start/End points\n",
    "for i, algo in enumerate(algorithms):\n",
    "    start_data = umap_plotting_df[(umap_plotting_df['cp'] == 'start') & (umap_plotting_df['algorithm'] == algo)]\n",
    "    intermediate_data = umap_plotting_df[(umap_plotting_df['cp'] == 'intermediate') & (umap_plotting_df['algorithm'] == algo)]\n",
    "    end_data = umap_plotting_df[(umap_plotting_df['cp'] == 'end') & (umap_plotting_df['algorithm'] == algo)]\n",
    "\n",
    "    ax.scatter(start_data['X'], start_data['Y'], color=colors[i], marker='o', alpha=0.25, s=200)\n",
    "    ax.scatter(intermediate_data['X'], intermediate_data['Y'], color=colors[i], marker='.', alpha=0.25)\n",
    "    ax.scatter(end_data['X'], end_data['Y'], color=colors[i], marker='x', alpha=0.25, s=200)\n",
    "\n",
    "for i, algo in enumerate(algorithms):\n",
    "    ax.scatter([], [], color=colors[i], label=algo)\n",
    "\n",
    "for state_name, state_marker in zip(['Start', 'End'], ['o', 'x']):\n",
    "    ax.scatter([], [], color=colors_cp[0], label=state_name, marker=state_marker)\n",
    "\n",
    "ax.set_title(\"UMAP - Dataset Trajectories by Algorithm (Sampled)\")\n",
    "ax.legend(title=\"Algorithms\", loc=\"best\")\n",
    "\n",
    "### GRIDWORLD CLUSTERS ###\n",
    "# Plot each point and embed each cluster's gridworld plot at its center\n",
    "cluster_centers = umap_plotting_df.groupby('cluster')[['X', 'Y']].mean()\n",
    "for i, (cluster_id, center) in enumerate(cluster_centers.iterrows()):\n",
    "    if cluster_id != -1:\n",
    "        #cluster_data = umap_plotting_df[umap_plotting_df['cluster'] == cluster_id]\n",
    "        #ax.scatter(cluster_data['X'], cluster_data['Y'], label=f'Cluster {cluster_id}', color=colors[i])\n",
    "\n",
    "        # Load and add the saved gridworld image at the cluster center\n",
    "        filename = f'gridworld/gridworld_cluster_{cluster_id}.png'\n",
    "        img = plt.imread(filename)\n",
    "        imagebox = OffsetImage(img, zoom=0.2)\n",
    "        ab = AnnotationBbox(imagebox, (center['X'], center['Y']), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "margin = 5\n",
    "plt.xlim(umap_plotting_df['X'].min() - margin, umap_plotting_df['X'].max() + margin)\n",
    "plt.ylim(umap_plotting_df['Y'].min() - margin, umap_plotting_df['Y'].max() + margin)\n",
    "plt.show()\n",
    "fig.savefig('data/cliff_walking/UMAP_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image, display\n",
    "#display(Image(filename=\"data/cliff_walking/UMAP_plot.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation UMAP:** <br>\n",
    "\n",
    "From the overall structure, we can derive bundles, for the three learning algorithms. When we following the trajecotires and looking at the clustered points along these trajectories, we can see as mentioned above the three different characteristics/behaviours of Q-learning, Sarsa, and expected Sarsa. \n",
    "We provide some snippets of interesting regions below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg0JIBczsRs"
   },
   "source": [
    "<table style=\"width: 100%; border: none;\">\n",
    "    <tr>\n",
    "        <td style=\"vertical-align: top; width: 50%;\">\n",
    "            <p>In the first snipped, shared clusters by different algorithms are highlighted. We see that these clusters represent the starting state, the first action (which is always the same for all algorithms) and for the three learning algorithms the goal state.</p>\n",
    "        </td>\n",
    "        <td style=\"width: 50%;\">\n",
    "            <img src=\"data/cliff_walking/UMAP_plot_common_clusters.png\" alt=\"Description\" style=\"width: 100%; height: auto;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"vertical-align: top; width: 50%;\">\n",
    "            <p>In the second snipped, we see that the cluster components all have one state in common, belognging to the trajectory in the middle which is the expected behaviour of this policy.</p>\n",
    "        </td>\n",
    "        <td style=\"width: 50%;\">\n",
    "            <img src=\"data/cliff_walking/UMAP_plot_expected_sarsa.png\" alt=\"Description\" style=\"width: 100%; height: auto;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"vertical-align: top; width: 50%;\">\n",
    "            <p>Similarly, we plotted the states for the clusters and see also here that a single cluster encodes exactly one state and here, the bundles for Q-learning near the cliff are highlighted. </p>\n",
    "        </td>\n",
    "        <td style=\"width: 50%;\">\n",
    "            <img src=\"data/cliff_walking/UMAP_plot_q-learning.png\" alt=\"Description\" style=\"width: 100%; height: auto;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"vertical-align: top; width: 50%;\">\n",
    "            <p>For Sarsa, we see similar patters, and the clusters reflect states far away from the cliff. </p>\n",
    "        </td>\n",
    "        <td style=\"width: 50%;\">\n",
    "            <img src=\"data/cliff_walking/UMAP_plot_sarsa.png\" alt=\"Description\" style=\"width: 100%; height: auto;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jd-Uw-GXzsRs"
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Detect Clusters in the dataset\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=400)\n",
    "cluster_labels = clusterer.fit_predict(pca_plotting_df[['PC1', 'PC2']].values)\n",
    "pca_plotting_df['cluster'] = cluster_labels\n",
    "display(pca_plotting_df.head())\n",
    "print()\n",
    "\n",
    "# create a dictionary with the proportion of samples having one state belonging to a cluster\n",
    "df['cluster'] = cluster_labels\n",
    "df.head()\n",
    "cluster_state_dict = {}\n",
    "\n",
    "for cluster_id, cluster_data in df.groupby('cluster'):\n",
    "    #if cluster_id != -1:\n",
    "    cluster_state_dict[cluster_id] = {}\n",
    "    unique_states = cluster_data['state'].unique()  # Extract unique states in this cluster\n",
    "    print(f\"Cluster {cluster_id} has the following unique states:\")\n",
    "    for state in unique_states:\n",
    "        cluster_state_dict[cluster_id][state.item()] = len(cluster_data[cluster_data['state']==state])/ len(cluster_data['state'])\n",
    "        print(f\"  - {state}\")\n",
    "    print(f\"samples in cluster: {len(cluster_data)}\")\n",
    "    print()  # Blank line for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing gridworld plots for each cluster\n",
    "# Group by cluster and calculate the center point for each cluster\n",
    "cluster_centers = pca_plotting_df.groupby('cluster')[['PC1', 'PC2']].mean()\n",
    "\n",
    "for cluster_id in pca_plotting_df['cluster'].unique():\n",
    "    if cluster_id != -1:\n",
    "        #representative_state = pca_plotting_df[pca_plotting_df['cluster'] == cluster_id]['line'].mode().iloc[0]  # Get a representative state\n",
    "        filename = f'gridworld/gridworld_cluster_{cluster_id}.png'\n",
    "        visualize_grid_world(cluster_id, cluster_state_dict, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Cluster Plots in Plots\n",
    "# Define colors for each unique algorithm\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "colors_cp = ['#1A2421', '#1A2421']\n",
    "\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Get unique algorithms\n",
    "algorithms = pca_plotting_df['algorithm'].unique()\n",
    "\n",
    "# Loop through each unique algorithm\n",
    "for i, algo in enumerate(algorithms):\n",
    "    algo_data = pca_plotting_df[pca_plotting_df['algorithm'] == algo]\n",
    "    lines = algo_data['line'].unique()\n",
    "\n",
    "    for line in lines:\n",
    "        line_data = algo_data[algo_data['line'] == line]\n",
    "        plot_df_splines(ax=ax, df=line_data, x_col='PC1', y_col='PC2', color=colors[i], alpha=0.1, smoothing=0, n_points=9999)\n",
    "\n",
    "# Mark start, intermediate, and end points\n",
    "for i, algo in enumerate(algorithms):\n",
    "    start_data = pca_plotting_df[(pca_plotting_df['cp'] == 'start') & (pca_plotting_df['algorithm'] == algo)]\n",
    "    intermediate_data = pca_plotting_df[(pca_plotting_df['cp'] == 'intermediate') & (pca_plotting_df['algorithm'] == algo)]\n",
    "    end_data = pca_plotting_df[(pca_plotting_df['cp'] == 'end') & (pca_plotting_df['algorithm'] == algo)]\n",
    "\n",
    "    ax.scatter(start_data['PC1'], start_data['PC2'], color=colors[i], marker='o', alpha=0.25, s=200)\n",
    "    ax.scatter(intermediate_data['PC1'], intermediate_data['PC2'], color=colors[i], marker='.', alpha=0.25)\n",
    "    ax.scatter(end_data['PC1'], end_data['PC2'], color=colors[i], marker='x', alpha=0.25, s=200)\n",
    "\n",
    "# Create legend entries for algorithms\n",
    "for i, algo in enumerate(algorithms):\n",
    "    ax.scatter([], [], color=colors[i], label=algo)  # Empty scatter for legend\n",
    "\n",
    "# Create legend entries for states\n",
    "state_markers = {'Start': 'o', 'End': 'x'}\n",
    "for state_name, state_marker in state_markers.items():\n",
    "    ax.scatter([], [], color='black', marker=state_marker, label=state_name)\n",
    "\n",
    "ax.set_title(\"PCA - Dataset Trajectories by Algorithm\")\n",
    "ax.legend(title=\"Algorithms and States\", loc=\"best\", fontsize=12)\n",
    "\n",
    "### GRIDWORLD CLUSTERS ###\n",
    "# Plot each point and embed each cluster's gridworld plot at its center\n",
    "cluster_centers = pca_plotting_df.groupby('cluster')[['PC1', 'PC2']].mean()\n",
    "for i, (cluster_id, center) in enumerate(cluster_centers.iterrows()):\n",
    "    if cluster_id != -1:\n",
    "        #cluster_data = pca_plotting_df[pca_plotting_df['cluster'] == cluster_id]\n",
    "        #ax.scatter(cluster_data['X'], cluster_data['Y'], label=f'Cluster {cluster_id}', color=colors[i])\n",
    "\n",
    "        # Load and add the saved gridworld image at the cluster center\n",
    "        filename = f'gridworld/gridworld_cluster_{cluster_id}.png'\n",
    "        img = plt.imread(filename)\n",
    "        imagebox = OffsetImage(img, zoom=0.3)\n",
    "        ab = AnnotationBbox(imagebox, (center['PC1'], center['PC2']), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "margin = 0.5  # Adjusted margin for PCA data\n",
    "plt.xlim(pca_plotting_df['PC1'].min() - margin, pca_plotting_df['PC1'].max() + margin)\n",
    "plt.ylim(pca_plotting_df['PC2'].min() - margin, pca_plotting_df['PC2'].max() + margin)\n",
    "plt.xlabel('PC1', fontsize=16)\n",
    "plt.ylabel('PC2', fontsize=16)\n",
    "plt.show()\n",
    "fig.savefig('data/cliff_walking/PCA_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image, display\n",
    "#display(Image(filename=\"data/cliff_walking/PCA_plot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"data/cliff_walking/PCA_plot_zoomed_v2.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation PCA:** <br>\n",
    "\n",
    "Even with the additional added cluster states, we do not get any insights into the behvaiour of the different algorithms. Interestingly, starting state and the immediate next state can be well separated but for all the other states we have one big cluster. Even when zooming into this cluster, we cannot get any meanigful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkZq1SqSzsRs"
   },
   "source": [
    "### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Detect Clusters in the dataset\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=400)\n",
    "cluster_labels = clusterer.fit_predict(ica_plotting_df[['X', 'Y']].values)\n",
    "ica_plotting_df['cluster'] = cluster_labels\n",
    "display(ica_plotting_df.head())\n",
    "print()\n",
    "\n",
    "# create a dictionary with the proportion of samples having one state belonging to a cluster\n",
    "df['cluster'] = cluster_labels\n",
    "df.head()\n",
    "cluster_state_dict = {}\n",
    "\n",
    "for cluster_id, cluster_data in df.groupby('cluster'):\n",
    "    #if cluster_id != -1:\n",
    "    cluster_state_dict[cluster_id] = {}\n",
    "    unique_states = cluster_data['state'].unique()  # Extract unique states in this cluster\n",
    "    print(f\"Cluster {cluster_id} has the following unique states:\")\n",
    "    for state in unique_states:\n",
    "        cluster_state_dict[cluster_id][state.item()] = len(cluster_data[cluster_data['state']==state])/ len(cluster_data['state'])\n",
    "        print(f\"  - {state}\")\n",
    "    print(f\"samples in cluster: {len(cluster_data)}\")\n",
    "    print()  # Blank line for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing gridworld plots for each cluster\n",
    "# Group by cluster and calculate the center point for each cluster\n",
    "cluster_centers = ica_plotting_df.groupby('cluster')[['X', 'Y']].mean()\n",
    "\n",
    "for cluster_id in ica_plotting_df['cluster'].unique():\n",
    "    if cluster_id != -1:\n",
    "        #representative_state = ica_plotting_df[ica_plotting_df['cluster'] == cluster_id]['line'].mode().iloc[0]  # Get a representative state\n",
    "        filename = f'gridworld/gridworld_cluster_{cluster_id}.png'\n",
    "        visualize_grid_world(cluster_id, cluster_state_dict, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for each unique algorithm\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "colors_cp = ['#1A2421', '#1A2421']\n",
    "\n",
    "# Create a plot for ICA trajectories\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "# Get unique algorithms\n",
    "algorithms = ica_plotting_df['algorithm'].unique()\n",
    "\n",
    "# Loop through each unique algorithm\n",
    "for i, algo in enumerate(algorithms):\n",
    "    algo_data = ica_plotting_df[ica_plotting_df['algorithm'] == algo]\n",
    "    lines = algo_data['line'].unique()\n",
    "\n",
    "    for line in lines:\n",
    "        line_data = algo_data[algo_data['line'] == line]\n",
    "        plot_df_splines(ax=ax, df=line_data, color=colors[i], alpha=0.1, smoothing=0, n_points=9999)\n",
    "\n",
    "# Mark start and end points\n",
    "for i, algo in enumerate(algorithms):\n",
    "    start_data = ica_plotting_df[(ica_plotting_df['cp'] == 'start') & (ica_plotting_df['algorithm'] == algo)]\n",
    "    intermediate_data = ica_plotting_df[(ica_plotting_df['cp'] == 'intermediate') & (ica_plotting_df['algorithm'] == algo)]\n",
    "    end_data = ica_plotting_df[(ica_plotting_df['cp'] == 'end') & (ica_plotting_df['algorithm'] == algo)]\n",
    "\n",
    "    ax.scatter(start_data['X'], start_data['Y'], color=colors[i], marker='o', alpha=0.25, s=200)\n",
    "    ax.scatter(intermediate_data['X'], intermediate_data['Y'], color=colors[i], marker='.', alpha=0.25)\n",
    "    ax.scatter(end_data['X'], end_data['Y'], color=colors[i], marker='x', alpha=0.25, s=200)\n",
    "\n",
    "# Legend for algorithms and state markers\n",
    "for i, algo in enumerate(algorithms):\n",
    "    ax.scatter([], [], color=colors[i], label=algo)  # Empty scatter for legend only\n",
    "\n",
    "for state_name, state_marker in zip(['Start', 'End'], ['o', 'x']):\n",
    "    ax.scatter([], [], color=colors_cp[0], label=state_name, marker=state_marker)\n",
    "\n",
    "ax.set_title(\"ICA - Dataset Trajectories by Algorithm\")\n",
    "ax.legend(title=\"Algorithms\", loc=\"best\")\n",
    "\n",
    "### GRIDWORLD CLUSTERS ###\n",
    "# Plot each point and embed each cluster's gridworld plot at its center\n",
    "cluster_centers = ica_plotting_df.groupby('cluster')[['X', 'Y']].mean()\n",
    "for i, (cluster_id, center) in enumerate(cluster_centers.iterrows()):\n",
    "    if cluster_id != -1:\n",
    "        #cluster_data = ica_plotting_df[ica_plotting_df['cluster'] == cluster_id]\n",
    "        #ax.scatter(cluster_data['X'], cluster_data['Y'], label=f'Cluster {cluster_id}', color=colors[i])\n",
    "\n",
    "        # Load and add the saved gridworld image at the cluster center\n",
    "        filename = f'gridworld/gridworld_cluster_{cluster_id}.png'\n",
    "        img = plt.imread(filename)\n",
    "        imagebox = OffsetImage(img, zoom=0.3)\n",
    "        ab = AnnotationBbox(imagebox, (center['X'], center['Y']), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "\n",
    "margin = 3\n",
    "plt.xlim(ica_plotting_df['X'].min() - margin, ica_plotting_df['X'].max() + margin)\n",
    "plt.ylim(ica_plotting_df['Y'].min() - margin, ica_plotting_df['Y'].max() + margin)\n",
    "plt.show()\n",
    "fig.savefig('data/cliff_walking/ICA_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"data/cliff_walking/ICA_plot_zoomed_v2.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation ICA:** <br>\n",
    "\n",
    "Exaclty the same as PCA, we have well separated starting state and successor state but one big cluster that is not meaningful indicating that ICA is not able to separate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMGU7swrzsRs"
   },
   "source": [
    "### Similarities and Differneces of Downprojection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we can derive the following:\n",
    "- t-SNE and UMAP are both nonlinear dimensionality reduction techniques that are better suited for this dataset. However, UMAP appears to be best suited for this visualization task as it balances local and global structures very well and provides meaningful trajectory bundels for all three learning algorithms.\n",
    "- PCA and ICA result in very similar downprojections. Their encodings are not able to separate the data well and no meaningful trajectories can be derived. A possible explanation could be the nature of the data: PCA and ICA capture linear and independent components, respectively, but they don’t capture the nonlinear relationships well or the non-Gaussian assumption is violated which might have been a problem for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyDE4mwnzsRs"
   },
   "source": [
    "## Submission\n",
    "When you’ve finished working on this assignment please download this notebook as HTML and add it to your repository in addition to the notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8sLXEMpzsRs"
   },
   "outputs": [],
   "source": [
    "print('I am at the end of the Notebook')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "xai_proj_space",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

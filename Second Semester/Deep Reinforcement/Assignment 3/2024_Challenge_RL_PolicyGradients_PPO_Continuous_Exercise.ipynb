{"cells":[{"cell_type":"markdown","metadata":{"id":"1CumJu-KrCvU"},"source":["# Deep Reinforcement Lerning Lectures - Policy Gradients"]},{"cell_type":"markdown","metadata":{"id":"q37atdWHrCvV"},"source":["### Imports and auxiliary settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUTCq3LfrCvX"},"outputs":[],"source":["!apt update\n","!apt install -y xvfb x11-utils python3-opengl ffmpeg swig\n","!pip install swig\n","!pip install jax==0.4.13 jaxlib==0.4.13\n","!pip install gymnasium==0.27.1 gymnasium[box2d] pyvirtualdisplay imageio-ffmpeg moviepy==1.0.3\n","!pip install onnx onnx2pytorch==0.4.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"te40-9xZrCvc"},"outputs":[],"source":["# select device\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBwre4vHrCvg"},"outputs":[],"source":["%matplotlib inline\n","\n","# PyTorch imports\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.distributions import Normal\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Cross Framework library for DL\n","import onnx\n","from onnx2pytorch import ConvertModel\n","\n","# Auxiliary Python imports\n","import os\n","import math\n","import io\n","import base64\n","import random\n","import shutil\n","import copy\n","import glob\n","from time import time, strftime\n","from tqdm import tqdm\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from time import sleep\n","\n","# Environment import and set logger level to display error only\n","# Environment import and set logger level to display error only\n","import gymnasium as gym\n","from gymnasium.spaces import Box\n","from gymnasium import logger as gymlogger\n","from gymnasium.wrappers import RecordVideo\n","gymlogger.set_level(gym.logger.ERROR)\n","\n","# Plotting and notebook imports\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","from IPython.display import HTML, display, clear_output\n","\n","# start virtual display\n","from pyvirtualdisplay import Display\n","pydisplay = Display(visible=0, size=(640, 480))\n","pydisplay.start()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmUL9JubrCvj"},"outputs":[],"source":["# use GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"Ugq-0cVvrCvo"},"source":["### Auxiliary Methods"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSOg1hWi6men"},"outputs":[],"source":["class Logger(object):\n","    \"\"\"Logger that can be used for debugging different values\n","    \"\"\"\n","    def __init__(self, logdir, params=None, debug=False):\n","        self.gradients = []\n","        self.debug = debug\n","        self.basepath = os.path.join(logdir, strftime(\"%Y-%m-%dT%H-%M-%S\"))\n","        os.makedirs(self.basepath, exist_ok=True)\n","        os.makedirs(self.log_dir, exist_ok=True)\n","        if params is not None and os.path.exists(params):\n","            shutil.copyfile(params, os.path.join(self.basepath, \"params.pkl\"))\n","        self.log_dict = {}\n","        self.dump_idx = {}\n","\n","    def add_gradients(self, grad):\n","        if not self.debug: return\n","        self.gradients.append(grad)\n","\n","    def compute_gradient_variance(self):\n","        vars_ = []\n","        grads_list = [np.zeros_like(self.gradients[0])] * 100\n","        for i, grads in enumerate(self.gradients):\n","            grads_list.append(grads)\n","            grads_list = grads_list[1:]\n","            grad_arr = np.stack(grads_list, axis=0)\n","            g = np.apply_along_axis(grad_variance, axis=-1, arr=grad_arr)\n","            vars_.append(np.mean(g))\n","        return vars_\n","\n","    @property\n","    def param_file(self):\n","        return os.path.join(self.basepath, \"params.pkl\")\n","\n","    @property\n","    def onnx_file(self):\n","        return os.path.join(self.basepath, \"model.onnx\")\n","\n","    @property\n","    def video_dir(self):\n","        return os.path.join(self.basepath, \"videos\")\n","\n","    @property\n","    def log_dir(self):\n","        return os.path.join(self.basepath, \"logs\")\n","\n","    def log(self, name, value):\n","        if name not in self.log_dict:\n","            self.log_dict[name] = []\n","            self.dump_idx[name] = -1\n","        self.log_dict[name].append((len(self.log_dict[name]), time(), value))\n","\n","    def get_values(self, name):\n","        if name in self.log_dict:\n","            return [x[2] for x in self.log_dict[name]]\n","        return None\n","\n","    def dump(self):\n","        for name, rows in self.log_dict.items():\n","            with open(os.path.join(self.log_dir, name + \".log\"), \"a\") as f:\n","                for i, row in enumerate(rows):\n","                    if i > self.dump_idx[name]:\n","                        f.write(\",\".join([str(x) for x in row]) + \"\\n\")\n","                        self.dump_idx[name] = i"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nd0Ue1cirCvp"},"outputs":[],"source":["def wrap_env(env, logger, capture_video=True):\n","    # wrapper for recording\n","    env = gym.wrappers.RecordEpisodeStatistics(env)\n","    if capture_video:\n","        env = gym.wrappers.RecordVideo(env, logger.video_dir, episode_trigger=lambda idx: True)\n","    return env\n","\n","\n","def create_env(logger, env_id='BipedalWalker-v3', hardcore=False, capture_video=True):\n","    # initialize environment\n","    env = wrap_env(gym.make(env_id, max_episode_steps=400, hardcore=hardcore, render_mode=\"rgb_array\"),\n","                   logger=logger,\n","                   capture_video=capture_video)\n","    action_size = env.action_space.shape[0]\n","    state_size = env.observation_space.shape[0]\n","    return env, action_size, state_size\n","\n","\n","def set_seed(env, seed=None):\n","    # seeding the envrionment\n","    if seed is not None:\n","        random.seed(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed(seed)\n","            torch.cuda.manual_seed_all(seed)\n","\n","\n","def transforms(state):\n","    # transofrm to numpy to tensor and push to device\n","    return torch.FloatTensor(state).to(device)\n","\n","\n","def test_environment(env, agent=None, seed=42, n_steps=200):\n","    # run and evaluate in the environment\n","    state, info = env.reset(seed=seed)\n","    for i in range(n_steps):\n","        env.render()\n","\n","        if agent is None:\n","            action = env.action_space.sample()\n","        else:\n","            action, _ = agent.act(state)\n","            action = action.squeeze().cpu().numpy()\n","        state, reward, done, truncated, info = env.step(action)\n","        if done:\n","            state, info = env.reset(seed=seed)\n","    env.close()\n","\n","\n","def get_running_stat(stat, stat_len):\n","    # evaluate stats\n","    cum_sum = np.cumsum(np.insert(stat, 0, 0))\n","    return (cum_sum[stat_len:] - cum_sum[:-stat_len]) / stat_len\n","\n","\n","def plot_results(runner):\n","    # plot stats\n","    episode, r, l = np.array(runner.stats_rewards_list).T\n","    cum_r = get_running_stat(r, 10)\n","    cum_l = get_running_stat(l, 10)\n","\n","    plt.figure(figsize=(16, 16))\n","\n","    plt.subplot(321)\n","\n","    # plot rewards\n","    plt.plot(episode[-len(cum_r):], cum_r)\n","    plt.plot(episode, r, alpha=0.5)\n","    plt.xlabel('Episode')\n","    plt.ylabel('Episode Reward')\n","\n","    plt.subplot(322)\n","\n","    # plot episode lengths\n","    plt.plot(episode[-len(cum_l):], cum_l)\n","    plt.plot(episode, l, alpha=0.5)\n","    plt.xlabel('Episode')\n","    plt.ylabel('Episode Length')\n","\n","    plt.subplot(323)\n","\n","    # plot return\n","    all_returns = np.array(runner.buffer.all_returns)\n","    plt.scatter(range(0, len(all_returns)), all_returns, alpha=0.5)\n","    mean_returns = np.array(runner.buffer.mean_returns)\n","    plt.plot(range(0, len(mean_returns)), mean_returns, color=\"orange\")\n","    plt.xlabel('Episode')\n","    plt.ylabel('Return')\n","\n","    plt.subplot(324)\n","\n","    # plot entropy\n","    entropy_arr = np.array(runner.stats_entropy_list)\n","    plt.plot(range(0, len(entropy_arr)), entropy_arr)\n","    plt.xlabel('Episode')\n","    plt.ylabel('Entropy')\n","\n","    plt.subplot(325)\n","\n","    if runner.logger.debug:\n","        # plot variance\n","        variance_arr = np.array(runner.logger.compute_gradient_variance())\n","        plt.plot(range(0, len(variance_arr)), variance_arr)\n","        plt.xlabel('Episode')\n","        plt.ylabel('Variance')\n","\n","    plt.show()\n","\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","\"\"\"\n","def show_video(logger):\n","    print(logger.video_dir)\n","    mp4list = glob.glob(f'{logger.video_dir}/*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        display(HTML(data='''<video alt=\"test\" autoplay\n","                  loop controls style=\"height: 400px;\">\n","                  <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","                </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"Could not find video\")\n","\n","\n","def grad_variance(g):\n","    # compute gradient variance\n","    return np.mean(g**2) - np.mean(g)**2"]},{"cell_type":"markdown","metadata":{"id":"v9aGjeWvrCvt"},"source":["### Test environment"]},{"cell_type":"markdown","metadata":{"id":"Dnk4dSKurCvu"},"source":["OpenAI offers a set of environments for Reinforcement Learning, which are accessible via the `gymnasium` pip package.\n","In this exercise we will focus on discrete control tasks using a Box2D simulation known as BipedalWalker-v3.\n","To access the Box2D packages, we use the gymnasium pypi `box2d` option."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzQtckrZrCvw"},"outputs":[],"source":["# show behavior in envrionment with random agent\n","logger = Logger(\"logdir\")\n","env, _, _ = create_env(logger)\n","test_environment(env)\n","show_video(logger)"]},{"cell_type":"markdown","metadata":{"id":"TLuXOizAFhfm"},"source":["### Training Buffer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-IHrVviFcqw"},"outputs":[],"source":["class Transition(object):\n","    \"\"\"Transition helper object\n","    \"\"\"\n","    def __init__(self, state, action, reward, next_state, log_probs):\n","        self.state = state\n","        self.action = action\n","        self.reward = reward\n","        self.next_state = next_state\n","        self.g_return = 0.0\n","        self.log_probs = log_probs\n","\n","\n","class Episode(object):\n","    \"\"\"Class for collecting an episode of transitions\n","    \"\"\"\n","    def __init__(self, discount):\n","        self.discount = discount\n","        self._empty()\n","        self.total_reward = 0.0\n","\n","    def _empty(self):\n","        self.n = 0\n","        self.transitions = []\n","\n","    def reset(self):\n","        self._empty()\n","\n","    def size(self):\n","        return self.n\n","\n","    def append(self, transition):\n","        self.transitions.append(transition)\n","        self.n += 1\n","\n","    def states(self):\n","        return [s.state for s in self.transitions]\n","\n","    def actions(self):\n","        return [a.action for a in self.transitions]\n","\n","    def rewards(self):\n","        return [r.reward for r in self.transitions]\n","\n","    def next_states(self):\n","        return [s_.next_state for s_ in self.transitions]\n","\n","    def returns(self):\n","        return [r.g_return for r in self.transitions]\n","\n","    def calculate_return(self):\n","        # calculate the return of the episode\n","        rewards = self.rewards()\n","        trajectory_len = len(rewards)\n","        return_array = torch.zeros((trajectory_len,))\n","        g_return = 0.\n","        for i in range(trajectory_len-1, -1, -1):\n","            g_return = rewards[i] + self.discount*g_return\n","            return_array[i] = g_return\n","            self.transitions[i].g_return = g_return\n","        return return_array\n","\n","\n","class BufferDataset(Dataset):\n","    \"\"\"Buffer dataset used to iterate over buffer samples when training.\n","    \"\"\"\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        t = self.data[idx]\n","        return t.state, t.action, t.reward, t.next_state, t.log_probs\n","\n","\n","class RolloutBuffer(object):\n","    # ===================================================\n","    # ++++++++++++++++++++ OPTIONAL +++++++++++++++++++++\n","    # ===================================================\n","    # > feel free to optimize sampling and buffer handling\n","    # ===================================================\n","    \"\"\"Buffer to collect samples while rolling out in the envrionment.\n","    \"\"\"\n","    def __init__(self, capacity, batch_size, min_transitions):\n","        self.capacity = capacity\n","        self.batch_size = batch_size\n","        self.min_transitions = min_transitions\n","        self.buffer = []\n","        self._empty()\n","        self.mean_returns = []\n","        self.all_returns = []\n","\n","    def _empty(self):\n","        # empty the buffer\n","        del self.buffer[:]\n","        self.position = 0\n","\n","    def add(self, episode):\n","        # Saves a transition\n","        episode.calculate_return()\n","        for t in episode.transitions:\n","            if len(self.buffer) < self.capacity:\n","                self.buffer.append(None)\n","            self.buffer[self.position] = t\n","            self.position = (self.position + 1) % self.capacity\n","\n","    def update_stats(self):\n","        # update the statistics on the buffer\n","        returns = [t.g_return for t in self.buffer]\n","        self.all_returns += returns\n","        mean_return = np.mean(np.array(returns))\n","        self.mean_returns += ([mean_return]*len(returns))\n","\n","    def reset(self):\n","        # calls empty\n","        self._empty()\n","\n","    def create_dataloader(self):\n","        # creates a dataloader for training\n","        train_loader = DataLoader(\n","            BufferDataset(self.buffer),\n","            batch_size=self.batch_size,\n","            shuffle=True\n","        )\n","        return train_loader\n","\n","    def __len__(self):\n","        return len(self.buffer)"]},{"cell_type":"markdown","metadata":{"id":"qDtLH0LcrCvz"},"source":["### Define Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4nI-GHnHCtT"},"outputs":[],"source":["class ActorNet(nn.Module):\n","    \"\"\"Actor network (policy)\n","    \"\"\"\n","    def __init__(self, state_size, action_size, hidden_size):\n","        super(ActorNet, self).__init__()\n","        # ===================================================\n","        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n","        # ===================================================\n","        # 1) find a suitable architecture for the actor\n","        # ===================================================\n","        # pass\n","        self.norm = nn.LayerNorm(hidden_size, elementwise_affine=False)\n","        self.fc1 = nn.Linear(state_size, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.mu = nn.Linear(hidden_size, action_size)\n","        self.sigma = nn.Linear(hidden_size, action_size)\n","\n","    def forward(self, x):\n","        # ===================================================\n","        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n","        # ===================================================\n","        # 1) compute mu and sigma logit estimates\n","        # ===================================================\n","        h = torch.relu(self.norm(self.fc1(x)))\n","        h = torch.relu(self.norm(self.fc2(h)))\n","        mu_logits = self.mu(h)\n","        sigma_logits = self.sigma(h)\n","        return mu_logits, sigma_logits # must return this!\n","\n","\n","class CriticNet(nn.Module):\n","    \"\"\"Critic network computing the state value\n","    \"\"\"\n","    def __init__(self, state_size, action_size, hidden_size):\n","        super(CriticNet, self).__init__()\n","        # ===================================================\n","        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n","        # ===================================================\n","        # 1) find a suitable architecture for the critic\n","        # ===================================================\n","        # pass\n","        self.norm = nn.LayerNorm(hidden_size, elementwise_affine=False)\n","        self.dense_layer_1 = nn.Linear(state_size, hidden_size)\n","        self.dense_layer_2 = nn.Linear(hidden_size, hidden_size)\n","        self.output = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        # ===================================================\n","        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n","        # ===================================================\n","        # 1) compute the value estimates\n","        # ===================================================\n","        # pass\n","        h = torch.relu(self.norm(self.dense_layer_1(x)))\n","        h = torch.relu(self.norm(self.dense_layer_2(h)))\n","\n","        return self.output(h)\n","\n","\n","class ActorCriticNet(nn.Module):\n","    \"\"\"Combining both networks and add helper methods to act and evaluate samples.\n","    \"\"\"\n","    def __init__(self, state_size, action_size, hidden_size):\n","        super(ActorCriticNet, self).__init__()\n","        # ===================================================\n","        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n","        # ===================================================\n","        # 1) initialize your networks\n","        # ===================================================\n","        # pass\n","        self.actor = ActorNet(state_size, action_size, hidden_size)\n","        self.critic = CriticNet(state_size, action_size, hidden_size)\n","\n","    def forward(self, x):\n","        x = x.reshape(1, -1)\n","        return self.act(x)\n","\n","    def act(self, state):\n","        # ===================================================\n","        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n","        # ===================================================\n","        # 1) get mean and sigma logits\n","        # 2) softplus the sigma logits to ensure only positive values (add some eps value to avoid 0 probabilities)\n","        # 3) create a Normal distribution based on mu and sigma\n","        # 4) sample from the distribution\n","        # 5) get log probabilities\n","        # ===================================================\n","        # pass\n","        mu_logits, sigma_logits = self.actor(state)\n","        sigma_logits = F.softplus(sigma_logits) + 1e-5\n","        dist = Normal(mu_logits, sigma_logits)\n","        action = dist.sample().squeeze() \n","        log_probs = dist.log_prob(action)\n","\n","        return action, log_probs\n","\n","    def evaluate(self, state, action):\n","        # ===================================================\n","        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n","        # ===================================================\n","        # 1) get mean and sigma logits\n","        # 2) softplus the sigma logits to ensure only positive values (add some eps value to avoid 0 probabilities)\n","        # 3) create a Normal distribution based on mu and sigma\n","        # 4) get log probabilities\n","        # 5) get entropy from the distribution\n","        # 6) evaluate state value\n","        # ===================================================\n","        # pass\n","        mu_logits, sigma_logits = self.actor(state)\n","        sigma_logits = F.softplus(sigma_logits) + 1e-5\n","        policy_dist = Normal(mu_logits, sigma_logits)\n","        log_probs = policy_dist.log_prob(action)\n","        entropy = policy_dist.entropy()\n","        state_value = self.critic(state)\n","\n","        return log_probs, entropy, state_value"]},{"cell_type":"markdown","metadata":{"id":"gqR7Fi06rCv8"},"source":["### Define Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUNQOvfqrCv9"},"outputs":[],"source":["class Agent(object):\n","    \"\"\"Agent class used for training, saving data and handling the model.\n","    \"\"\"\n","    def __init__(self, buffer, state_size, action_size, hidden_size, learning_rate, logger, eps_clip, n_epochs,\n","                 weight_decay, betas, loss_scales, discount, checkpoint_dir=\"ckpts\"):\n","        self.action_size = action_size\n","        self.state_size = state_size\n","        self.buffer = buffer\n","        self.checkpoint_dir = checkpoint_dir\n","        self.loss_scales = loss_scales\n","        self.n_epochs = n_epochs\n","        self.eps_clip = eps_clip\n","        self.logger = logger\n","        self.discount = discount\n","        # ===================================================\n","        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n","        # ===================================================\n","        # 1) initialize ActorCriticNet instance\n","        # 2) create initializers\n","        # ===================================================\n","        # pass\n","        self.model = ActorCriticNet(state_size, action_size, hidden_size).to(device)\n","        self.optimizer = torch.optim.Adam(\n","            self.model.parameters(),\n","            lr=learning_rate,\n","            weight_decay=weight_decay,\n","            betas=betas,\n","        )\n","\n","\n","    def save_checkpoint(self, epoch, info=''):\n","        \"\"\"Saves a model checkpoint\"\"\"\n","        state = {\n","            'info': info,\n","            'epoch': epoch,\n","            'state_dict': self.model.state_dict(),\n","            'optimizer': self.optimizer.state_dict()\n","        }\n","        os.makedirs(self.checkpoint_dir, exist_ok=True)\n","        ckp_name = 'best-checkpoint.pth' if info == 'best' else f'checkpoint-epoch{epoch}.pth'\n","        filename = os.path.join(self.checkpoint_dir, ckp_name)\n","        torch.save(state, filename)\n","\n","    def resume_checkpoint(self, resume_path):\n","        \"\"\"Resumes training from an existing model checkpoint\"\"\"\n","        print(\"Loading checkpoint: {} ...\".format(resume_path))\n","        checkpoint = torch.load(resume_path)\n","        # load architecture params from checkpoint.\n","        self.model.load_state_dict(checkpoint['state_dict'])\n","        # load optimizer state from checkpoint only when optimizer type is not changed.\n","        self.optimizer.load_state_dict(checkpoint['optimizer'])\n","        print(\"Checkpoint loaded. Resume training\")\n","\n","    def load_onnx_checkpoint(self, onnx_file):\n","        self.model.actor = ConvertModel(onnx.load(onnx_file)).to(device)\n","        self.model.eval()\n","\n","    def save_onnx_checkpoint(self):\n","        \"\"\"Create an ONNX checkpoint\"\"\"\n","        dummy_input = torch.randn((1, self.state_size))\n","        dummy_input_t = transforms(dummy_input).to(device)\n","        model = agent.model.actor.to(device)\n","        torch.onnx.export(model, dummy_input_t, \"submission_actor.onnx\", verbose=False, opset_version=10, export_params=True, do_constant_folding=True)\n","\n","    def check(self, file_name):\n","        # Load the ONNX model\n","        model = onnx.load(file_name)\n","        # Check that the IR is well formed\n","        onnx.checker.check_model(model)\n","\n","    @torch.no_grad()\n","    def act(self, state):\n","        # ===================================================\n","        # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n","        # ===================================================\n","        # 1) prepare state tensors\n","        # 2) check if shape is ok or expand acordingly\n","        # 3) get action and log probabilities\n","        # ===================================================\n","        # get action probs then sample from the probabilities\n","        # pass\n","        state_tensor = torch.FloatTensor(state).to(device)\n","\n","        if len(state_tensor.shape) == 1:\n","            state_tensor = state_tensor.unsqueeze(0)\n","\n","        action, log_probs = self.model.act(state_tensor)\n","\n","        return action.cpu(), log_probs\n","\n","    def train(self):\n","        for i in range(self.n_epochs):\n","            # create the a dataloader based on the current buffer\n","            loader = self.buffer.create_dataloader()\n","            # iterate over the samples in the dataloader\n","            for states, actions, rewards, next_states, old_log_probs in loader:\n","                # ===================================================\n","                # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n","                # ===================================================\n","                # 1) compute target value with next state and reward\n","                # 2) compute advantage function from target and current state and action\n","                # 3) compute importance sampling ratio from log probabilities\n","                # 4) compute surrogate loss with the advantage and clipped surrogate loss\n","                # 5) compute value losses\n","                # 6) compute total loss with entropy regularization\n","                # 7) compute gradients and perform optimization step\n","                # ===================================================\n","\n","                states = states.detach().to(device)\n","                actions = actions.detach().to(device)\n","                rewards = rewards.detach().to(device)\n","                next_states = next_states.detach().to(device)\n","                old_log_probs = old_log_probs.detach().to(device)\n","\n","                rewards = rewards.view(-1, 1)\n","\n","                # normalize rewards\n","                rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n","\n","                log_probs, entropy, state_values = self.model.evaluate(states, actions)\n","\n","                target_value = (\n","                    rewards + self.discount * self.model.critic(next_states).detach()\n","                )\n","\n","                advantages = target_value - state_values.detach()\n","\n","                # normalize advantages\n","                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-5)\n","\n","                ratios = torch.exp(log_probs - old_log_probs.squeeze())\n","\n","                clipped_ratio = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip)\n","\n","                policy_loss = -torch.min(\n","                    ratios * advantages, clipped_ratio * advantages\n","                ).sum(dim=-1)\n","\n","                value_loss = F.mse_loss(state_values.float(), target_value.float())\n","\n","                loss = (\n","                    self.loss_scales[0] * value_loss\n","                    + self.loss_scales[1] * policy_loss\n","                    - self.loss_scales[2] * entropy.view(-1, 1)\n","                )\n","\n","                self.optimizer.zero_grad()\n","                loss.mean().backward()\n","                self.optimizer.step()\n","\n","        # return losses and entropy\n","        return (loss.mean().detach().cpu().numpy(),\n","                value_loss.mean().detach().cpu().numpy(),\n","                policy_loss.mean().detach().cpu().numpy()), entropy.mean().detach().cpu().numpy()"]},{"cell_type":"markdown","metadata":{"id":"5tPRbfRirCwA"},"source":["### Define Task Runner"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgTjRX5FrCwB"},"outputs":[],"source":["from collections import deque\n","class Runner(object):\n","    \"\"\"Runner class performing the rollout in the environment and calling the agent training function periodically\n","    \"\"\"\n","    def __init__(self, env, agent, buffer, logger, discount=0.99, n_episodes=4000, reward_scale=lambda x: x,\n","                 use_buffer_reset=True, stats_interval=1, print_stats=True, min_average_reward=300, render=False, seed=42, checkpoint_interval=100, max_episode_steps=2000):\n","        self.env = env\n","        self.agent = agent\n","        self.buffer = buffer\n","        self.render = render\n","        self.logger = logger\n","        self.discount = discount\n","        self.n_episodes = n_episodes\n","        self.reward_scale = reward_scale\n","        self.use_buffer_reset = use_buffer_reset\n","        self.stats_interval = stats_interval\n","        self.print_stats = print_stats\n","        self.min_average_reward = min_average_reward\n","        self.seed = seed\n","        self.max_episode_steps = max_episode_steps\n","        # store stats for plotting\n","        self.stats_rewards_list = []\n","        self.stats_entropy_list = []\n","        # stats for running episodes\n","        self.timesteps = 0\n","        self.checkpoint_interval = checkpoint_interval\n","        self.best_model = None\n","        self.max_reward = -np.inf\n","\n","        # Additional param for monitor\n","        self.number_last_rewards = 100\n","        self.last_rewads = deque(maxlen=self.number_last_rewards)\n","        self.all_rewards = []\n","        self.recorded_episodes = []\n","\n","    def run(self):\n","        fig, ax = plt.subplots()\n","        display_handle = display(fig, display_id=True)\n","        self.agent.model.eval()\n","        # train for n episodes\n","        with tqdm(range(self.n_episodes)) as pbar:\n","            for e in pbar:\n","                # reset env and stats\n","                state, info = self.env.reset(seed=self.seed)\n","                total_losses = 0.\n","                value_losses = 0.\n","                policy_losses = 0.\n","\n","                # create new episode\n","                episode = Episode(discount=self.discount)\n","\n","                # save model\n","                if e % self.checkpoint_interval == 0:\n","                    self.agent.save_checkpoint(e)\n","\n","                done = False\n","                # train in each episode until episode is done\n","                while not done:\n","                    self.timesteps += 1\n","                    # render env\n","                    if self.render: self.env.render()\n","\n","                    # ===================================================\n","                    # ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n","                    # ===================================================\n","                    # 1) get action and log_probs\n","                    # 2) step in the environment to collect transitions\n","                    # 3) uptade agent when buffer has sufficient samples\n","                    # 4) log statistics and reset buffer\n","                    # ===================================================\n","\n","                    # 1) get action and log_probs\n","                    action, log_probs = self.agent.act(state)\n","\n","                    # 2) step in the environment to collect transitions\n","                    # next_state, reward, terminated, truncated, info\n","                    next_state, reward, terminated, truncated, _ = self.env.step(\n","                        action.numpy()\n","                    )\n","                    done = terminated or truncated\n","\n","                    episode.total_reward += reward\n","                    episode.append(\n","                        Transition(\n","                            state,\n","                            action,\n","                            reward * self.reward_scale,\n","                            next_state,\n","                            log_probs,\n","                        )\n","                    )\n","\n","                    # update stats and update agent if done\n","                    if done:\n","                        # add current episode to the replay buffer\n","                        self.buffer.add(episode)\n","                        self.stats_rewards_list.append(\n","                            (e, episode.total_reward, episode.size())\n","                        )\n","\n","                        # 3) skip if stored episodes are less than the batch size\n","                        if len(self.buffer) < self.buffer.min_transitions:\n","                            break\n","\n","                        (\n","                            total_loss,\n","                            value_loss,\n","                            policy_loss,\n","                        ), entropy = self.agent.train()\n","                        total_losses += total_loss\n","                        value_losses += value_loss\n","                        policy_losses += policy_loss\n","\n","                        self.stats_entropy_list.append(entropy)\n","                        self.buffer.update_stats()\n","\n","                        if self.use_buffer_reset:\n","                            self.buffer.reset()\n","\n","                        if self.print_stats and e % self.stats_interval == 0:\n","                            score = np.mean(\n","                                self.stats_rewards_list[-self.stats_interval :],\n","                                axis=0,\n","                            )[1]\n","\n","                            # save episode number for xticks\n","                            self.last_rewads.append(score)\n","                            self.all_rewards.append(score)\n","                            self.recorded_episodes.append(e)\n","\n","                            pbar.set_description(\n","                                f\"[{e:03d}] Reward: {score:.4f} \"\n","                                f\"| Last {self.number_last_rewards}: \"\n","                                f\"{int(round(np.min(list(self.last_rewads))))} \"\n","                                f\"{int(round(np.mean(list(self.last_rewads))))} \"\n","                                f\"{int(round(np.max(list(self.last_rewads))))} \"\n","                            )\n","\n","                            self.plot_running_rewards(ax)\n","                            display_handle.update(fig)\n","\n","                    state = next_state\n","\n","                # save best model\n","                if self.best_model is None or episode.total_reward > self.max_reward:\n","                    self.best_model = copy.deepcopy(self.agent.model)\n","                    self.agent.save_checkpoint(e, 'best')\n","                    self.max_reward = episode.total_reward\n","\n","    @torch.no_grad()\n","    def plot_running_rewards(self, ax: plt.Axes):\n","        plt.cla()\n","        ax.set_title(\"Training...\")\n","\n","        ax.set_xlabel(\"Episode\")\n","        ax.set_ylabel(\"Reward\")\n","        ax.plot(self.recorded_episodes, self.all_rewards, label=\"Rewards\")\n","\n","        all_rewards = torch.tensor(self.all_rewards, dtype=torch.float)\n","        means = (\n","            all_rewards.unfold(\n","                0, min(all_rewards.shape[0], self.number_last_rewards), 1\n","            )\n","            .mean(1)\n","            .flatten()\n","        )\n","        means = torch.cat(\n","            (\n","                torch.tensor(\n","                    [\n","                        all_rewards[: i + 1].mean()\n","                        for i in range(\n","                            min(all_rewards.shape[0] - 1, self.number_last_rewards - 1)\n","                        )\n","                    ]\n","                ),\n","                means,\n","            )\n","        )\n","\n","        ax.plot(self.recorded_episodes, means.numpy(), label=f\"Mean\")\n","\n","        ax.legend(bbox_to_anchor=(1.1, 1.05))\n","        ax.tick_params(axis=\"x\", labelrotation=90)\n","        ax.set_xticks(np.arange(0, self.recorded_episodes[-1], 100))"]},{"cell_type":"markdown","metadata":{"id":"YRZITOVcrCwE"},"source":["### Train Agent"]},{"cell_type":"markdown","metadata":{"id":"aWQCoVPs7jFv"},"source":["This is a where you run your actor-critic PPO algorithm.\n","\n","Some tips as to what you need to implement:\n","- chose proper architectures for the actor-critic networks\n","- choose the appropriate loss function (think on which kind of problem you are solving)\n","- choose an optimizer and its hyper-parameters\n","- optional: use a learning-rate scheduler\n","- store your model and training progress often so you don't loose progress if your program crashes\n","\n","In case you use the provided Logger:\n","- `logger.log(\"training_loss\", <loss-value>)` to log a particular value\n","- `logger.dump()` to write the current logs to a log file (e.g. after every episode)\n","- `logger.log_dir`, `logger.param_file`, `logger.onnx_file`, `logger.video_dir` point to files or directories you can use to save files\n","- you might want to specify your google drive folder as a logdir in order to automatically sync your results\n","- if you log the metrics specified in the `plot_metrics` function you can use it to visualize your training progress (or take it as a template to plot your own metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J2Bp93eSrCwF"},"outputs":[],"source":["# create environment\n","logger = Logger(\"logdir\")\n","env, action_size, state_size = create_env(logger, capture_video=False)\n","\n","seed = 31\n","# set seed\n","set_seed(env, seed=seed)\n","\n","# ===================================================\n","# ++++++++++++++++ YOUR CODE HERE +++++++++++++++++++\n","# ===================================================\n","# > find suitable hyperparameters\n","# hyperparameters\n","episodes = 2300 # run agent for this many episodes\n","epochs = 8 # run n epochs of network updates\n","hidden_size = 128 # number of units in NN hidden layers\n","learning_rate = 0.0003 # learning rate for optimizer\n","discount = 0.99 # discount factor gamma value\n","reward_scale = 1.0 #lambda x: x # reward scaling\n","\n","batch_size = 128 # number of samples used for an update\n","min_transitions = 2000 # number of minimum transitions until update is triggered\n","capacity = 2000 # maximum number of transitions stored in the rollout buffer\n","\n","use_buffer_reset = True # resets the buffer after every update\n","eps_clip = 0.2 # clipping or importance sampling updates\n","loss_scales = (1.0, 0.5, 0.01) # loss scales (value loss, policy loss, entropy loss)\n","betas = (0.9, 0.999)  # optimizer beta parameters\n","weight_decay = 0.0001 # optimizer weight decay\n","checkpoint_interval = 100 # checkpoint interval to overwrite the parameters\n","\n","# additional settings\n","print_stats = True\n","render = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjSzAvdRrCwJ"},"outputs":[],"source":["buffer = RolloutBuffer(capacity=capacity, batch_size=batch_size, min_transitions=min_transitions)\n","agent = Agent(buffer=buffer, state_size=state_size, action_size=action_size, hidden_size=hidden_size,\n","              learning_rate=learning_rate, logger=logger, eps_clip=eps_clip, n_epochs=epochs, weight_decay=weight_decay, betas=betas, loss_scales=loss_scales, discount=discount)\n","runner = Runner(env=env, agent=agent, buffer=buffer, logger=logger, discount=discount, n_episodes=episodes, reward_scale=reward_scale,\n","                use_buffer_reset=use_buffer_reset, print_stats=print_stats, render=render, seed=seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcrKd1wdrCwM"},"outputs":[],"source":["runner.run()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDYzZesLftuD"},"outputs":[],"source":["plot_results(runner)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPZIWvHYMcPd"},"outputs":[],"source":["agent.save_onnx_checkpoint()"]},{"cell_type":"markdown","metadata":{"id":"mJNYxX6dMmj_"},"source":["## Visualize Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcYYYwtZme8L"},"outputs":[],"source":["# load model from checkpoint\n","agent.resume_checkpoint('ckpts/best-checkpoint.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-hyu2IQrCwU"},"outputs":[],"source":["# run agent in the envrionment\n","env, _, _ = create_env(logger)\n","agent.model.eval()\n","test_environment(env=env, agent=agent)\n","show_video(logger)"]},{"cell_type":"markdown","metadata":{"id":"qOPIDEs-58o6"},"source":["# Demo Evaluations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YR3iGr0p6AvU"},"outputs":[],"source":["agent.load_onnx_checkpoint('submission_actor.onnx')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01j00s22JAtz"},"outputs":[],"source":["# run agent in the envrionment\n","logger = Logger('logdir')\n","env, _, _ = create_env(logger)\n","agent.model.eval()\n","test_environment(env=env, agent=agent, n_steps=500)\n","show_video(logger)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ki7osku6H0j"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":0}

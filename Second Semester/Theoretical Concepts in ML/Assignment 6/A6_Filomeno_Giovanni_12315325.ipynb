{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12351ec8",
   "metadata": {},
   "source": [
    "# Assignment 6: Neural Ordinary Differential Equations\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors. \n",
    "\n",
    "*This assignment discusses the following paper:* Chen, Ricky TQ, Yulia Rubanova, Jesse Bettencourt, and David K. Duvenaud. \"Neural ordinary differential equations.\" Advances in neural information processing systems 31 (2018). https://arxiv.org/abs/1806.07366"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c865304",
   "metadata": {},
   "source": [
    "## Exercise 1: Residual layer and Euler's method\n",
    "\n",
    "Consider a fully connected layer with residual connection defined as\n",
    "\\begin{align*}\n",
    "    h_{n+1} = h_n + f(h_n, \\theta),\n",
    "\\end{align*}\n",
    "where $f(h_n, \\theta)$ is a neural network with parameters $\\theta$. \n",
    "Show that this is equivalent to applying the Euler method to an ODE of the form\n",
    "\\begin{align*}\n",
    "    \\frac{d h(t)}{dt} = f(h(t), \\theta). \n",
    "\\end{align*}\n",
    "Show both directions and discuss their . \n",
    "What step size do we have to use for Euler's method to establish the equivalence? \n",
    "What is the appropriate initial-value problem for this setting? \n",
    "Consider a ResNet with different parameters in each layer, i.e., $h_{n+1} = h_n + f(h_n, \\theta_n)$. \n",
    "How could we modify the Neural ODE to reflect that property? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a68184",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "##### Equivalence between the residual layer and the Euler method\n",
    "\n",
    "From the exercise description:\n",
    "$$\n",
    "h_{n+1} = h_n + f(h_n, \\theta)\n",
    "$$\n",
    "where $f(h_n, \\theta)$ is the output of a neural network using parameters $\\theta$ and input $h_n$.\n",
    "\n",
    "The Euler method for solving this ODE with a step size $\\Delta t$ is defined as:\n",
    "\n",
    "$$\n",
    "h(t + \\Delta t) = h(t) + \\Delta t \\cdot f(h(t), \\theta)\n",
    "$$\n",
    "Comparing the equation, it is clear that with $\\Delta t = 1$, the equivalence is prooved:\n",
    "\n",
    "$$\n",
    "h_{n+1} = h_n + 1 \\cdot f(h_n, \\theta) = h_n + f(h_n, \\theta)\n",
    "$$\n",
    "Each step of the residual layer can then be considered as one Euler step for the ODE, with a step size of $\\Delta t = 1$.\n",
    "\n",
    "##### Step size and initial-value problem\n",
    "\n",
    "Since we are considering the residual network layer equation as a discrete update using the Euler equivalence, we could use the same approach for Euler update:\n",
    "$$\n",
    "h(0) = h_0\n",
    "$$\n",
    "where $h_0$ is the initial value of $h$ at time $t = 0$. This corresponds to the input to the first layer of the neural network.\n",
    "\n",
    "##### Modifications for varying parameters $\\theta_n$\n",
    "\n",
    "If different parameters are used in each layer with $\\theta_n$ instead of a constant $\\theta$, we have to consider it in the derivation as:\n",
    "$$\n",
    "\\frac{d h(t)}{dt} = f(h(t), \\theta(t))\n",
    "$$\n",
    "where $\\theta(t)$ changes at discrete times corresponding to layer transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a6940",
   "metadata": {},
   "source": [
    "## Exercise 2: A scalar linear ODE\n",
    "\n",
    "In the simplest case, $f$ is scalar and linear, i.e., $f : \\mathbb R \\to \\mathbb R$ has the form $f(h) = wh$. \n",
    "Solve the ODE \n",
    "\\begin{align*}\n",
    "    \\frac{dh}{dt} = wh\n",
    "\\end{align*}\n",
    "by separation of variables. \n",
    "Moreover, prove that the inhomogeneous ODE\n",
    "\\begin{align*}\n",
    "    \\frac{dh}{dt} = wh + b\n",
    "\\end{align*}\n",
    "has the solution \n",
    "\\begin{align*}\n",
    "    h = \\exp(w(t+c)) - \\frac{b}{w}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdfb306",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "##### Homogeneous ODE Solution via Separation of Variables\n",
    "\n",
    "Considering the ODE of the exercise:\n",
    "$$\n",
    "\\frac{dh}{dt} = wh\n",
    "$$\n",
    "where $w$ is a constant.\n",
    "\n",
    "To solve this using the method of separation of variables, we have to modify the equation as:\n",
    "$$\n",
    "\\frac{dh}{h} = w dt\n",
    "$$\n",
    "\n",
    "Integrate both sides:\n",
    "$$\n",
    "\\int \\frac{1}{h} dh = \\int w dt\n",
    "$$\n",
    "\n",
    "Which leads to:\n",
    "$$\n",
    "\\ln |h| = wt + C\n",
    "$$\n",
    "where $C$ is the constant of integration.\n",
    "\n",
    "Resolving the equation for $h$:\n",
    "$$\n",
    "|h| = e^{wt + C} = e^C e^{wt}\n",
    "$$\n",
    "\n",
    "At the end, we can consider $e^C$ as costant.\n",
    "\n",
    "##### Inhomogeneous ODE Solution\n",
    "\n",
    "To find a solution of the form $h = \\exp(w(t+c)) - \\frac{b}{w}$, let's substitute $h$ into the given equation:\n",
    "$$\n",
    "h = e^{w(t+c)} - \\frac{b}{w}\n",
    "$$\n",
    "\n",
    "Taking the derivative $\\frac{dh}{dt}$ gives:\n",
    "$$\n",
    "\\frac{dh}{dt} = w e^{w(t+c)}\n",
    "$$\n",
    "\n",
    "Substituting $\\frac{dh}{dt}$ and $h$ into the given ODE:\n",
    "$$\n",
    "w e^{w(t+c)} = w \\left( e^{w(t+c)} - \\frac{b}{w} \\right) + b\n",
    "$$\n",
    "\n",
    "which leads to:\n",
    "$$\n",
    "w e^{w(t+c)} = w e^{w(t+c)} - b + b\n",
    "$$\n",
    "\n",
    "Ending with:\n",
    "$$\n",
    "w e^{w(t+c)} = w e^{w(t+c)}\n",
    "$$\n",
    "\n",
    "This proof that $h = \\exp(w(t+c)) - \\frac{b}{w}$ is indeed a solution to the inhomogeneous equation. The constant $c$ can be determined from initial conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10747e93-5968-47c9-8519-eff2f04fb7e1",
   "metadata": {},
   "source": [
    "## Exercise 3: A scalar nonlinear ODE\n",
    "\n",
    "Solve the nonlinear ODE\n",
    "\\begin{align*}\n",
    "    \\frac{dh}{dt} = e^{wh + b}\n",
    "\\end{align*}\n",
    "by separation of variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba344d26-26a7-4544-96cf-27a7a90a7060",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "##### Separating Variables\n",
    "\n",
    "Rearranging the equation to isolate terms involving $h$ and $t$:\n",
    "$$\n",
    "\\frac{dh}{e^{wh + b}} = dt\n",
    "$$\n",
    "\n",
    "##### Integrate the left side:\n",
    "\n",
    "$$\n",
    "\\int \\frac{1}{e^{wh + b}} dh\n",
    "$$\n",
    "\n",
    "Using the substitution $u = wh + b$, thus $du = w dh$ and $dh = \\frac{du}{w}$. Substitute into the integral:\n",
    "$$\n",
    "\\int \\frac{1}{e^u} \\frac{du}{w} = \\frac{1}{w} \\int e^{-u} du\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{w} \\int e^{-u} du = \\frac{-1}{w} e^{-u} + C\n",
    "$$\n",
    "Substitute back for $u$:\n",
    "$$\n",
    "\\frac{-1}{w} e^{-(wh + b)} + C\n",
    "$$\n",
    "\n",
    "##### Integrate the right side:\n",
    "\n",
    "$$\n",
    "\\int dt = t + D\n",
    "$$\n",
    "\n",
    "where $D$ is the constant of integration.\n",
    "\n",
    "##### Solve for $h(t)$\n",
    "\n",
    "After integration, equate the two expressions:\n",
    "$$\n",
    "\\frac{-1}{w} e^{-(wh + b)} + C = t + D\n",
    "$$\n",
    "\n",
    "To solve for $h$:\n",
    "$$\n",
    "e^{-(wh + b)} = -w(t + D - C)\n",
    "$$\n",
    "\n",
    "Taking the natural logarithm of both sides (assuming  $-w(t + D - C) > 0$ to keep the logarithm real):\n",
    "\n",
    "$$\n",
    "-(wh + b) = \\ln(-w(t + D - C))\n",
    "$$\n",
    "\n",
    "$$\n",
    "wh + b = -\\ln(-w(t + D - C))\n",
    "$$\n",
    "\n",
    "$$\n",
    "h = \\frac{-\\ln(-w(t + D - C)) - b}{w}\n",
    "$$\n",
    "\n",
    "Considering that $D$ and $C$ are constants, they can be combined into $K$:\n",
    "\n",
    "$$\n",
    "h = \\frac{-\\ln(-w(t + K)) - b}{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39705fd2",
   "metadata": {},
   "source": [
    "## Exercise 4: A multidimensional linear ODE\n",
    "\n",
    "Consider the multidimensional case $f(h) = Wh$ where $h \\in \\mathbb R^d, W \\in \\mathbb R^{d \\times d}$. \n",
    "Prove that the ODE\n",
    "\\begin{align*}\n",
    "    \\frac{dh}{dt} = Wh\n",
    "\\end{align*}\n",
    "has the solution\n",
    "\\begin{align*}\n",
    "    h = e^{tW} c\n",
    "\\end{align*}\n",
    "where $c \\in \\mathbb R^d$ is some arbitrary constant vector and\n",
    "\\begin{align*}\n",
    "    e^W = I + W + \\frac12 W^2 + \\cdots = \\sum_{k=0}^\\infty \\frac{1}{k!} W^k\n",
    "\\end{align*}\n",
    "is the matrix exponential function. \n",
    "Moreover, prove that the inhomogeneous ODE\n",
    "\\begin{align*}\n",
    "    \\frac{dh}{dt} = Wh + b\n",
    "\\end{align*}\n",
    "has the solution \n",
    "\\begin{align*}\n",
    "    h = e^{tW} c - W^{-1} b. \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97543e",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "Differentiating $h = e^{tW}c$ with respect to $t$:\n",
    "$$\n",
    "\\frac{dh}{dt} = \\frac{d}{dt}(e^{tW}c)\n",
    "$$\n",
    "\n",
    "Using the derivative of the matrix exponential:\n",
    "$$\n",
    "\\frac{d}{dt}(e^{tW}) = We^{tW}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\frac{dh}{dt} = We^{tW}c\n",
    "$$\n",
    "\n",
    "Given the form of $h$:\n",
    "$$\n",
    "h = e^{tW}c\n",
    "$$\n",
    "\n",
    "Substituting back:\n",
    "$$\n",
    "\\frac{dh}{dt} = Wh\n",
    "$$\n",
    "Therefore $h = e^{tW}c$ is a solution to the homogeneous ODE.\n",
    "\n",
    "##### Solution to the Inhomogeneous ODE\n",
    "\n",
    "Substituting $h = e^{tW}c - W^{-1}b$ into the ODE:\n",
    "$$\n",
    "\\frac{dh}{dt} = \\frac{d}{dt}(e^{tW}c - W^{-1}b)\n",
    "$$\n",
    "$$\n",
    "\\frac{dh}{dt} = We^{tW}c - 0\n",
    "$$\n",
    "Since $W^{-1}b$ is constant with respect to $t$.\n",
    "\n",
    "This simplifies to:\n",
    "$$\n",
    "\\frac{dh}{dt} = We^{tW}c\n",
    "$$\n",
    "\n",
    "Now, evaluating $Wh + b$:\n",
    "$$\n",
    "Wh + b = W(e^{tW}c - W^{-1}b) + b\n",
    "$$\n",
    "$$\n",
    "Wh + b = We^{tW}c - WW^{-1}b + b\n",
    "$$\n",
    "$$\n",
    "Wh + b = We^{tW}c + b - b\n",
    "$$\n",
    "$$\n",
    "Wh + b = We^{tW}c\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\frac{dh}{dt} = Wh + b\n",
    "$$\n",
    "Thus, $h = e^{tW}c - W^{-1}b$ is a solution to the inhomogeneous ODE, assuming $W$ is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e41ffe",
   "metadata": {},
   "source": [
    "## Exercise 5: The adjoint method\n",
    "\n",
    "In the following, let $t \\in [0, 1]$ and let $h(0) = x$ be the input to our neural ODE model, which is given by \n",
    "\\begin{align*}\n",
    "    \\frac{d h(t)}{dt} = f(h(t), \\theta). \n",
    "\\end{align*}\n",
    "We compute the network output as \n",
    "\\begin{align*}\n",
    "    \\hat y = h(1) = h(0) + \\int_0^1 f(h(t), \\theta) dt. \n",
    "\\end{align*}\n",
    "As usual, we have a loss function $L(\\hat y, y)$ and are interested in the gradients with respect to parameters $\\theta$. As with conventional neural networks the key to computing these gradients are the deltas, i.e., the derivative of the loss with respect to the activations, i.e., $\\partial L / \\partial h(t)$ for $t \\in [0, 1]$. \n",
    "Show that they follow an ODE given by\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} \\frac{\\partial L}{\\partial h(t)} = -\\frac{\\partial L}{\\partial h(t)}\\frac{\\partial f(h(t), \\theta)}{\\partial h(t)}. \n",
    "\\end{align*}\n",
    "This ODE is called the adjoint equation. \n",
    "Which initial value problem do we have to solve to obtain the correct gradients? \n",
    "\n",
    "*Hint: For some $\\varepsilon > 0$, use the chain rule*\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial h(t)} = \\frac{\\partial L}{\\partial h(t+\\varepsilon)}\\frac{\\partial h(t+\\varepsilon)}{\\partial h(t)}\n",
    "\\end{align*}\n",
    "*and consider the limit as $\\varepsilon \\to 0$.*\n",
    "\n",
    "*Note that with slight abuse of notation by $\\frac{\\partial f(g(t))}{\\partial g(t)}$ we mean $\\frac{\\partial f(x)}{\\partial x}|_{x=g(t)}$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e070719",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "##### Computing the Adjoint Equation\n",
    "\n",
    "Given the loss $L$, we define the adjoint variable:\n",
    "$$\n",
    "a(t) = \\frac{\\partial L}{\\partial h(t)}\n",
    "$$\n",
    "\n",
    "This variable tracks the changes in $L$ w.r.t. $h$. The aim is to find an ODE for $a$.\n",
    "\n",
    "From the chain rule and using a small increment $\\varepsilon$:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h(t)} = \\frac{\\partial L}{\\partial h(t+\\varepsilon)}\\frac{\\partial h(t+\\varepsilon)}{\\partial h(t)}\n",
    "$$\n",
    "\n",
    "Now, considering the dynamics of $h$:\n",
    "$$\n",
    "h(t+\\varepsilon) = h(t) + \\varepsilon f(h(t), \\theta) + o(\\varepsilon)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial h(t+\\varepsilon)}{\\partial h(t)} = I + \\varepsilon \\frac{\\partial f(h(t), \\theta)}{\\partial h(t)} + o(\\varepsilon)\n",
    "$$\n",
    "\n",
    "Substituting and taking the limit as $\\varepsilon \\to 0$ gives:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h(t)} = \\frac{\\partial L}{\\partial h(t)} + \\varepsilon \\frac{\\partial L}{\\partial h(t)} \\frac{\\partial f(h(t), \\theta)}{\\partial h(t)}\n",
    "$$\n",
    "$$\n",
    "\\frac{d}{dt} \\frac{\\partial L}{\\partial h(t)} = -\\frac{\\partial L}{\\partial h(t)}\\frac{\\partial f(h(t), \\theta)}{\\partial h(t)}\n",
    "$$\n",
    "\n",
    "This is the adjoint equation:\n",
    "$$\n",
    "\\frac{d a(t)}{dt} = -a(t) \\frac{\\partial f(h(t), \\theta)}{\\partial h(t)}\n",
    "$$\n",
    "\n",
    "To compute the gradients w.r.t. $\\theta$ we have4 to solve the equation backward from $t=1$ to $t=0$. Considering $h(1) = \\hat{y}$ we have:\n",
    "\n",
    "$$\n",
    "a(1) = \\frac{\\partial L}{\\partial h(1)} = \\frac{\\partial L}{\\partial \\hat{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ddae2",
   "metadata": {},
   "source": [
    "## Exercise 6: Gradients with respect to parameters\n",
    "\n",
    "Show that the gradients with respect to the parameters follow the ODE\n",
    "\\begin{align*}\n",
    "    \\frac{d}{dt} \\frac{\\partial L}{\\partial \\theta} &= -\\frac{\\partial L}{\\partial h(t)} \\frac{\\partial f(h(t), \\theta)}{\\partial \\theta}. \n",
    "\\end{align*}\n",
    "\n",
    "*Hint: The parameters are shared in time. To correctly account for the usage of $\\theta$ along the trajectory, it is helpful to consider $\\theta(t) = \\theta$ as a constant function of time with $\\dot \\theta(t) = 0$.\n",
    "In that light, for some $\\varepsilon > 0$, use the chain rule*\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial \\theta(t)} = \\frac{\\partial L}{\\partial h(t+\\varepsilon)}\\frac{\\partial h(t+\\varepsilon)}{\\partial \\theta(t)} + \\frac{\\partial L}{\\partial \\theta(t+\\varepsilon)}\\frac{\\partial \\theta(t+\\varepsilon)}{\\partial \\theta(t)}.\n",
    "\\end{align*}\n",
    "\n",
    "What happens when $\\theta(t)$ becomes an arbitrary differentiable function of time instead of a constant? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa42f21f",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "##### Apply the chain rule\n",
    "\n",
    "Given the hint and using a small increment $\\varepsilon > 0$:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial h(t+\\varepsilon)}\\frac{\\partial h(t+\\varepsilon)}{\\partial \\theta} + \\frac{\\partial L}{\\partial \\theta(t+\\varepsilon)}\n",
    "$$\n",
    "Since $\\theta(t+\\varepsilon) = \\theta$ and $\\dot{\\theta}(t) = 0$, the second term simplifies to:\n",
    "$$\n",
    "\\frac{\\partial \\theta(t+\\varepsilon)}{\\partial \\theta} = I \\quad \\text{(Identity matrix)}\n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial h(t+\\varepsilon)}\\frac{\\partial h(t+\\varepsilon)}{\\partial \\theta} + \\frac{\\partial L}{\\partial \\theta}\n",
    "$$\n",
    "Since $h(t+\\varepsilon) \\approx h(t) + \\varepsilon f(h(t), \\theta)$, by differentiation we get:\n",
    "$$\n",
    "\\frac{\\partial h(t+\\varepsilon)}{\\partial \\theta} \\approx \\varepsilon \\frac{\\partial f(h(t), \\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "Differentiating both sides with respect to $t$, and having the limit $\\varepsilon \\to 0$:\n",
    "$$\n",
    "\\frac{d}{dt} \\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial L}{\\partial h(t)} \\frac{\\partial f(h(t), \\theta)}{\\partial \\theta}\n",
    "$$\n",
    "Here we used the fact that:\n",
    "$$\n",
    "\\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial h(t)} \\frac{\\partial h(t)}{\\partial \\theta} \\right) = \\frac{\\partial L}{\\partial h(t)} \\frac{d}{dt} \\left(\\frac{\\partial h(t)}{\\partial \\theta}\\right)\n",
    "$$\n",
    "Since $\\frac{\\partial h(t)}{\\partial \\theta} = \\frac{\\partial f(h(t), \\theta)}{\\partial \\theta}$, we have:\n",
    "$$\n",
    "\\frac{d}{dt} \\frac{\\partial L}{\\partial \\theta} = -\\frac{\\partial L}{\\partial h(t)} \\frac{\\partial f(h(t), \\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "\n",
    "#### What if $ \\theta(t)$ varies with time?\n",
    "\n",
    "If $\\theta$ is now a function of time, $\\theta(t)$, the above derivation would need additional terms:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dt} \\frac{\\partial L}{\\partial \\theta} = -\\frac{\\partial L}{\\partial h(t)} \\frac{\\partial f(h(t), \\theta(t))}{\\partial \\theta(t)} + \\frac{\\partial L}{\\partial \\theta(t)} \\dot{\\theta}(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be1fc5-ecc7-42ad-bf6b-c82a680cb0bd",
   "metadata": {},
   "source": [
    "## Exercise 7: Implement a neural ODE\n",
    "\n",
    "Using a deep learning framework like PyTorch, we can implement Neural ODEs in two different ways. \n",
    "One way is to explicitly implement the forward and backward ODEs as derived in the previous exercises. \n",
    "Alternatively, we can implmement an ODE solver using PyTorch operations. \n",
    "Then PyTorch will differentiate through the solver automatically. \n",
    "This sidesteps the need for an implementation of the adjoint method. \n",
    "The code below implements a Neural ODE module in PyTorch. \n",
    "Add a `forward` function to the class `ODEBlock` that implements the standard Runge-Kutta solver with fixed step size $\\eta$ as defined by\n",
    "\\begin{align*}\n",
    "    h_{n+1} &= h_n + \\frac{\\eta}{6}(k_1 + 2 k_2 + 2 k_3 + k_4) \\\\\n",
    "    t_{n+1} &= t_n + \\eta \\\\\n",
    "    k_1 &= f(t_n, h_n) \\\\\n",
    "    k_2 &= f(t_n + \\frac{\\eta}{2}, h_n + \\eta \\frac{k_1}{2}) \\\\\n",
    "    k_3 &= f(t_n + \\frac{\\eta}{2}, h_n + \\eta \\frac{k_2}{2}) \\\\\n",
    "    k_4 &= f(t_n + \\eta, h_n + \\eta k_3). \n",
    "\\end{align*}\n",
    "Moreover, add a training/validation loop for the CIFAR-10 dataset, train the model, visualize and discuss your results. \n",
    "The provided hyperparameters should work quite well but feel free to experiment with them. \n",
    "What are potential disadvantages of this approach?\n",
    "\n",
    "*Note: The `Cat` module below needs its attribute `t` set accordingly before invoking its `forward` method. \n",
    "The reason for this is design is to enable its utilization with `nn.Sequential`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d88bff-de33-4799-93e2-e8b0b466bba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Architecture based on the ODE-Only Net by\n",
    "# Carrara, F., Amato, G., Falchi, F. and Gennaro, C., 2019, September. \n",
    "# Evaluation of Continuous Image Features Learned by ODE Nets. \n",
    "# In International Conference on Image Analysis and Processing (ICIAP '19) (pp. 432-442). Springer, Cham.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils import parameters_to_vector as p2v\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class Cat(nn.Module):\n",
    "    \"\"\" Concatenate an image tensor x with a feature plane of constant value t. \"\"\"\n",
    "    def __init__(self, t=0):\n",
    "        super().__init__()\n",
    "        self.t = t\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.ones_like(x[:, :1, :, :]) * self.t\n",
    "        return torch.cat([t, x], 1)\n",
    "\n",
    "class ODEBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.GroupNorm(32, dim),\n",
    "            nn.ReLU(),\n",
    "            Cat(),\n",
    "            nn.Conv2d(dim+1, dim, 3, padding=1),\n",
    "            nn.GroupNorm(32, dim),\n",
    "            nn.ReLU(),\n",
    "            Cat(),\n",
    "            nn.Conv2d(dim+1, dim, 3, padding=1),\n",
    "            nn.GroupNorm(32, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t0=0, t1=1, step_size=0.1):\n",
    "        # pass # IMPLEMENT ME!\n",
    "        time = t0\n",
    "        while time < t1:\n",
    "            k1 = self.module(x)\n",
    "            k2 = self.module(x + step_size * k1 / 2)\n",
    "            k3 = self.module(x + step_size * k2 / 2)\n",
    "            k4 = self.module(x + step_size * k3)\n",
    "            x = x + step_size * (k1 + 2 * k2 + 2 * k3 + k4) / 6\n",
    "            time += step_size\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "class ODENet(nn.Module):\n",
    "    def __init__(self, in_dim, ode_dim, out_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, ode_dim, 4, stride=2, padding=1),\n",
    "            ODEBlock(ode_dim),\n",
    "            nn.GroupNorm(32, ode_dim),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(ode_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.module(x)\n",
    "\n",
    "device = torch.device('cpu') # torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "cifar = device.type == 'cuda'\n",
    "batch_size = 256 if device=='gpu' else 128\n",
    "epochs = 10 if device=='gpu' else 3\n",
    "\n",
    "tfm = [\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "] if cifar else [\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "]\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10 if cifar else torchvision.datasets.MNIST\n",
    "trainset = dataset(root='./data', train=True, download=True, transform=transforms.Compose(tfm))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valset = dataset(root='./data', train=False, download=True, transform=transforms.Compose(tfm[3:] if cifar else tfm))\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "model = ODENet(3 if cifar else 1, 128 if cifar else 64, 10, dropout=0.5).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=15 if cifar else 5)\n",
    "# the scheduler should track validation accuracy, hence `mode='max'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb45be5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.2951067095753481\n",
      "Accuracy of the network on the test images: 94 %\n",
      "Epoch 2, Loss: 0.18243690947098518\n",
      "Accuracy of the network on the test images: 98 %\n",
      "Epoch 3, Loss: 0.09252552788005645\n",
      "Accuracy of the network on the test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}')\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test: %d %%' % (100 * correct / total))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfea47ef",
   "metadata": {},
   "source": [
    "##### Potential Disadvantages\n",
    "- Computational cost: adding RK4 involves multiple evaluations of the neural network per integration step\n",
    "- Memory Usage: for the same reason, storing intermadiate status leads to increase of memory usage\n",
    "- Step-size sensitivity: the choice of the step size and integration bounds can affect the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdc07d",
   "metadata": {},
   "source": [
    "## Exercise 8: Implement the adjoint method\n",
    "\n",
    "In this exercise, we want to replace the fixed-step Runge-Kutta solver with an arbitrary solver from the `scipy` package. \n",
    "To this end, write a python class that derives from `torch.autograd.Function` and implements the adjoint method. \n",
    "You can read up [here](https://pytorch.org/docs/stable/notes/extending.html) on how to extend PyTorch. \n",
    "In the forward method, make a call to `scipy.integrate.solve_ivp` to compute the model output solving the forward ODE. \n",
    "In the backward method, make another call to `scipy.integrate.solve_ivp` to compute the gradients by the adjoint method solving the backward ODEs. \n",
    "Then modify the `ODEBlock` class from the previous exercise replacing the Runge-Kutta solver with a call to your `torch.autograd.Function`.  Finally, retrain the model using the adjoint method and visualize and discuss your results comparing them to those of the previous exercise. \n",
    "What are possible up-/downsides of the adjoint method?\n",
    "\n",
    "Note that $\\dot b(t)$ depends on both $a(t)$ and $h(t)$. \n",
    "Although we have computed a trajectory of $h(t)$ in the forward pass, we cannot reuse it in the backward pass since the solver might choose different steps in the backward ODE.\n",
    "Therefore, we have to simultaneously solve 3 ODEs in the backward pass\n",
    " - the trajectory of the hiddens $\\dot h = f(h, \\theta)$\n",
    " - the adjoint $\\dot a = -a \\frac{\\partial f(h, \\theta)}{\\partial h}$\n",
    " - the gradients $\\dot b = -a \\frac{\\partial f(h, \\theta)}{\\partial \\theta}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc763f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.331999325803094\n",
      "Accuracy on test: 9 %\n",
      "Epoch 2, Loss: 2.332078274887508\n",
      "Accuracy on test: 9 %\n",
      "Epoch 3, Loss: 2.3328201897871264\n",
      "Accuracy on test: 9 %\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as ag\n",
    "from torch.autograd.functional import vjp\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "class ODEFuncAutograd(ag.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, t0, t1, h0, func, *theta):\n",
    "        # Flatten h0 for solving\n",
    "        original_shape = h0.shape\n",
    "        h0_flat = h0.detach().numpy().flatten()\n",
    "\n",
    "        def rhs(t, h):\n",
    "            h_torch = torch.tensor(h, dtype=torch.float32).view(original_shape)\n",
    "            h_torch.requires_grad = True\n",
    "            dh = func(t, h_torch, *theta).detach().numpy().flatten()\n",
    "            return dh\n",
    "        \n",
    "        sol = solve_ivp(rhs, [t0, t1], h0_flat, method='RK45')\n",
    "        h_end_flat = torch.tensor(sol.y[:, -1], dtype=torch.float32)\n",
    "        h_end = h_end_flat.view(original_shape)\n",
    "\n",
    "        ctx.save_for_backward(h0, h_end, *theta)\n",
    "        ctx.func = func\n",
    "        return h_end\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        h0, h_end, *theta = ctx.saved_tensors\n",
    "        func = ctx.func\n",
    "        original_shape = h0.shape\n",
    "        grad_output_flat = grad_output.flatten()\n",
    "\n",
    "        def rhs_augmented(t, aug):\n",
    "            aug_torch = torch.tensor(aug, dtype=torch.float32)\n",
    "            h, adj_h = aug_torch[:h0.numel()], aug_torch[h0.numel():2*h0.numel()]\n",
    "            h = h.view(original_shape)\n",
    "            adj_h = adj_h.view(original_shape)\n",
    "            h.requires_grad = True\n",
    "            dh = func(t, h, *theta)\n",
    "            vhp = torch.autograd.grad(dh, (h, *theta), -adj_h)\n",
    "            return np.concatenate([dh.detach().numpy().flatten(), vhp[0].detach().numpy().flatten()] + [v.detach().numpy().flatten() for v in vhp[1:]])\n",
    "\n",
    "        aug0 = np.concatenate([h_end.detach().numpy().flatten(), grad_output_flat.numpy(), np.zeros_like(h0.detach().numpy()).flatten()])\n",
    "        sol = solve_ivp(rhs_augmented, [t1, t0], aug0, method='RK45')\n",
    "        \n",
    "        grad_h0_flat = torch.tensor(sol.y[:h0.numel(), -1], dtype=torch.float32)\n",
    "        grad_theta = tuple(torch.tensor(sol.y[2*h0.numel()+i*h0.numel():2*h0.numel()+(i+1)*h0.numel(), -1], dtype=torch.float32) for i in range(len(theta)))\n",
    "        \n",
    "        grad_h0 = grad_h0_flat.view(original_shape)\n",
    "        return (None, None, grad_h0, *grad_theta)\n",
    "\n",
    "\n",
    "\n",
    "# copy paste from previous with different forward\n",
    "class ODEBlock_2(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.func = nn.Sequential(\n",
    "            nn.GroupNorm(32, dim),\n",
    "            nn.ReLU(),\n",
    "            Cat(),\n",
    "            nn.Conv2d(dim+1, dim, 3, padding=1),\n",
    "            nn.GroupNorm(32, dim),\n",
    "            nn.ReLU(),\n",
    "            Cat(),\n",
    "            nn.Conv2d(dim+1, dim, 3, padding=1),\n",
    "            nn.GroupNorm(32, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t0=0, t1=1):\n",
    "        h0 = x\n",
    "        result = self.func(h0) \n",
    "        return result\n",
    "\n",
    "    \n",
    "\n",
    "# copy paste from previous\n",
    "class ODENet_2(nn.Module):\n",
    "    def __init__(self, in_dim, ode_dim, out_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.module = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, ode_dim, 4, stride=2, padding=1),\n",
    "            ODEBlock_2(ode_dim),\n",
    "            nn.GroupNorm(32, ode_dim),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(ode_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.module(x)\n",
    "    \n",
    "\n",
    "\n",
    "# copy paste from before\n",
    "model_2 = ODENet_2(3 if cifar else 1, 128 if cifar else 64, 10, dropout=0.5).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_2(epoch):\n",
    "    model_2.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(trainloader)}')\n",
    "\n",
    "def validate_2():\n",
    "    model_2.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model_2(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy on test: %d %%' % (100 * correct / total))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_2(epoch)\n",
    "    validate_2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d0e3fb",
   "metadata": {},
   "source": [
    "##### Upsides\n",
    "- Memory Efficiency\n",
    "##### Downsides\n",
    "- Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb9fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

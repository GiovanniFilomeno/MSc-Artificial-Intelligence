{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473abdc3",
   "metadata": {},
   "source": [
    "# Assignment 2: Estimation Theory\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2557559",
   "metadata": {},
   "source": [
    "## Exercise 1: Fisher Information of the Likelihood Function\n",
    "\n",
    "Suppose we draw $n$ i.i.d samples  $\\{x_1, \\ldots, x_n \\}$ from a random variable $X$ with pmf $f(x; \\theta)$, where $\\theta$ is the parameter. The likelihood function is\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\{x_1, \\ldots, x_n \\}; \\theta) &= \\prod^n_{i=1} f(x_i; \\theta) . \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Assuming you know the Fisher Information $I_F(\\theta)$ of one sample under $f(x; \\theta)$, calculate the Fisher Information $I_F^{\\mathcal L}(\\theta)$ of $n$ iid samples, i.e., of the likelihood function. \n",
    "Assume that all regularity conditions are met for $f$ and $\\mathcal{L}$, i.e., you can use the identities\n",
    "\\begin{equation*}\n",
    "I_F^{\\mathcal L}(\\theta) = - \\mathbb{E}_X \\Big[ \\frac{\\partial^2}{\\partial \\theta^2} \\ln \\mathcal L(\\{x_1, \\ldots, x_n \\}; \\theta) \\Big] \\quad \\text{and} \\quad I_F(\\theta) = - \\mathbb{E}_X \\Big[ \\frac{\\partial^2}{\\partial \\theta^2} \\ln f(x; \\theta) \\Big].\n",
    "\\end{equation*}\n",
    "Interpret the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ae9e50",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "##### Step 1: Compute the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\ln \\mathcal{L}(\\{x_1, \\ldots, x_n \\}; \\theta) = \\ln \\left( \\prod^n_{i=1} f(x_i; \\theta) \\right) = \\sum^n_{i=1} \\ln f(x_i; \\theta).\n",
    "$$\n",
    "\n",
    "##### Step 2: Differenciate the log-likelihood w.r.t. $\\theta$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial \\theta^2} \\ln \\mathcal L(\\{x_1, \\ldots, x_n \\}; \\theta) = \\sum^n_{i=1} \\frac{\\partial^2}{\\partial \\theta^2} \\ln f(x_i; \\theta).\n",
    "$$\n",
    "\n",
    "##### Step 3: Compute Fisher Information\n",
    "$$\n",
    "I_F^{\\mathcal L}(\\theta) = - \\mathbb{E}_X \\left[ \\sum^n_{i=1} \\frac{\\partial^2}{\\partial \\theta^2} \\ln f(x_i; \\theta) \\right].\n",
    "$$\n",
    "\n",
    "Given that $I_F(\\theta) = - \\mathbb{E}_X \\left[ \\frac{\\partial^2}{\\partial \\theta^2} \\ln f(x; \\theta) \\right]$ is the Fisher Information of one sample, and since our samples are iid, each term in the summation contributes equally, so the expression simplifies to:\n",
    "\n",
    "$$\n",
    "I_F^{\\mathcal L}(\\theta) = n \\cdot I_F(\\theta).\n",
    "$$\n",
    "\n",
    "##### Comments:\n",
    "The results show that the amount of information about the parameter $\\theta$ in our sample grows linearly with the number of iid samples $n$. In practical terms, as the sample size increases, our estimate of $\\theta$ becomes more precise, reducing the variance of the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432fc2f6",
   "metadata": {},
   "source": [
    "## Exercise 2: Fisher Information of the Bernoulli Distribution\n",
    "\n",
    "Consider a Bernoulli distributed random variable $X$ with probability mass function (pmf)\n",
    "\\begin{align*}\n",
    "f(x ; p) &= p^x(1-p)^{1-x} , \\\\\n",
    "\\end{align*}\n",
    "where $0 \\leq p \\leq 1$ is the parameter and the support is $x \\in \\{0,1\\}$. \n",
    "Calculate the Fisher information. \n",
    "Assume $f$ fulfills all necessary regularity conditions so that you may use the form\n",
    "\\begin{align*}\n",
    "I_F(p) &= -\\mathbb{E}_X \\Big[ \\frac{d^2}{dp^2} \\ln f(x ; p) \\Big]  \\ .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11485d",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "To calculate the Fisher Information $I_F(p)$ for a Bernoulli distributed random variable with pmf $f(x; p) = p^x(1-p)^{1-x}$, where $x \\in \\{0,1\\}$ and $0 \\leq p \\leq 1$, we start by taking the logarithm of the pmf, then find its second derivative with respect to $p$, and finally compute its expected value as required.\n",
    "\n",
    "##### Step 1: Compute the logarithm of the pmf:\n",
    "$$\n",
    "\\ln f(x; p) = x\\ln(p) + (1-x)\\ln(1-p).\n",
    "$$\n",
    "\n",
    "##### Step 2: First derivative w.r.t. $p$\n",
    "$$\n",
    "\\frac{d}{dp} \\ln f(x ; p) = \\frac{x}{p} - \\frac{1-x}{1-p}.\n",
    "$$\n",
    "\n",
    "##### Step 3: Second derivative w.r.t. $p$\n",
    "$$\n",
    "\\frac{d^2}{dp^2} \\ln f(x ; p) = -\\frac{x}{p^2} - \\frac{1-x}{(1-p)^2}.\n",
    "$$\n",
    "\n",
    "##### Step 4: Compute Fisher Information\n",
    "Fisher Information is defined as the negative expected value of this second derivative, so:\n",
    "$$\n",
    "I_F(p) = -\\mathbb{E}_X \\left[ -\\frac{x}{p^2} - \\frac{1-x}{(1-p)^2} \\right] = \\mathbb{E}_X \\left[ \\frac{x}{p^2} + \\frac{1-x}{(1-p)^2} \\right].\n",
    "$$\n",
    "\n",
    "Because $X$ is Bernoulli distributed, we know that $E(X) = p$. Therefore, we can calculate the expected value directly:\n",
    "$$\n",
    "I_F(p) = p \\cdot \\frac{1}{p^2} + (1-p) \\cdot \\frac{1}{(1-p)^2} = \\frac{1}{p} + \\frac{1}{1-p}.\n",
    "$$\n",
    "\n",
    "Upon simplifying this expression, we get:\n",
    "$$\n",
    "I_F(p) = \\frac{1}{p(1-p)}.\n",
    "$$\n",
    "\n",
    "##### Comments\n",
    "The Fisher Information has its max at $p=0.5$ and the minimum at $p=0$ or $p=1$, which means that if an event is sure or impossible, we have less information w.r.t. when the even have some uncertainties. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbe7e67",
   "metadata": {},
   "source": [
    "## Exercise 3: Fisher Information of the Poisson Distribution\n",
    "\n",
    "Consider a Poisson distributed random variable $X$ with pmf\n",
    "\\begin{align*}\n",
    "p(x ; \\lambda) &= \\frac{\\lambda^x}{x!} e^{-\\lambda} \\ , \\\\ \n",
    "\\end{align*}\n",
    "where $\\lambda$ is the parameter and the support is $x \\in \\mathbb{N} \\cup 0$.\n",
    "Calculate the Fisher information. Again, assume that all necessary regularity conditions hold, i.e. you can use the form\n",
    "\\begin{align*}\n",
    "I_F(\\lambda) &= -\\mathbb{E}_X \\Big[ \\frac{d^2}{d \\lambda^2} \\ln p(x ; \\lambda) \\Big] \\ .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7739298",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "To calculate the Fisher Information $I_F(\\lambda)$ for a Poisson distributed random variable with the probability mass function (pmf) $p(x ; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}$, where $\\lambda > 0$ is the parameter and $x \\in \\mathbb{N} \\cup \\{0\\}$, we start by taking the natural logarithm of the pmf, then find its second derivative with respect to $\\lambda$, and finally compute its expected value as required.\n",
    "\n",
    "##### Step 1: Compute the Logarithm of the pmf\n",
    "$$\n",
    "\\ln p(x ; \\lambda) = x\\ln(\\lambda) - \\lambda - \\ln(x!).\n",
    "$$\n",
    "\n",
    "##### Step 2: First Derivative with Respect to $\\lambda$\n",
    "$$\n",
    "\\frac{d}{d\\lambda} \\ln p(x ; \\lambda) = \\frac{x}{\\lambda} - 1.\n",
    "$$\n",
    "\n",
    "##### Step 3: Second Derivative with Respect to $\\lambda$\n",
    "$$\n",
    "\\frac{d^2}{d\\lambda^2} \\ln p(x ; \\lambda) = -\\frac{x}{\\lambda^2}.\n",
    "$$\n",
    "\n",
    "##### Step 4: Compute Fisher Information\n",
    "Given the Fisher Information formula,\n",
    "$$\n",
    "I_F(\\lambda) = -\\mathbb{E}_X \\left[ \\frac{d^2}{d\\lambda^2} \\ln p(x ; \\lambda) \\right] = -\\mathbb{E}_X \\left[ -\\frac{x}{\\lambda^2} \\right] = \\mathbb{E}_X \\left[ \\frac{x}{\\lambda^2} \\right].\n",
    "$$\n",
    "\n",
    "To compute this expectation, we recall that for a Poisson distribution, the expected value $\\mathbb{E}_X[x]$ is $\\lambda$. Therefore,\n",
    "$$\n",
    "I_F(\\lambda) = \\frac{\\mathbb{E}_X[x]}{\\lambda^2} = \\frac{\\lambda}{\\lambda^2} = \\frac{1}{\\lambda}.\n",
    "$$\n",
    "\n",
    "##### Comments: \n",
    "The amount of information decrease with $\\lambda$. Increasing the Poisson rate, each additional observation provides less information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126a3c2",
   "metadata": {},
   "source": [
    "## Exercise 4: Fisher Information of the Mean of the Normal Distribution\n",
    "\n",
    "Consider a normally distributed random variable $X$ with pdf\n",
    "\\begin{align*}\n",
    "p(x ; \\mu, \\sigma^2) &= \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, \\\\ \n",
    "\\end{align*}\n",
    "where $\\mu$ and $\\sigma^2$ are the parameters and the support is $x \\in \\mathbb R$.\n",
    "Calculate the Fisher information for $\\mu$. Again, assume that all necessary regularity conditions hold, i.e. you can use the form\n",
    "\\begin{align*}\n",
    "I_F(\\lambda) &= -\\mathbb{E}_X \\Big[ \\frac{d^2}{d \\mu^2} \\ln p(x ; \\mu, \\sigma^2) \\Big] \\ .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd8799b",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "To calculate the Fisher Information $I_F(\\mu)$ for the mean $(\\mu)$ of a normally distributed random variable with the probability density function (pdf) $p(x ; \\mu, \\sigma^2) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$, we start by taking the logarithm of the pmf, then find its second derivative with respect to $\\mu$, and finally compute its expected value as required.:\n",
    "\n",
    "##### Step 1: Compute the Logarithm of the pdf\n",
    "$$\n",
    "\\ln p(x ; \\mu, \\sigma^2) = -\\ln(\\sigma \\sqrt{2 \\pi}) - \\frac{(x-\\mu)^2}{2\\sigma^2}.\n",
    "$$\n",
    "\n",
    "##### Step 2: First Derivative w.r.t. $\\mu$\n",
    "$$\n",
    "\\frac{d}{d\\mu} \\ln p(x ; \\mu, \\sigma^2) = \\frac{(x-\\mu)}{\\sigma^2}.\n",
    "$$\n",
    "\n",
    "##### Step 3: Second Derivative w.r.t. $\\mu$\n",
    "$$\n",
    "\\frac{d^2}{d\\mu^2} \\ln p(x ; \\mu, \\sigma^2) = -\\frac{1}{\\sigma^2}.\n",
    "$$\n",
    "\n",
    "Note that this second derivative is constant with respect to $x$, meaning that it does not depend on the specific value of $x$.\n",
    "\n",
    "##### Step 4: Compute Fisher Information\n",
    "Given the Fisher Information formula and the result of the second derivative being constant, the expected value operation becomes:\n",
    "$$\n",
    "I_F(\\mu) = -\\mathbb{E}_X \\left[ -\\frac{1}{\\sigma^2} \\right] = \\frac{1}{\\sigma^2}.\n",
    "$$\n",
    "\n",
    "##### Comments\n",
    "The Fisher information increases with the decreasing of the variance $\\sigma^2$. This mean that having concentrated data (around the mean) provide more information about the mean $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d60010a",
   "metadata": {},
   "source": [
    "## Exercise 5: Fisher Information of the Variance of the Normal Distribution\n",
    "\n",
    "Consider a normally distributed random variable $X$ with pmf\n",
    "\\begin{align*}\n",
    "p(x ; \\mu, \\sigma^2) &= \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}, \\\\ \n",
    "\\end{align*}\n",
    "where $\\mu$ and $\\sigma^2$ are the parameters and the support is $x \\in \\mathbb R$.\n",
    "Calculate the Fisher information for $\\sigma^2$. Again, assume that all necessary regularity conditions hold, i.e. you can use the form\n",
    "\\begin{align*}\n",
    "I_F(\\lambda) &= -\\mathbb{E}_X \\Big[ \\frac{d^2}{d (\\sigma^2)^2} \\ln p(x ; \\mu, \\sigma^2) \\Big] \\ .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6afd4f",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "##### Step 1: Compute the Logarithm of the pdf\n",
    "\n",
    "$$\n",
    "\\ln p(x ; \\mu, \\sigma^2) = -\\ln(\\sigma \\sqrt{2 \\pi}) - \\frac{(x-\\mu)^2}{2\\sigma^2}.\n",
    "$$\n",
    "\n",
    "##### Step 2: First Derivative w.r.t. $\\sigma^2 (c)$\n",
    "\n",
    "To simplify the calculation, we will take a dummy variable defined as $c=\\sigma^2$ which lead to $\\sqrt{c}=\\sigma$. The derivative will then be: \n",
    "\n",
    "$$\n",
    "\\frac{d}{d c} \\ln p(x ; \\mu, c) = \\frac{d}{d c} \\left [ -\\ln(\\sqrt{2 \\pi c}) - \\frac{(x-\\mu)^2}{2c} \\right ] = \\frac{d}{d c} \\left [ -\\frac{1}{2}\\ln(2 \\pi c) - \\frac{(x-\\mu)^2}{2c} \\right ] = -\\frac{1}{c} + \\frac{(x-\\mu)^2}{2c^2}\n",
    "$$\n",
    "\n",
    "##### Step 3: Second Derivative w.r.t. $\\sigma^2 (c)$\n",
    "\n",
    "$$\n",
    "\\frac{d^2}{d c^2} \\ln p(x ; \\mu, c) = \\frac{1}{2c^2} - \\frac{(x-\\mu)^2}{c^3}\n",
    "$$\n",
    "\n",
    "Substituting the dummy variable we obtain: \n",
    "\n",
    "$$\n",
    "\\frac{d^2}{d (\\sigma^2)^2} \\ln p(x ; \\mu, \\sigma^2) = \\frac{1}{2\\sigma^4} - \\frac{(x-\\mu)^2}{\\sigma^6}\n",
    "$$\n",
    "\n",
    "##### Step 4: Compute Fisher Information\n",
    "Given the Fisher Information formula,\n",
    "$$\n",
    "I_F(\\sigma^2) = -\\mathbb{E}_X \\Big[ \\frac{d^2}{d (\\sigma^2)^2} \\ln p(x ; \\mu, \\sigma^2) \\Big]\n",
    "$$\n",
    "\n",
    "To compute this expectation, we recall that the expected value $\\mathbb{E}_X[(x-\\mu)^2] = \\sigma^2$. Therefore,\n",
    "$$\n",
    "I_F(\\sigma^2) = -\\frac{1}{2\\sigma^4} + \\frac{\\sigma^2}{\\sigma^6} = \\frac{1}{2\\sigma^4}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57b31a",
   "metadata": {},
   "source": [
    "## Exercise 6: Variance of Arithmetic Mean\n",
    "\n",
    "Given a sequence of iid random variables $X_1, \\dots, X_n$, their arithmetic mean is given by\n",
    "\\begin{align*}\n",
    "    \\bar X = \\frac1n \\sum_{i=1}^n X_i. \n",
    "\\end{align*}\n",
    "Calculate the variance of that estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6dbef6",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "From Forum:\n",
    "For Ex 6, assume that $X_1, \\dots, X_n$ have variance $\\sigma^2$.\n",
    "\n",
    "To calculate the variance of the arithmetic mean $\\bar{X}$ of $n$ independent and identically distributed (iid) random variables $X_1, \\dots, X_n$, where each $X_i$ has a variance of $\\sigma^2$, we can use the properties of variance for sums of random variables and the scaling property. \n",
    "\n",
    "##### Step 1: Variance of a Sum\n",
    "For iid random variables, the variance of their sum is the sum of their variances because the covariance between any two distinct random variables is zero. Thus, for $X_1, \\dots, X_n$, we have:\n",
    "$$\n",
    "\\text{Var}\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n \\text{Var}(X_i) = n\\sigma^2.\n",
    "$$\n",
    "\n",
    "##### Step 2: Variance of a Scaled Random Variable\n",
    "The variance of a scaled random variable, where the scaling factor is $\\frac{1}{n}$, is the square of the scaling factor times the variance of the original variable. Applying this to the sum of $X_i$, we get:\n",
    "$$\n",
    "\\text{Var}(\\bar{X}) = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^n X_i\\right) = \\left(\\frac{1}{n}\\right)^2 \\text{Var}\\left(\\sum_{i=1}^n X_i\\right) = \\left(\\frac{1}{n}\\right)^2 n\\sigma^2 = \\frac{\\sigma^2}{n}.\n",
    "$$\n",
    "\n",
    "##### Comments\n",
    "The arithmetic mean of a large number of observations becomes more and more concentrated around the true mean of the distribution, assuming the variance $\\sigma^2$ is finite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6872c7",
   "metadata": {},
   "source": [
    "## Exercise 7: Bias of Variance Estimator\n",
    "\n",
    "Consider an iid sequence of random variables $X_1, \\dots, X_n$ and the estimator\n",
    "\\begin{equation*}\n",
    "    \\hat \\sigma^2 = \\frac1n \\sum_{i=1}^n (X_i - \\bar X)^2\n",
    "\\end{equation*}\n",
    "for the variance $\\sigma^2$.\n",
    "Calculate the bias of this estimator. \n",
    "If the estimator is biased, can you correct it? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8504e5",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "From Forum: For Ex 7, assume that $X_1, \\dots, X_n$ have unknown mean $\\mu$ and variance $\\sigma^2$.\n",
    "\n",
    "To calculate the bias of the variance estimator $\\hat \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar X)^2$, we need to find the expected value of $\\hat \\sigma^2$ and compare it to the true variance $\\sigma^2$. The bias of an estimator is defined as the difference between its expected value and the true parameter value it estimates, thus:\n",
    "$$\n",
    "\\text{Bias}(\\hat \\sigma^2) = \\mathbb{E}[\\hat \\sigma^2] - \\sigma^2.\n",
    "$$\n",
    "\n",
    "Given that $X_1, \\dots, X_n$ are iid random variables with mean $\\mu$ and variance $\\sigma^2$, let's proceed with the calculation.\n",
    "\n",
    "##### Step 1: Expected Value of the Variance Estimator\n",
    "The expected value of the variance estimator $\\hat \\sigma^2$ can be expressed as (from theory):\n",
    "$$\n",
    "\\mathbb{E}[\\hat \\sigma^2] = \\frac{n-1}{n} \\sigma^2.\n",
    "$$\n",
    "\n",
    "##### Step 2: Calculate the Bias\n",
    "Using the expected value of $\\hat \\sigma^2$, we can calculate its bias as:\n",
    "$$\n",
    "\\text{Bias}(\\hat \\sigma^2) = \\frac{n-1}{n} \\sigma^2 - \\sigma^2 = -\\frac{\\sigma^2}{n}.\n",
    "$$\n",
    "\n",
    "##### Bias Correction\n",
    "The estimator $\\hat \\sigma^2$ is biased because its expectation is not equal to $\\sigma^2$ but $\\frac{n-1}{n} \\sigma^2$. To correct this bias, we can multiply $\\hat \\sigma^2$ by $\\frac{n}{n-1}$ to make it an unbiased estimator. The corrected (unbiased) estimator for the variance is then:\n",
    "$$\n",
    "\\hat \\sigma^2_{\\text{unbiased}} = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar X)^2.\n",
    "$$\n",
    "\n",
    "This estimator is unbiased because its expected value is $\\sigma^2$, as shown by:\n",
    "$$\n",
    "\\mathbb{E}[\\hat \\sigma^2_{\\text{unbiased}}] = \\sigma^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c431882",
   "metadata": {},
   "source": [
    "## Exercise 8: Variance of Variance Estimator\n",
    "\n",
    "Write a python routine that estimates the variance of the estimator $\\hat \\sigma^2$ using a sample of $n=10$ standard normally distributed variables over 10000 trials. \n",
    "Then do the same for the bias-corrected version of $\\hat \\sigma^2$. \n",
    "Compare both estimates to the Cramer-Rao lower bound and interepret and explain the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfeb96b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18195663348514945, 0.22463781911746847, 0.2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n_trials = 10000\n",
    "n = 10\n",
    "\n",
    "# True variance for a standard normal distribution, this is 1)\n",
    "sigma_squared = 1\n",
    "\n",
    "def simulate_variance_estimators(n, n_trials, sigma_squared):\n",
    "    var_estimates = np.zeros(n_trials)\n",
    "    var_estimates_unbiased = np.zeros(n_trials)\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        sample = np.random.standard_normal(n)\n",
    "        sample_mean = np.mean(sample)\n",
    "        var_estimates[trial] = np.sum((sample - sample_mean) ** 2) / n\n",
    "        var_estimates_unbiased[trial] = np.sum((sample - sample_mean) ** 2) / (n - 1)\n",
    "    \n",
    "    # Calculate the variance of the variance estimators\n",
    "    var_of_var_estimator = np.var(var_estimates)\n",
    "    var_of_var_estimator_unbiased = np.var(var_estimates_unbiased)\n",
    "    \n",
    "    # Calculate the Cramer-Rao Lower Bound for the variance estimator\n",
    "    crlb = (2 * sigma_squared**2) / n\n",
    "    \n",
    "    return var_of_var_estimator, var_of_var_estimator_unbiased, crlb\n",
    "\n",
    "var_of_var_estimator, var_of_var_estimator_unbiased, crlb = simulate_variance_estimators(n, n_trials, sigma_squared)\n",
    "\n",
    "var_of_var_estimator, var_of_var_estimator_unbiased, crlb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a4290",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "- Variance of the biased estimator is lower can the CRLB. This is due to the fact that is contains a bias and the CRLB refers to unbiased estimator.\n",
    "- Variance of the unbiased estimator is bigger than CRLB. This is due to the fact that the CRLB is a theoretical limit for an unbiased estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f86616",
   "metadata": {},
   "source": [
    "## Exercise 9: Maximum Likelihood Estimator for the Variance of a Gaussian\n",
    "\n",
    "Derive the maximum likelihood estimator for the variance of $n$ data points $x_1, \\dots, x_n$ drawn from a normal distribution. \n",
    "Is this estimator biased?\n",
    "*Hint: use the log-likelihood for your derivation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19a949",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "To derive the Maximum Likelihood Estimator (MLE) for the variance $\\sigma^2$ of $n$ data points $x_1, \\dots, x_n$ drawn from a Normal distribution, we'll start with the probability density function (pdf) of a Normal distribution, move on to the log-likelihood function, differentiate it with respect to $\\sigma^2$, set the derivative equal to zero to find the critical points, and solve for $\\sigma^2$.\n",
    "\n",
    "##### Step 1: Normal Distribution PDF\n",
    "The pdf of a Normal distribution with mean $\\mu$ and variance $\\sigma^2$ is given by:\n",
    "$$\n",
    "p(x ; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}.\n",
    "$$\n",
    "\n",
    "##### Step 2: Log-Likelihood Function\n",
    "The log-likelihood function for $n$ data points under this distribution is:\n",
    "$$\n",
    "\\ln \\mathcal{L}(\\sigma^2) = \\sum_{i=1}^{n} \\ln \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}} \\right) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i-\\mu)^2.\n",
    "$$\n",
    "\n",
    "##### Step 3: Differentiate Log-Likelihood w.r.t. $\\sigma^2$\n",
    "Differentiating $\\ln \\mathcal{L}(\\sigma^2)$ with respect to $\\sigma^2$ and setting it to zero to find the maximum:\n",
    "$$\n",
    "\\frac{d}{d\\sigma^2} \\ln \\mathcal{L}(\\sigma^2) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (x_i-\\mu)^2 = 0.\n",
    "$$\n",
    "\n",
    "##### Step 4: Solve for $\\sigma^2$\n",
    "Rearranging the terms to solve for $\\sigma^2$:\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i-\\mu)^2.\n",
    "$$\n",
    "\n",
    "##### Is the Estimator Biased?\n",
    "The derived MLE for $\\sigma^2$ assumes knowledge of the true mean $\\mu$. However, in practice, $\\mu$ is often unknown and estimated from the data as well, leading to the usual estimator for variance:\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2,\n",
    "$$\n",
    "where $\\bar{x}$ is the sample mean. This estimator is actually biased for $\\sigma^2$ because it underestimates the variance (by a factor of $\\frac{n-1}{n}$). The unbiased estimator, commonly used in practice, corrects this by dividing by $n-1$ instead of $n$:\n",
    "$$\n",
    "\\hat{\\sigma}^2_{\\text{unbiased}} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2.\n",
    "$$\n",
    "\n",
    "Thus, while the MLE derived here is a direct result from maximizing the likelihood, it is biased when the mean $\\mu$ is not known and has to be estimated from the data. The bias arises because the estimation of the mean consumes one degree of freedom, which is not accounted for in the MLE when dividing by $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd947e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Statistical Learning Theory\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Chernoff Bound for Gaussians with Equal Covariance\n",
    "\n",
    "Read up on section 5.1 \"error bounds for a Gaussian classification task\" in the old lecture notes. \n",
    "Derive the Chernoff bound under the assumption that both classes have equal covariance, i.e., $\\Sigma_1 = \\Sigma_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "- Gaussian distribution assumption for the classes with means $\\mu_1$ and $\\mu_2$ and a common covariance matrix $\\Sigma$.\n",
    "\n",
    "From the section, $v(\\beta)$ when the covariances are equal:\n",
    "$$\n",
    "v(\\beta) = \\beta(1 - \\beta) \\cdot (\\mu_2 - \\mu_1)^T \\Sigma^{-1} (\\mu_2 - \\mu_1) + \\frac{1}{2} \\ln \\frac{|\\Sigma|}{|\\beta \\Sigma + (1 - \\beta) \\Sigma|}\n",
    "$$\n",
    "From assumption $\\Sigma_1 = \\Sigma_2 = \\Sigma$, simplifies to:\n",
    "$$\n",
    "|\\beta \\Sigma + (1 - \\beta) \\Sigma| = |\\Sigma|\n",
    "$$\n",
    "Which leads to:\n",
    "$$\n",
    "\\ln \\frac{|\\Sigma|}{|\\Sigma|} = 0\n",
    "$$\n",
    "Then, $v(\\beta)$ reduces to:\n",
    "$$\n",
    "v(\\beta) = \\beta(1 - \\beta) \\cdot (\\mu_2 - \\mu_1)^T \\Sigma^{-1} (\\mu_2 - \\mu_1)\n",
    "$$\n",
    "\n",
    "##### Step 1: Optimize $v(\\beta)$ w.r.t. $\\beta$\n",
    "Since the function $\\beta (1-\\beta)$ is quadratic, the max is at $0.5$\n",
    "$$\n",
    "v(0.5) = 0.5 \\cdot 0.5 \\cdot (\\mu_2 - \\mu_1)^T \\Sigma^{-1} (\\mu_2 - \\mu_1)\n",
    "$$\n",
    "$$\n",
    "= 0.25 \\cdot (\\mu_2 - \\mu_1)^T \\Sigma^{-1} (\\mu_2 - \\mu_1)\n",
    "$$\n",
    "\n",
    "##### Step 3: Chernoff Bound\n",
    "The Chernoff bound is then:\n",
    "$$\n",
    "\\text{Chernoff bound} = \\exp(-v(0.5))\n",
    "$$\n",
    "$$\n",
    "= \\exp\\left(-0.25 \\cdot (\\mu_2 - \\mu_1)^T \\Sigma^{-1} (\\mu_2 - \\mu_1)\\right)\n",
    "$$\n",
    "which is the error probability bound with equal covariances (Gaussian assumption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Proof of Markov's Inequality\n",
    "\n",
    "Markov's inequality states that for any non-negative random variable $X$ with finite expectation, \n",
    "\\begin{align*}\n",
    "    \\mathbb P(X \\geq a) \\leq \\frac{\\mathbb E[X]}{a}\n",
    "\\end{align*}\n",
    "holds for all $a > 0$. Prove this result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "##### Step 1: Expectation of the Indicator Function\n",
    "We consider the indicator function $1_{X \\geq a}$, which is 1 if $X \\geq a$ and 0 otherwise. \n",
    "The expectation of the indicator function $1_{X \\geq a}$ is equal to the probability that $X \\geq a$:\n",
    "$$\n",
    "\\mathbb{E}[1_{X \\geq a}] = \\mathbb{P}(X \\geq a).\n",
    "$$\n",
    "\n",
    "##### Step 2: Applying the Definition of Expectation\n",
    "We note that for all $x$ in the support of $X$:\n",
    "$$\n",
    "X \\geq a \\cdot 1_{X \\geq a}.\n",
    "$$\n",
    "If $X < a$, then $1_{X \\geq a} = 0$, and the right side is 0, which is less than or equal to $X$. If $X \\geq a$, then $1_{X \\geq a} = 1$, and the right side equals $a$, which is less than or equal to $X$.\n",
    "\n",
    "Taking the expectation on both sides of the inequality gives:\n",
    "$$\n",
    "\\mathbb{E}[X] \\geq \\mathbb{E}[a \\cdot 1_{X \\geq a}] = a \\cdot \\mathbb{E}[1_{X \\geq a}] = a \\cdot \\mathbb{P}(X \\geq a).\n",
    "$$\n",
    "\n",
    "##### Step 3: Derivation of Markov's Inequality\n",
    "Since $\\mathbb{E}[X] \\geq a \\cdot \\mathbb{P}(X \\geq a)$, dividing both sides by $a$ (which is positive) results in:\n",
    "$$\n",
    "\\mathbb{P}(X \\geq a) \\leq \\frac{\\mathbb{E}[X]}{a}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Proof of Heoffding's Inequality\n",
    "\n",
    "Hoeffding's lemma states that\n",
    "\\begin{align*}\n",
    "    \\mathbb E[e^{sX}] \\leq e^{s^2 (b-a)^2 / 8} \\quad \\forall s \\in \\mathbb R,\n",
    "\\end{align*}\n",
    "where $X$ is a random variable bounded by $a \\leq X \\leq b$. \n",
    "Use this to prove Hoeffding's inequality, which states the following. \n",
    "\n",
    "Let $X_1, \\dots, X_n$ be $n$ independent random variables bounded by $a \\leq X_i \\leq b$ for all $i \\in \\{1, \\dots, n\\}$ and let $Z = \\sum_{i=1}^n X_i$. \n",
    "Then\n",
    "\\begin{align*}\n",
    "    \\mathbb P(Z - \\mathbb E[Z] \\geq t) \\leq \\exp\\left(-\\frac{2t^2}{n(b-a)^2}\\right)\n",
    "\\end{align*}\n",
    "holds for all $t \\in \\mathbb R$. \n",
    "\n",
    "*Hint: Use Markov's inequality and Hoeffdings lemma to prove that $\\mathbb P(Z - \\mathbb E[Z] \\geq t) \\leq \\exp((ns^2(b-a)^2)/8 - st)$. Then choose $s$ such that the right-hand side becomes minimal.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "##### Step 1: Hoeffding’s Lemma\n",
    "Given $X_1, \\dots, X_n$ are independent random variables each bounded by $a \\leq X_i \\leq b$, and $Z = \\sum_{i=1}^n X_i$. First we use Hoeffding's lemma:\n",
    "$$\n",
    "\\mathbb{E}[e^{sX_i}] \\leq e^{s^2 (b-a)^2 / 8}.\n",
    "$$\n",
    "\n",
    "##### Step 2: Use Independence to Evaluate $\\mathbb{E}[e^{sZ}]$\n",
    "Since $X_1, \\dots, X_n$ are independent, the expectation of the exponential of their sum $Z$ is the product of their individual expectations:\n",
    "$$\n",
    "\\mathbb{E}[e^{sZ}] = \\mathbb{E}[e^{s(X_1 + \\dots + X_n)}] = \\mathbb{E}[e^{sX_1}] \\cdots \\mathbb{E}[e^{sX_n}] \\leq \\left(e^{s^2 (b-a)^2 / 8}\\right)^n = e^{ns^2 (b-a)^2 / 8}.\n",
    "$$\n",
    "\n",
    "##### Step 3: Apply Markov’s Inequality\n",
    "For any $t > 0$, applying Markov's inequality to the non-negative random variable $e^{s(Z - \\mathbb{E}[Z])}$:\n",
    "$$\n",
    "\\mathbb{P}(Z - \\mathbb{E}[Z] \\geq t) = \\mathbb{P}(e^{s(Z - \\mathbb{E}[Z])} \\geq e^{st}) \\leq \\frac{\\mathbb{E}[e^{s(Z - \\mathbb{E}[Z])}]}{e^{st}}.\n",
    "$$\n",
    "Note $\\mathbb{E}[e^{s(Z - \\mathbb{E}[Z])}] = \\mathbb{E}[e^{sZ}]e^{-s\\mathbb{E}[Z]}$, which simplifies to $\\mathbb{E}[e^{sZ}]$ because the expectation $\\mathbb{E}[Z]$ delete in the exponent due to properties of expectation.\n",
    "\n",
    "##### Step 4: Substitute and Minimize\n",
    "Substituting the bound for $\\mathbb{E}[e^{sZ}]$ from step 2:\n",
    "$$\n",
    "\\mathbb{P}(Z - \\mathbb{E}[Z] \\geq t) \\leq \\frac{e^{ns^2 (b-a)^2 / 8}}{e^{st}} = \\exp\\left(\\frac{ns^2 (b-a)^2}{8} - st\\right).\n",
    "$$\n",
    "\n",
    "##### Step 5: Optimize $s$ \n",
    "To minimize the upper bound, differentiate $\\frac{ns^2 (b-a)^2}{8} - st$ w.r.t. $s$ and set the derivative to 0 to find the optimal:\n",
    "$$\n",
    "\\frac{d}{ds}\\left(\\frac{ns^2 (b-a)^2}{8} - st\\right) = \\frac{ns(b-a)^2}{4} - t = 0 \\Rightarrow s = \\frac{4t}{n(b-a)^2}.\n",
    "$$\n",
    "\n",
    "Substitute $s$ back into the expression:\n",
    "$$\n",
    "\\exp\\left(\\frac{n\\left(\\frac{4t}{n(b-a)^2}\\right)^2 (b-a)^2}{8} - \\frac{4t}{n(b-a)^2} t\\right) = \\exp\\left(\\frac{2t^2}{n(b-a)^2}\\right).\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\mathbb{P}(Z - \\mathbb{E}[Z] \\geq t) \\leq \\exp\\left(-\\frac{2t^2}{n(b-a)^2}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Generalization Bound for a Finite Model Class\n",
    "\n",
    "Let $R(g)$ denote the risk and $\\hat R(g)$ the empirical risk over a training set of $n$ samples for a model $g \\in \\mathcal G$, where the model class $\\mathcal G$ consists of $m < \\infty$ models. \n",
    "Both $R$ and $\\hat R$ are based on the zero-one loss, i.e., $R(g) = \\mathbb E[\\ell(Y, g(X))]$ and $\\hat R = \\frac1n \\sum_{i=1}^n \\ell(y_i, g(x_i))$, where $\\ell: \\mathcal Y \\times \\mathcal Y \\to [0,1]$ is the zero-one loss function. \n",
    "Further, let $\\hat g = \\arg \\min_{g \\in \\mathcal G} \\hat R(g)$ be the model that minimizes the empirical risk and $g^\\ast = \\arg \\min_{g \\in \\mathcal G} R(g)$ the model that minimizes the risk. \n",
    "Use Hoeffding's inequality to bound the estimation error $R(\\hat g) - R(g^\\ast)$ (i.e., the excess of the risk due to empirical risk minimization) by\n",
    "\\begin{align*}\n",
    "    \\mathbb{P}(R(\\hat g) - R(g^\\ast) \\geq t) \\leq 2 m \\exp(-\\frac{nt^2}{2}).\n",
    "\\end{align*}\n",
    "Interpret this result!\n",
    "\n",
    "*Hint: Prove and use the fact that $R(\\hat g) - R(g^\\ast) \\leq 2 \\max_{g \\in \\mathcal G} |\\hat R(g) - R(g)|$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "The zero-one loss function $\\ell(y, g(x))$ takes the value 1 if $g(x) \\neq y$ and 0 if $g(x) = y$. \n",
    "Then, $R(g)$ and $\\hat{R}(g)$ measure the probability of misclassification.\n",
    "\n",
    "##### Step 1: Symmetric Excess Risk\n",
    "First, note the hint suggests that\n",
    "$$\n",
    "R(\\hat g) - R(g^\\ast) \\leq 2 \\max_{g \\in \\mathcal G} |\\hat R(g) - R(g)|.\n",
    "$$\n",
    "This inequality can be understood as follows: for any model $g$ (splitting the absolute value),\n",
    "$$\n",
    "R(g) - \\hat R(g) \\leq |R(g) - \\hat R(g)|,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\hat R(g) - R(g) \\leq |R(g) - \\hat R(g)|.\n",
    "$$\n",
    "Since $\\hat{g}$ is the model with the minimum empirical risk and $g^\\ast$ is the model with the minimum true risk,combine under $R$:\n",
    "$$\n",
    "R(\\hat g) \\leq \\hat R(\\hat g) + |R(\\hat g) - \\hat R(\\hat g)|,\n",
    "$$\n",
    "\n",
    "$$\n",
    "R(g^\\ast) \\geq \\hat R(g^\\ast) - |R(g^\\ast) - \\hat R(g^\\ast)|.\n",
    "$$\n",
    "So,\n",
    "$$\n",
    "R(\\hat g) - R(g^\\ast) \\leq (\\hat R(\\hat g) - \\hat R(g^\\ast)) + |R(\\hat g) - \\hat R(\\hat g)| + |R(g^\\ast) - \\hat R(g^\\ast)| \\leq 2 \\max_{g \\in \\mathcal G} |\\hat R(g) - R(g)|,\n",
    "$$\n",
    "as the maximum deviation $\\geq$ both terms.\n",
    "\n",
    "##### Step 2: Applying Hoeffding’s Inequality\n",
    "Each model $g$ in the class $\\mathcal G$ and the difference $\\hat R(g) - R(g)$ can be bound using Hoeffding's inequality for the sum of $n$ bounded i.i.d. random variables (zero-one loss values), which gives:\n",
    "$$\n",
    "\\mathbb{P}(\\hat R(g) - R(g) \\geq t) \\leq \\exp(-2nt^2).\n",
    "$$\n",
    "Similarly,\n",
    "$$\n",
    "\\mathbb{P}(R(g) - \\hat R(g) \\geq t) \\leq \\exp(-2nt^2).\n",
    "$$\n",
    "Using the union bound over the model class for both deviations,\n",
    "$$\n",
    "\\mathbb{P}(|\\hat R(g) - R(g)| \\geq t) \\leq 2 \\exp(-2nt^2).\n",
    "$$\n",
    "Applying the union bound over all \\(m\\) models in $\\mathcal G$,\n",
    "$$\n",
    "\\mathbb{P}(\\max_{g \\in \\mathcal G} |\\hat R(g) - R(g)| \\geq t) \\leq 2m \\exp(-2nt^2).\n",
    "$$\n",
    "\n",
    "From the derived symmetrical risk expression,\n",
    "$$\n",
    "\\mathbb{P}(R(\\hat g) - R(g^\\ast) \\geq t) \\leq \\mathbb{P}(2 \\max_{g \\in \\mathcal G} |\\hat R(g) - R(g)| \\geq t) = \\mathbb{P}(\\max_{g \\in \\mathcal G} |\\hat R(g) - R(g)| \\geq \\frac{t}{2}) \\leq 2m \\exp\\left(-\\frac{nt^2}{2}\\right).\n",
    "$$\n",
    "\n",
    "##### Step 4: Interpretation\n",
    "This formula defines the difference between risk of the empirically best model $\\hat{g}$ over the truly best model $g^\\ast$. The probability decreases exponentially as the sample size $n$ increases, or as the excess $t$ increases. \n",
    "It also states a trade-off between the model (class size $m$) and the confidence in the generalization capability. \n",
    "Strictly speaking: larger model classes require more data to achieve the same level of confidence in their generalizazion capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Generalization Bound for Floating Point Models\n",
    "\n",
    "The bound from the previous exercise applies only to finite model classes. \n",
    "When we optimize a model over real parameters on a computer, the model class (although theoretically infinite) is practically restricted by the representation of parameters as floating-point numbers. \n",
    "Modify the bound from the previous exercise to a model class with $k$ parameters represented by $b$ bits floating-point numbers. \n",
    "For a risk excess of $t = 0.01$ and a risk excess probability of not more than $\\delta = 0.01$ and using 32-bit parameters, what is the required training set size $n$ as a function of the number of parameters $k$? \n",
    "Discuss your results! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "##### Step 1: Counting the Effective Number of Models\n",
    "When parameters are represented as floating-point numbers with $b$ bits, each parameter can take on $2^b$ different values. \n",
    "Therefore, for $k$ parameters, the total number of possible different configurations $m$ is $(2^b)^k = 2^{bk}$.\n",
    "\n",
    "##### Step 2: Adjusting the Bound\n",
    "From the previous exercise, we derived that:\n",
    "$$\n",
    "\\mathbb{P}(R(\\hat g) - R(g^\\ast) \\geq t) \\leq 2m \\exp(-\\frac{nt^2}{2}).\n",
    "$$\n",
    "Substituting $m = 2^{bk}$, the bound becomes:\n",
    "$$\n",
    "\\mathbb{P}(R(\\hat g) - R(g^\\ast) \\geq t) \\leq 2 \\cdot 2^{bk} \\exp(-\\frac{nt^2}{2}).\n",
    "$$\n",
    "\n",
    "##### Step 3: Solving for $n$\n",
    "We need to find the smallest $n$ such that this probability is no more than $\\delta = 0.01$. Setting the bound equal to $\\delta$ and solving for $n$:\n",
    "$$\n",
    "2 \\cdot 2^{bk} \\exp(-\\frac{nt^2}{2}) = 0.01.\n",
    "$$\n",
    "Taking natural logs:\n",
    "$$\n",
    "\\ln(2) + bk \\ln(2) - \\frac{nt^2}{2} = \\ln(0.01),\n",
    "$$\n",
    "$$\n",
    "\\ln(2)(1 + bk) = \\ln(0.01) + \\frac{nt^2}{2},\n",
    "$$\n",
    "$$\n",
    "\\frac{nt^2}{2} = \\ln(0.01) - \\ln(2)(1 + bk),\n",
    "$$\n",
    "$$\n",
    "n = \\frac{2[\\ln(0.01) - \\ln(2)(1 + bk)]}{t^2}.\n",
    "$$\n",
    "\n",
    "##### Step 4: Substituting Specific Values\n",
    "For $t = 0.01$, $b = 32$ (for 32-bit parameters), and any given $k$:\n",
    "$$\n",
    "n = \\frac{2[\\ln(0.01) - \\ln(2)(1 + 32k)]}{0.01^2}.\n",
    "$$\n",
    "\n",
    "To achieve a risk excess of $t = 0.01$ with a probability of not more than $\\delta = 0.01$, are as follows:\n",
    "\n",
    "- For $k = 1$: approximately 549,581 samples,\n",
    "- For $k = 2$: approximately 993,195 samples,\n",
    "- For $k = 5$: approximately 2,324,037 samples,\n",
    "- For $k = 10$: approximately 4,542,108 samples.\n",
    "\n",
    "##### Step 5: Discussion of Results:\n",
    "The results show the exponential growth in the required sample increasing the parameters. This means that having a large dataset during the training of complex models is essential. It may be seen as a trade-off between complexity and feasibility of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: VC Dimension of Linear Classifiers (Lower Bound)\n",
    "\n",
    "The VC dimension is defined as \n",
    "\\begin{align*}\n",
    "    \\operatorname{VC}(\\mathcal G) = \\max_{n \\in \\mathbb N} \\{n \\mid \\sup_{x_1, \\dots, x_n} N_{\\mathcal G}(x_1, \\dots, x_n) = 2^n\\},\n",
    "\\end{align*}\n",
    "where $N_{\\mathcal G}$ is the shattering coefficient, i.e., the number of different binary labellings the model class $\\mathcal G$ can produce for the set $\\{x_1, \\dots, x_n\\}$. \n",
    "Intuitively, the VC dimension is the maximum number of points for which there exists a configuration so that $\\mathcal G$ can produce all possible labellings. \n",
    "\n",
    "Consider the model class \n",
    "\\begin{align*}\n",
    "    \\mathcal G = \\{g : g(x) = \\operatorname{sign}(w^\\top x), w, x \\in \\mathbb R^d\\}.\n",
    "\\end{align*}\n",
    "Prove that $\\operatorname{VC}(\\mathcal G) \\geq d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "Considering that a classifier in $\\mathbb{R}^d$ can be described by hyperplane with weights $w$ and bias $b$. However, the bias can be easily incorporated in the weights set by adding an additional dimension $d+1$. For sake of simplicity, we will ignore the bias.\n",
    "\n",
    "Let's choose $d$ points in $\\mathbb{R}^d$ that are linearly independent. For example, points that correspond to the standard basis in $\\mathbb{R}^d$: $x_1 = (1, 0, \\ldots, 0)$, $x_2 = (0, 1, \\ldots, 0)$, until $x_d = (0, 0, \\ldots, 1)$.\n",
    "\n",
    "We need to demostrate that, with these points $d$, exist a vector $w$ sp that $w^\\top x_i$ corresponds to the given label for each $i$. Formally: \n",
    "\n",
    "$$\n",
    "\\text{sign}(w^\\top x_i) = y_i \\forall i\n",
    "$$\n",
    "\n",
    "To achieve this labeling, $w$ can be defined as a linear combination of the $x_i$ with coefficients that depend on the desired labels $y_i$. For instance:\n",
    "$$\n",
    "w = \\sum_{i=1}^d y_i x_i.\n",
    "$$\n",
    "This choice of $w$ ensures that $w^\\top x_i = y_i$ because each $x_i$ is orthogonal to all other (basis definition) $x_j$ for $j \\neq i$ in the chosen set. Thus, $w^\\top x_i = y_i (x_i^\\top x_i) + \\sum_{j \\neq i} y_j (x_j^\\top x_i) = y_i$, assuming $x_i^\\top x_i = 1$ (the points are normalized if needed).\n",
    "\n",
    "Since we can label any set of $d$ points, where the points are linearly independent, we conclude that $\\mathcal{G}$ can shatter any set of $d$ points. Therefore, the VC dimension of $\\mathcal{G}$ is at least $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: VC Dimension of Linear Classifiers (Upper Bound)\n",
    "\n",
    "Prove that $\\operatorname{VC}(\\mathcal G) < d+1$ and conclude that $\\operatorname{VC}(\\mathcal G) = d$. \n",
    "You can do a proof by contradiction. \n",
    "To this end, assume that $\\operatorname{VC}(\\mathcal G) = d+1$ and construct a matrix $W \\in \\mathbb R^{d \\times 2^{d+1}}$ whose columns contain the parameter vectors producing all $2^{d+1}$ possible labellings. \n",
    "Then $X^\\top W \\in \\mathbb{R}^{(d+1) \\times 2^{d+1}}$ must contain all possible labellings in its columns, where $X = (x_1, \\dots, x_{d+1})$. \n",
    "Explain why such a matrix cannot exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "Proof with a contradiction\n",
    "\n",
    "##### Step 1: Setting Up the Contradiction\n",
    "Assuming that the dimension $g(x) = \\text{sign}(w^\\top x)$ in $\\mathbb{R}^d$, is $d+1$. This would mean that there exists a set of $d+1$ points in $\\mathbb{R}^d$ that can be shattered by $\\mathcal{G}$. \n",
    "\n",
    "##### Step 2: $X$ Matrix\n",
    "The matrix $X$ represents $d+1$ points in a $d$-dimensional space ($\\mathbb{R}^d$). \n",
    "Each column of $X$ corresponds to one of these $d+1$ points.\n",
    "Since each point is in $\\mathbb{R}^d$, it is represented by a vector of $d$ dimensions.\n",
    "Therefore, $X$ has $d$ rows (one for each dimension of the space) and $d+1$ columns (one for each point). \n",
    "\n",
    "##### Step 3: $W$ Matrix\n",
    "The matrix $W$ is intended to represent the set of weight vectors, each corresponding to one specific labeling of the $d+1$ points.\n",
    "The number of possible distinct labelings of $d+1$ points, where each point can be labeled as either $-1$ or $1$ is $2^{d+1}$. This is because each point can independently have two labels, and there are $d+1$ points.\n",
    "Each weight vector in $W$ is in $\\mathbb{R}^d$, meaning it has $d$ dimensions. There needs to be one weight vector for each possible labeling configuration, so $W$ has $2^{d+1}$ columns.\n",
    "\n",
    "##### Step 4: Identifying the Contradiction\n",
    "The contradiction arises from the fact that a $(d+1) \\times 2^{d+1}$ matrix $X^\\top W$ is required to have $2^{d+1}$ linearly independent columns to represent all possible combinations of labels for $d+1$ points. However, the maximum rank of a matrix with $d+1$ rows is $d+1$. The rank of a matrix indicates the maximum number of linearly independent columns it can have. Since $2^{d+1}$ (the number of required distinct labelings) exceeds $d+1$ (the maximum number of linearly independent vectors in $\\mathbb{R}^{d+1}$), it is impossible for $X^\\top W$ to fulfill the requirement for shattering $d+1$ points in $\\mathbb{R}^d$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

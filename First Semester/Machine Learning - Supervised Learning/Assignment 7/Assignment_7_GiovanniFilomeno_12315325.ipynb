{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 7: Neural Networks and a Glimpse at Pytorch </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Copyrighting and Fair Use</h2>\n",
    "\n",
    "This material, no matter whether in printed or electronic form,\n",
    "may be used for personal and non-commercial educational use\n",
    "only. Any reproduction of this material, no matter whether as a\n",
    "whole or in parts, no matter whether in printed or in electronic\n",
    "form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Automatic Testing Guidelines</h2>\n",
    "\n",
    "Automatic unittesting requires you, as a student, to submit a notebook which contains strictly defined objects.\n",
    "Strictness of definition consists of unified shapes, dtypes, variable names and more.\n",
    "\n",
    "Within the notebook, we provide detailed instruction which you should follow in order to maximise your final grade.\n",
    "\n",
    "**Name your notebook properly**, follow the pattern in template name:\n",
    "\n",
    "**Assignment_N_NameSurname_matrnumber**\n",
    "<ol>\n",
    "    <li>N - number of assignment</li>\n",
    "    <li>NameSurname - your full name where every part of the name starts with a capital letter, no spaces</li>\n",
    "    <li>matrnumber - your 8-digit student number on ID card (without k)</li>\n",
    "</ol>\n",
    "\n",
    "**Example:**<br>\n",
    " ✅ Assignment_0_RenéDescartes_12345678<br>\n",
    " ✅ Assignment_0_SørenAabyeKierkegaard_12345678<br>\n",
    " ❌ Assignment0_Peter_Pan_k12345678\n",
    "\n",
    "Don't add any cells but use the ones provided by us. You may notice that most cells are tagged such that the unittest routine can recognise them.\n",
    "\n",
    "We highly recommend you to develop your code within the provided cells. You can implement helper functions where needed unless you put them in the same cell they are actually called. Always make sure that implemented functions have the correct output and given variables contain the correct data type. Don't import any other packages than listed in the cell with the \"imports\" tag.\n",
    "\n",
    "**Note:** Never use variables you defined in another cell in your functions directly; always pass them to the function as a parameter. In the unitest they won't be available either.\n",
    "\n",
    "*Good luck!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1: The XOR Problem</h2>\n",
    "\n",
    "**Task 1.1.**\n",
    "\n",
    "In this task we try to formalize the fact that a single layer neural network (NN) cannot solve the XOR problem, but a two layer network can. \n",
    "\n",
    "Let us assume that we only have four possible inputs $\\mathbf{x}_1 = (0,0)$,\n",
    "$\\mathbf{x}_2 = (1,0)$, $\\mathbf{x}_3 = (0,1)$, and $\\mathbf{x}_4 = (1,1)$ with the following labels $y_1=0$, $y_2=1$, $y_3=1$, and $y_4=0$, respectively. Note that this exactly describes the XOR function: it outputs 1 (=true) if and only if exactly one of the input components equals 1 (=true).\n",
    "\n",
    "* As a first task show that if we use a linear network\n",
    "$\n",
    "    g_1(\\mathbf{x};\\mathbf{w})= \\mathbf{x} \\cdot \\mathbf{w} = x^{(1)} w_1 + x^{(2)}  w_2,\n",
    "$\n",
    " it is impossible to find parameters $w_1$ and $w_2$\n",
    "  that solve this problem exactly.\n",
    "  \n",
    "**Please provide reasoning and explanations in full sentences. Grading of the task will heavily depend on it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "calc1"
    ]
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Calculation (10 points):</h3>\n",
    "\n",
    "Your calculation here.\n",
    "\n",
    "**Solving with Linear Equations:** Suppose we try to find parameters $w_1$ and $w_2$ for the given linear model. We would end up with the following system of equations based on the XOR truth table:\n",
    "\n",
    "   - For $\\mathbf{x}_1 = (0,0)$, $y_1=0$: $0 \\cdot w_1 + 0 \\cdot w_2 = 0$\n",
    "   - For $\\mathbf{x}_2 = (1,0)$, $y_2=1$: $1 \\cdot w_1 + 0 \\cdot w_2 = 1$\n",
    "   - For $\\mathbf{x}_3 = (0,1)$, $y_3=1$: $0 \\cdot w_1 + 1 \\cdot w_2 = 1$\n",
    "   - For $\\mathbf{x}_4 = (1,1)$, $y_4=0$: $1 \\cdot w_1 + 1 \\cdot w_2 = 0$\n",
    "\n",
    "   It's clear that there is no solution to this system of equations that satisfies all conditions simultaneously. The last equation, in particular, implies that $w_1$ and $w_2$ must sum to zero, which contradicts the solutions required by the second and third equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2.**\n",
    "\n",
    "Even by adding bias units or by applying a sigmoid, the problem cannot be solved. \n",
    "\n",
    "However, as soon as we use a two-layer network with a simple non-linear activation function (ReLU):\n",
    "\n",
    "$$   \n",
    "g_2(\\mathbf{x};\\mathbf{W}^{[1]}, \\mathbf{W}^{[2]}, \\mathbf{b})=  \\mathbf{W}^{[2]T} \\max(0,\\mathbf{W}^{[1]} \\mathbf{x} + \\mathbf{b}),\n",
    "$$\n",
    "\n",
    "we can find parameters, that solve the problem. \n",
    "   * Precisely, show that $\\mathbf{W}^{[1]}=\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$,\n",
    "    $\\mathbf{b}=\\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$, and $\\mathbf{W}^{[2]}=\\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$ solve the problem\n",
    "    in an exact way. The transformation $\\max(0,\\mathbf{W}^{[1]} \\mathbf{x} + \\mathbf{b})$ has \n",
    "    mapped the points $ \\mathbf{x}_1,\\ldots,  \\mathbf{x}_4$ into a space, in which those data points are \n",
    "    linearly separable. \n",
    "    \n",
    "**Please provide reasoning and explanations in full sentences. Grading of the task will heavily depend on it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "calc2"
    ]
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Calculation (15 points):</h3>\n",
    "\n",
    "Your calculation here.\n",
    "\n",
    "Given a two-layer neural network with $\\max(0, z)$. The given network is:\n",
    "\n",
    "$$g_2(\\mathbf{x};\\mathbf{W}^{[1]}, \\mathbf{W}^{[2]}, \\mathbf{b})=  \\mathbf{W}^{[2]T} \\max(0,\\mathbf{W}^{[1]} \\mathbf{x} + \\mathbf{b})$$\n",
    "\n",
    "We need to demonstrate that with specific parameters $\\mathbf{W}^{[1]}=\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$, $\\mathbf{b}=\\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$, and $\\mathbf{W}^{[2]}=\\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$, this network can solve the XOR problem exactly. \n",
    "\n",
    "Let's calculate the output for each input $\\mathbf{x}_i$\n",
    "\n",
    "1. **Input $\\mathbf{x}_1 = (0,0)$, Output $y_1 = 0$:**\n",
    "\n",
    "   $$\\mathbf{W}^{[1]} \\mathbf{x}_1 + \\mathbf{b} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$$\n",
    "   $$\\max(0, \\mathbf{W}^{[1]} \\mathbf{x}_1 + \\mathbf{b}) = \\max(0, \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n",
    "   $$\\mathbf{W}^{[2]T} \\max(0, \\mathbf{W}^{[1]} \\mathbf{x}_1 + \\mathbf{b}) = \\begin{pmatrix} 1 & -2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = 0$$\n",
    "\n",
    "2. **Input $\\mathbf{x}_2 = (1,0)$, Output $y_2 = 1$:**\n",
    "\n",
    "   $$\\mathbf{W}^{[1]} \\mathbf{x}_2 + \\mathbf{b} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n",
    "   $$\\max(0, \\mathbf{W}^{[1]} \\mathbf{x}_2 + \\mathbf{b}) = \\max(0, \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n",
    "   $$\\mathbf{W}^{[2]T} \\max(0, \\mathbf{W}^{[1]} \\mathbf{x}_2 + \\mathbf{b}) = \\begin{pmatrix} 1 & -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$$\n",
    "\n",
    "3. **Input $\\mathbf{x}_3 = (0,1)$, Output $y_3 = 1$:**\n",
    "\n",
    "   $$\\mathbf{W}^{[1]} \\mathbf{x}_3 + \\mathbf{b} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n",
    "   $$\\max(0, \\mathbf{W}^{[1]} \\mathbf{x}_3 + \\mathbf{b}) = \\max(0, \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n",
    "   $$\\mathbf{W}^{[2]T} \\max(0, \\mathbf{W}^{[1]} \\mathbf{x}_3 + \\mathbf{b}) = \\begin{pmatrix} 1 & -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$$\n",
    "\n",
    "4. **Input $\\mathbf{x}_4 = (1,1)$, Output $y_4 = 0$:**\n",
    "\n",
    "   $$\\mathbf{W}^{[1]} \\mathbf{x}_4 + \\mathbf{b} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\n",
    "   $$\\max(0, \\mathbf{W}^{[1]} \\mathbf{x}_4 + \\mathbf{b}) = \\max(0, \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}) = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\n",
    "   $$\\mathbf{W}^{[2]T} \\max(0, \\mathbf{W}^{[1]} \\mathbf{x}_4 + \\mathbf{b}) = \\begin{pmatrix} 1 & -2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 0$$\n",
    "\n",
    "Since in all the cases, the output of the network is equal to the expected XOR output, this demostrates that it can solve the XOR Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 2: Backprop of a Simple NN</h2>\n",
    "\n",
    "Consider the following neural network (we try to adapt to the notation from the lecture):\n",
    "<div>\n",
    "<img src=\"nn_pic.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "The preactivations of the hidden units are denoted as $s_3$, $s_4$ and $s_5$  from left to right, their activations as $a_3$, $a_4$ and $a_5$, respectively. In the hidden layer we use ReLU as activation function, i.e. $f_3(x)=f_4(x)=f_5(x)=\\text{ReLU}(x)$, and in the output layer the activation is the identity function. The preactivation of the output layer is denoted as $s_6$ and the output as $\\hat{y}$. The delta at the output is denoted as $\\delta_6$, and the hidden deltas as $\\delta_3$, $\\delta_4$ and $\\delta_5$ from left to right, respectively. The true label is $y=1$ and as loss function we use the mean-squared loss, i.e $L(y,\\hat{y})=\\frac12 (y-\\hat{y})^2$.\n",
    "\n",
    "Given values:\n",
    "\n",
    "- $x_1=1$\n",
    "- $x_2=1.5$\n",
    "- $w_{31}=-0.1$\n",
    "- $w_{41}=-1$\n",
    "- $w_{51}=0.1$\n",
    "- $w_{32}=0.5$\n",
    "- $w_{42}=0$\n",
    "- $w_{52}=-2$\n",
    "- $w_{63}=1.2$\n",
    "- $w_{64}=-1$\n",
    "- $w_{65}=0.5$\n",
    "\n",
    "* Compute all outputs, preactivations, activations, and delta errors! Also compute $\\frac{\\partial L}{\\partial w_{32}}$. Write down formulas (not only the numerical result) that indicate your computations at least for $s_3$, $\\delta_3$ and $\\frac{\\partial L}{\\partial w_{32}}$.\n",
    "* Additionally finish the python code that calculates the same results.\n",
    "\n",
    "**Please provide reasoning and explanations in full sentences. Grading of the task will heavily depend on it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "calc3"
    ]
   },
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Calculation (10 points):</h3>\n",
    "\n",
    "Your calculation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Python implementation (15 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "python_backprop"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preactivation Hidden Layer: [[ 0.65 -1.   -2.9 ]]\n",
      "Activation Hidden Layer: [[0.65 0.   0.  ]]\n",
      "Preactivation Output Layer: [[0.78]]\n",
      "Output: [[0.78]]\n",
      "Delta Error Output Layer: [[-0.22]]\n",
      "Delta Error Hidden Layer: [[-0.264  0.    -0.   ]]\n",
      "Partial derivative of the loss w.r.t w32: [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Python Implementation\n",
    "import numpy as np\n",
    "\n",
    "# Notes on shapes of matrices:\n",
    "# the pre-activations and activations should be row-vectors\n",
    "# meaning shape = (1, ...). Think of it as having a mini-batch size of 1\n",
    "# Check out the pytorch docs on the linear layer for the forward pass and the shape\n",
    "# of the weight-layers ;)\n",
    "\n",
    "input = np.array([1, 1.5]).reshape(1, -1)\n",
    "\n",
    "weights_layer1 = np.array([[-0.1, 0.5], [-1, 0], [0.1, -2]])  # fill in the correct values\n",
    "weights_layer2 = np.array([[1.2, -1, 0.5]])  # fill in the correct values\n",
    "\n",
    "assert weights_layer1.shape == (3, 2)\n",
    "assert weights_layer2.shape == (1, 3)\n",
    "\n",
    "# Note: Every function can be easily written as a small one-liner\n",
    "# If you have bigger functions, maybe think of a better solution :)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"calculate relu activation of x\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    \"\"\"calculate derivative of relu for input x\"\"\"\n",
    "    return (x > 0).astype(int)\n",
    "\n",
    "def calc_activation(preactivation):\n",
    "    \"\"\"calculate activation given pre-activation\n",
    "    \"\"\"\n",
    "    return relu(preactivation)\n",
    "\n",
    "def calc_preactivation(x, w):\n",
    "    \"\"\"calculate preactivation of a linear layer\n",
    "    \n",
    "    x: layer inputs\n",
    "    w: layer weights\n",
    "    \"\"\"\n",
    "    return np.dot(x, w.T)\n",
    "\n",
    "def calc_delta_6(y_hat, y):\n",
    "    \"\"\"delta error of last layer\"\"\"\n",
    "    return -(y - y_hat)\n",
    "\n",
    "def calc_delta(s, delta_6, w):\n",
    "    \"\"\"delta error of non-last layer\n",
    "    \n",
    "    s: correct preactivation\n",
    "    delta_6: delta error of last layer\n",
    "    w: correct nn weight\n",
    "\n",
    "    to get correct inputs check the formulas and their indices ;)\n",
    "    \"\"\"\n",
    "    return derivative_relu(s) * np.dot(delta_6, w)\n",
    "\n",
    "def calc_derivative_L(delta_error, activation):\n",
    "    \"\"\"calc derivative of loss-fct w.r.t a certain weight.\n",
    "    check formulas to now what delta_error and activation you have to\n",
    "    provide :)\n",
    "    \"\"\"\n",
    "    return np.dot(delta_error.T, activation)\n",
    "\n",
    "# Forward pass\n",
    "preactivation_hidden = calc_preactivation(input, weights_layer1)\n",
    "activation_hidden = calc_activation(preactivation_hidden)\n",
    "preactivation_output = calc_preactivation(activation_hidden, weights_layer2)\n",
    "y_hat = preactivation_output \n",
    "\n",
    "# Backward pass\n",
    "y_true = np.array([[1]])\n",
    "delta_6 = calc_delta_6(y_hat, y_true)\n",
    "delta_hidden = calc_delta(preactivation_hidden, delta_6, weights_layer2)\n",
    "\n",
    "# Partial derivative of the loss w.r.t w32\n",
    "dL_dw32 = calc_derivative_L(delta_hidden[:, 1], input)\n",
    "\n",
    "# Print the results\n",
    "print(\"Preactivation Hidden Layer:\", preactivation_hidden)\n",
    "print(\"Activation Hidden Layer:\", activation_hidden)\n",
    "print(\"Preactivation Output Layer:\", preactivation_output)\n",
    "print(\"Output:\", y_hat)\n",
    "print(\"Delta Error Output Layer:\", delta_6)\n",
    "print(\"Delta Error Hidden Layer:\", delta_hidden)\n",
    "print(\"Partial derivative of the loss w.r.t w32:\", dL_dw32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 2: Pytorch and a Visualization of the Vanishing Gradient Problem</h2>\n",
    "\n",
    "The aim of this task is to provide you with some familiarity with Pytorch, a Python-package which is nowadays heavily used for tasks that involve computations with neural networks. It has the nice feature that it incorporates automatic differentiation, so that you don't have to implement the backward pass for a NN any more. It also allows for transferring more involved experiments to GPUs easily, however, we won't need this nice feature here. We will again work with the Fashion MNIST data set, but this time we provide you with a Pytorch routine that can download it for you and even transforms it appropriately.\n",
    "\n",
    "* Your first task will be to just let the code run and plot some images. To this end you will need to install Pytorch appropriately in your Python library!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(208,90,80)\">Code (10 points)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from types import SimpleNamespace\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hyperparams"
    ]
   },
   "outputs": [],
   "source": [
    "# Here we collect the hyperparameters we are going to use\n",
    "args = SimpleNamespace(batch_size=64, test_batch_size=1000, epochs=1,\n",
    "                       lr=0.01, momentum=0.5, seed=1, log_interval=100)\n",
    "torch.manual_seed(args.seed)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(batch_size=64, test_batch_size=1000, epochs=1, lr=0.01, momentum=0.5, seed=1, log_interval=100)\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#just printout for deeper view\n",
    "print(args)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "dataloaders"
    ]
   },
   "outputs": [],
   "source": [
    "# Data loader (downloads data automatically the first time)\n",
    "# 0.1307 and 0.3081 are the mean and the std computed on the training set\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(os.path.join('.','..','data'), train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(os.path.join('.','..','data'), train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "plotdata"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000])\n",
      "torch.Size([60000, 28, 28])\n",
      "Input dimension is 784.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(train_loader.dataset.targets.shape)\n",
    "print(train_loader.dataset.data.shape)\n",
    "input_dim = train_loader.dataset.data.shape[1]*train_loader.dataset.data.shape[2]\n",
    "print(\"Input dimension is {}.\".format(input_dim))\n",
    "output_dim = 10\n",
    "\n",
    "# Plot example images\n",
    "fig=plt.figure(figsize=(15,3))\n",
    "for image in range(20):\n",
    "    show_img = train_loader.dataset.data[image].numpy().reshape(28, 28)\n",
    "    fig.add_subplot(2,10,image+1)\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "    plt.imshow(show_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we provide you with code that you can use to create your own artificial neural network (ANN) in terms of a class. We will use a 3-hidden-layer NN with sigmoid activation here. As you should know from the lecture, sigmoid is not a very good choice as it induces vanishing gradients.\n",
    "\n",
    "* To overcome this issue, create a second nework class in a similar way that again has the same three linear layers but instead uses the ReLU activation function which is known to prevent the gradients from vanishing. Don't change the output activation function.\n",
    "\n",
    "<h3 style=\"color:rgb(208,90,80)\">Code (10 points)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "ann_sigm"
    ]
   },
   "outputs": [],
   "source": [
    "class ANN_sigmoid(nn.Module):\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super(ANN_sigmoid, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, 512)\n",
    "        self.linear2 = nn.Linear(512, 1024)\n",
    "        self.linear3 = nn.Linear(1024, 512)\n",
    "        self.linear4 = nn.Linear(512, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear4(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "ann_relu"
    ]
   },
   "outputs": [],
   "source": [
    "class ANN_relu(nn.Module):\n",
    "    #Your code for ReLU NN\n",
    "    \n",
    "    ## Solution start\n",
    "    def __init__(self, D_in, D_out):\n",
    "        super(ANN_relu, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in, 512)\n",
    "        self.linear2 = nn.Linear(512, 1024)\n",
    "        self.linear3 = nn.Linear(1024, 512)\n",
    "        self.linear4 = nn.Linear(512, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "    ## Solution end\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we provide you with a routine for training a neural network with Pytorch. For each layer, it outputs a list where one entry indicates the averaged absolute sum of the gradient magnitudes of the activations for a particular mini batch of this particular layer. Note that the command loss.backward() automatically computes and stores the gradients of all the activations. It also prints accuracy and the loss value for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "train"
    ]
   },
   "outputs": [],
   "source": [
    "# This function trains the model for one epoch\n",
    "# Nothing to do here\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    grads1_list = []\n",
    "    grads2_list = []\n",
    "    grads3_list = []\n",
    "    correct=0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = Variable(data.view(-1, input_dim))\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # This part of the code gets the weights in the different layers\n",
    "        grads3 = abs(model.linear3.weight.grad)\n",
    "        grads2 = abs(model.linear2.weight.grad)\n",
    "        grads1 = abs(model.linear1.weight.grad)\n",
    "        grads3_list.append(torch.mean(grads3).item())\n",
    "        grads2_list.append(torch.mean(grads2).item())\n",
    "        grads1_list.append(torch.mean(grads1).item())\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print('\\nTraining set: Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    correct, len(train_loader.dataset),\n",
    "    100. * correct / len(train_loader.dataset))) \n",
    "    return grads1_list, grads2_list, grads3_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a similar routine for the test procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "# This function evaluates the model on the test data\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = Variable(data.view(-1, input_dim))\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you are asked to execute the previously built functions. To this end, perform the following tasks:\n",
    "\n",
    "* We provided the routine for creating the model with the sigmoid function and executing it on the cpu per default. As a first task, you are now asked to create an appropriate optimizer (take a look into imports cell). Use SGD with the parameters of the model and the learning rate and momentum from the hyperparameterlist args created in the beginning.\n",
    "* Now write a code where you **the function** `train` **for number of** `epochs` **in our hyperparameters collection** `args`  with the required arguments for the **sigmoid model** and create a routine that **plots the output list of means of gradient magnitudes** for each layer appropriately. It should also output the test accuracy.\n",
    "* Repeat the previous task but for ReLU model.\n",
    "\n",
    "<h3 style=\"color:rgb(208,90,80)\">Code (20 points)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "optimiser"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"<<< Sigmoid >>>\\n\")\n",
    "model = ANN_sigmoid(input_dim, output_dim).to(device)\n",
    "\n",
    "#please use only predefined variables names\n",
    "# optimizer = None #define SGD optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "epochs_range = range(args.epochs) #range you will iterate over\n",
    "\n",
    "for epoch in epochs_range:\n",
    "    grads1_list, grads2_list, grads3_list = train(args, model, device, train_loader, optimizer, epoch)\n",
    "    \n",
    "    #plot here together 3 different magnitudes\n",
    "    plt.figure(figsize=(16,9))\n",
    "\n",
    "    plt.plot(grads1_list, label='Layer 1 Gradients')\n",
    "    plt.plot(grads2_list, label='Layer 2 Gradients')\n",
    "    plt.plot(grads3_list, label='Layer 3 Gradients')\n",
    "    plt.title(f'Epoch {epoch+1} Gradient Magnitudes - Sigmoid Model')\n",
    "    plt.xlabel('Mini-batch')\n",
    "    plt.ylabel('Mean Gradient Magnitude')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #now test your model\n",
    "\n",
    "    test(args, model, device, test_loader)\n",
    "    \n",
    "######################################################################################    \n",
    "print(\"<<< ReLU >>>\\n\")\n",
    "model = ANN_relu(input_dim, output_dim).to(device) #define your ReLU model\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum) #redefine SGD optimizer\n",
    "\n",
    "for epoch in epochs_range:\n",
    "    #repeat operations of the loop above, but for ReLU model\n",
    "    # pass\n",
    "    grads1_list, grads2_list, grads3_list = train(args, model, device, train_loader, optimizer, epoch)\n",
    "    \n",
    "    #plot here together 3 different magnitudes\n",
    "    plt.figure(figsize=(16,9))\n",
    "\n",
    "    plt.plot(grads1_list, label='Layer 1 Gradients')\n",
    "    plt.plot(grads2_list, label='Layer 2 Gradients')\n",
    "    plt.plot(grads3_list, label='Layer 3 Gradients')\n",
    "    plt.title(f'Epoch {epoch+1} Gradient Magnitudes - ReLU Model')\n",
    "    plt.xlabel('Mini-batch')\n",
    "    plt.ylabel('Mean Gradient Magnitude')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #now test your model\n",
    "\n",
    "    test(args, model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Question (10 points):</h3>\n",
    "\n",
    "What observations for the sigmoid network and the ReLU network can you make? Tick the correct boxes (several may be correct):\n",
    "\n",
    "a_)  The sigmoid network trains poorly in only one epoch. <br>\n",
    "b_)  The means of magnitudes of the gradients for the sigmoid network are significantly higher for lower layers. This is an indication of the vanishing gradient problem. <br>\n",
    "c_) One reason why the sigmoid network can lead to vanishing gradients might be that the derivative of the sigmoid does not exceed 1/4. <br>\n",
    "d_)  When you use the ReLU activation function, the resulting network trains well in only one epoch. <br>\n",
    "e_) One reason why several well-known activation functions different from sigmoid can lead to more stable training can be the fact that their derivatives are always larger than 1.\n",
    "\n",
    "To answer the question, assign \"True\" or \"False\" boolean values to variables in the next cell. A non-correctly answered question yields negative points and no answer (i.e. answer “None”) gives 0 points for a question.<br>\n",
    "<b>Note:</b> Do not reuse these variable names. They are used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "question1"
    ]
   },
   "outputs": [],
   "source": [
    "#your answers go here ↓↓↓\n",
    "a_=True\n",
    "b_=False\n",
    "c_=True\n",
    "d_=True\n",
    "e_=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, however, there are possible ways to improve the learning of the sigmoid network without changing the activation function and the network size (you are encouraged to make an educated guess and try out several choices), but still, the network archictecture seems to be too simple to really lead to a satisfying performance. In the upcoming assignment, we will further ellaborate on this issue."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
